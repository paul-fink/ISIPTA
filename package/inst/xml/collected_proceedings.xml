<proceedingslist>
  <proceedings>
    <year>1999</year>
    <conference>
      <date>
        <start>1999-06-29</start>
        <end>1999-07-02</end>
      </date>
      <location>
        <country>
          <code>BE</code>
          <name>Belgium</name>
        </country>
        <city>
          <name>Ghent</name>
          <latitude>51.05000</latitude>
          <longitude>3.71667</longitude>
        </city>
        <university>
          <name>Ghent University</name>
          <department/>
        </university>
      </location>
    </conference>
    <paper>
      <id>049</id>
      <title>A Non-specificity Measure for Convex Sets of Probability Distributions</title>
      <authors>
        <author>
          <name>Joaquin Abellan</name>
          <email>jabellan@jet.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Malaga</name>
              <latitude>36.72016</latitude>
              <longitude>-4.42034</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>uncertainty</keyword>
        <keyword>imprecision</keyword>
        <keyword>non-specificity</keyword>
      </keywords>
      <abstract>In belief functions, there are two types of uncertainty which are due to lack of knowledge: randomness and non-specificity. In this paper, we present a non-specificity measure for convex sets of probability distributions that generalizes Dubois and Prade's non-specificity measure in the Dempster-Shafer theory of evidence.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/049.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Learning in prevision Space</title>
      <authors>
        <author>
          <name>Stefan Arnborg</name>
          <email>stefan@nada.kth.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>learning</keyword>
        <keyword>uncertainty</keyword>
        <keyword>decomposition</keyword>
        <keyword>uncertainty polytope</keyword>
      </keywords>
      <abstract>We investigate some problems related to implementation of uncertainty management, in particular the handling of computational and conceptual difficulties that easily appear in complex problems. The uncertainty polytope resulting from a set of inequality judgments on probabilities and means in a problem has very high dimension, but can be represented by a projection on a low-dimensional space if the judgments are structured into a graph with low tree-width. With this representation many judgments of independence become vacuous. The uncertainty polytope is high-dimensional and thus difficult to grasp or visualize. We propose a method to sample uniformly and efficiently from the polytope, as a means to obtain various summaries not obtainable by linear programming, such as volume, center of gravity, principal axes, etc.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/013.pdf</pdf>
    </paper>
    <paper>
      <id>052</id>
      <title>Globally Least Favorable Pairs and Neyman-Pearson Testing under Interval Probability</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval probability</keyword>
        <keyword>f-probability</keyword>
        <keyword>capacity</keyword>
        <keyword>neyman-pearson testing</keyword>
        <keyword>huber-strassen theory</keyword>
        <keyword>generalized neighborhood models</keyword>
        <keyword>least favorable pseudo-capacities</keyword>
      </keywords>
      <abstract>The paper studies the Generalized Neyman-Pearson problem where both hypotheses are described by interval probability. First the Huber-Strassen theorem and the literature based on it is reviewed. Then some recent results are presented indicating that the restrictive assumption of C-probability (two monotonicity) underlying all that work can be overcome in favor of considering general interval probability in sense of Weichselberger.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/052.pdf</pdf>
    </paper>
    <paper>
      <id>070</id>
      <title>Implicative Analysis for Multivariate Binary Data using an Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Jean-Marc Bernard</name>
          <email>berj@univ-paris8.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Saint-Denis</name>
              <latitude>48.93333</latitude>
              <longitude>2.36667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>quasi-implication</keyword>
        <keyword>logical model</keyword>
        <keyword>measure of association</keyword>
        <keyword>multivariate implicative index</keyword>
        <keyword>boolean methods</keyword>
        <keyword>bayesian inference</keyword>
        <keyword>upper and lower probability</keyword>
      </keywords>
      <abstract>Bayesian implicative analysis was proposed for summarizing the association in a $2\times 2$ contingency table in terms possibly, asymmetrical such as, \eg, ``presence of feature $a$ implies, in general, presence of feature $b$'' (``$a$ quasi-implies $b$'' in short). Here, we consider the multivariate version of this problem: having $n$ units which are classified according to $\Qcard$ binary questions, we want to summarize the association between questions in terms of quasi-implications between features. We will first show how at a descriptive level the notion of implication can be weakened into that of quasi-implication. The inductive step assumes that the $n$ units are a sample from a $2^\Qcard$-multinomial population. Uncertainty about the patterns' true frequencies is expressed by an imprecise Dirichlet model which yields upper and lower posterior probabilities for any quasi-implicative statement. This model is shown to have several advantages over the Bayesi
 an models based on a single Dirichlet prior, especially whenever $2^\Qcard$ is large and many patterns are thus unobserved by design.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/070.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>A Generalization of the Fundamental Theorem of de Finetti for Imprecise Conditional Probability Assessments</title>
      <authors>
        <author>
          <name>Veronica Biazzo</name>
          <email>biazzo@liotro.dipmat.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dipmat.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional events</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>generalized coherence</keyword>
        <keyword>coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>extension</keyword>
        <keyword>algorithm</keyword>
        <keyword>probability logic</keyword>
        <keyword>probabilistic deduction</keyword>
        <keyword>probabilistic satisfiability</keyword>
      </keywords>
      <abstract>In this paper, based on a suitable generalization of the coherence principle of de Finetti, we consider imprecise probability assessments on finite families of conditional events and we study the problem of their extension. Then, we extend some theoretical results and an algorithm, previously obtained for precise assessments, to the case of imprecise assessments and we propose a generalized version of the fundamental theorem of de Finetti. Our algorithm can be also exploited to produce coherent lower and upper probabilities. Moreover, we compare our approach to similar ones, like probability logic. Finally, we apply our algorithm to some well known inference rules under taxonomical knowledge.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/009.ps</pdf>
    </paper>
    <paper>
      <id>001</id>
      <title>Sharing Beliefs: Between Agreeing and Disagreeing</title>
      <authors>
        <author>
          <name>Antoine Billot</name>
          <email>billot@u-paris2.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alain Chateauneuf</name>
          <email>chateaun@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Itzhak Gilboa</name>
          <email>gilboa@econ.tau.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Tel Aviv</name>
              <latitude>32.08088</latitude>
              <longitude>34.78057</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jean-Marc Tallon</name>
          <email>jmtallon@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>betting</keyword>
        <keyword>multiple priors</keyword>
        <keyword>full insurance</keyword>
        <keyword>pareto optimality</keyword>
        <keyword>separation theorem</keyword>
      </keywords>
      <abstract>In an exchange economy with no aggregate uncertainty, and Bayesian agents, Pareto optimal allocations provide full insurance if and only if the agents have a common prior. It is hard to explain why there is relatively so little betting taking place. One is led to ask, when are full insurance allocations optimal for uncertainty averse agents? It turns out that commonality of beliefs, appropriately defined, is key again. Specifically, consider agents who are uncertainty averse and who maximize the minimal expected utility according to a set of possible priors. Pareto optimal allocations provide full insurance if and only if the agents share at least one prior. In the proof of this result, we develop a separation theorem among $n$ convex sets, that might be of independent interest.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/001.pdf</pdf>
    </paper>
    <paper>
      <id>058</id>
      <title>Plausibility and Belief Measures Induced by Kripke Models</title>
      <authors>
        <author>
          <name>Veselka Boeva</name>
          <email>boevi@mbox.digsys.bg</email>
          <location>
            <country>
              <code>BG</code>
              <name>Bulgaria</name>
            </country>
            <city>
              <name>Plovdiv</name>
              <latitude>42.15000</latitude>
              <longitude>24.75000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Elena Tsiporkova</name>
          <email>etsipork@baard.lhs.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Wemmel</name>
              <latitude>50.90812</latitude>
              <longitude>4.30613</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Bernard De Baets</name>
          <email>bernard.debaets@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>accessibility relation</keyword>
        <keyword>basic probability assignment</keyword>
        <keyword>belief measure</keyword>
        <keyword>modal logic</keyword>
        <keyword>multivalued mapping</keyword>
        <keyword>plausibility measure</keyword>
        <keyword>value assignment function</keyword>
      </keywords>
      <abstract>Modal logic interpretations of plausibility and belief measures are developed based on the observation that the inverse of the value assignment function in a model of modal logic induces the upper plausibility and lower belief measures of the plausibility and belief measures induced by the accessibility relation, regarded as a multivalued mapping.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/058.ps</pdf>
    </paper>
    <paper>
      <id>065</id>
      <title>A Review of Propagation Algorithms for Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Andres Cano</name>
          <email>acu@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>propagation algorithms</keyword>
        <keyword>valuation based systems</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>This paper reviews algorithms for local computation with imprecise probabilities. These algorithms try to solve problems of inference (calculation of conditional or unconditional probabilities) in cases in which there are a large number of variables. There are two main types in the literature depending on the nature of assumed independence relationships in each case. In both of them the global knowledge is composed of several pieces of information. The objective is to carry out a sound global computation bu using mainly the initial local representation.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/065.pdf</pdf>
    </paper>
    <paper>
      <id>036</id>
      <title>Axiomatic Characterization of Partial Ordinal Relations</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vant@stat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>qualitative representation of uncertainty</keyword>
        <keyword>axiomatization</keyword>
      </keywords>
      <abstract>In this paper we focus on the theoretical properties of non-numerical representation of the uncertainty. As usual, this representation is realized by an ``ordinal relation" (or, equivalently, by a ``comparative scale'') among the ``entities" (events, alternatives or acts) of a specific problem. After giving an overview of different known axioms characterizing some classes of ordinal relations (and their duals), we introduce some axioms capable to enclose the necessary and sufficient conditions for the representability of ordinal relations defined on arbitrary finite sets of events by the best-known uncertainty measures.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/036.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Open-frame Dempster Conditioning for Incomplete Interval Probabilities</title>
      <authors>
        <author>
          <name>Paola Castellan</name>
          <email>sgarro@univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Andrea Sgarro</name>
          <email>sgarro@univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>open-frame beliefs</keyword>
        <keyword>dempster's conditioning rule</keyword>
        <keyword>incomplete probabilities</keyword>
        <keyword>interval probability</keyword>
      </keywords>
      <abstract>The second author has put forward a theory of incomplete interval probabilities meant to give a common framework to both interval probabilities and open-frame bodies of evidence, as obtained by application of the non-normalized (open-frame) Dempster rule. Below we re-describe this proposal and then compare two possible ways of "conditioning" based on the open-frame Dempster rule: namely, we condition the original (possibly incomplete) knowledge by pooling it with new evidence which assigns certainty to the conditioning event. The idea is trying to build a probabilistic theory which would be able to cope not only wth uncertainty and ignorance, but also with [forms of] contradictoriness, to be included into the description of a possible state of knowledge.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/007.pdf</pdf>
    </paper>
    <paper>
      <id>051</id>
      <title>Ambiguity Reduction Through new Statistical Data</title>
      <authors>
        <author>
          <name>Alain Chateauneuf</name>
          <email>chateaun@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jean-Christophe Vergnaud</name>
          <email>vergnaud@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>revising</keyword>
        <keyword>information value</keyword>
        <keyword>belief function</keyword>
      </keywords>
      <abstract>We provide some objective foundations for a belief revision process in a situation where i)the decision maker's initial probabilistic knowledge is imprecise and characterized by the core of a belief function; ii)expected new data are themselves consistent with a belief function with known focal sets and iii) is based on belief function combination. We study the properties of the information value for such revising in the GILBOA - SCHMEIDLER multi-prior model.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/051.ps</pdf>
    </paper>
    <paper>
      <id>042</id>
      <title>Consumption / Pollution Tradeoffs under Hard Uncertainty and Irreversibility</title>
      <authors>
        <author>
          <name>Morgane Cheve</name>
          <email>mcheve@ecp.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Chatenay-Malabry</name>
              <latitude>48.76507</latitude>
              <longitude>2.26655</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ronan Congar</name>
          <email>rcongar@ecp.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Chatenay-Malabry</name>
              <latitude>48.76507</latitude>
              <longitude>2.26655</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>optimal pollution control</keyword>
        <keyword>environmental risk</keyword>
        <keyword>belief function</keyword>
        <keyword>random intervals</keyword>
        <keyword>representation of uncertainty</keyword>
      </keywords>
      <abstract>This paper deals with a model of pollution accumulation in which a catastrophic environmental event occurs once the pollution stock exceeds some uncertain critical level. This problem is studied in a context of "hard uncertainty" since we consider that the available knowledge concerning the value taken by the critical pollution threshold contains both randomness and imprecision. Such a general form of knowledge is modelled as a (closed) random interval. This approach is mathematically tractable and amenable to numerical simulations. In this framework we investigate the effect of hard uncertainty on the optimal pollution/consumption trade-off and we compare the results with those obtained both in the certainty case and in the case of "soft uncertainty" (where only randomness prevails).</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/042.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>An Experimental Study of Updating Ambiguous Beliefs</title>
      <authors>
        <author>
          <name>Michele Cohen</name>
          <email>cohenmd@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Itzhak Gilboa</name>
          <email>gilboa@econ.tau.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Tel Aviv</name>
              <latitude>32.08088</latitude>
              <longitude>34.78057</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jean-Yves Jaffray</name>
          <email>Jean-Yves.Jaffray@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>David Schmeidler</name>
          <email>schmeid@post.tau.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Tel Aviv</name>
              <latitude>32.08088</latitude>
              <longitude>34.78057</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>uncertainty</keyword>
        <keyword>capacity</keyword>
        <keyword>updating</keyword>
        <keyword>conditioning rule</keyword>
      </keywords>
      <abstract>Ambiguous beliefs`` are beliefs which are inconsistent with a unique, additive prior. The problem of their update in face of new information has been dealt with in the theoretical literature, and received several contradictory answers. In particular, the ``maximum likelihood update`` and the ``full Bayesian update`` have been axiomatized. This experimental study attempts to test the descriptive validity of these two theories by using the Ellsberg experiment framework.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/053.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Coherent Upper and Lower Bayesian Updating</title>
      <authors>
        <author>
          <name>Giulianella Coletti</name>
          <email>coletti@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Romano Scozzafava</name>
          <email>romscozz@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>bayesian updating</keyword>
        <keyword>upper and lower probability</keyword>
      </keywords>
      <abstract>The main concern of this paper is to show by means of suitable examples that a ``naif'' use of Bayesian updating can lead to wrong conclusions. Given some possible diseases (that could explain an initial piece of information) and a relevant tentative probability assessment, a doctor has usually at his disposal also a data base consisting of conditional probabilities P(E|K), where each K is a disease and each evidence E comes from a suitable test. Once the coherence (de Finetti) of the whole assessment is checked, we want to suitably update the prior probabilities: since we do not assume that the diseases constitute a partition of the certain event, the usual Bayes theorem cannot be applied. Then we proceed by referring to the relevant atoms (whose coherent probability assessment is, in a sense, ``imprecise'', since in general it is not unique). By checking again the coherence of the whole updated assessment, it turns out that we get upper and lower cond
 itional probabilities. These steps are iterated until a degree of belief sufficient to make a diagnosis is reached: the coherence condition acts as a control tool on every stage.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/038.pdf</pdf>
    </paper>
    <paper>
      <id>057</id>
      <title>Lower Desirability Functions: A Convenient Imprecise Hierarchical Uncertainty Model</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>hierarchical uncertainty model</keyword>
        <keyword>coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>imprecision</keyword>
      </keywords>
      <abstract>I introduce and study a fairly general imprecise second-order uncertainty model, in terms of lower desirability. A modeller's lower desirability for a gamble is defined as her lower probability for the event that a given subject will find the gamble (at least marginally) desirable. For lower desirability assessments, rationality criteria are introduced that go back to the criteria of avoiding sure loss and coherence in the theory of (first-order) imprecise probabilities. I also introduce a notion of natural extension that allows the least committal coherent extension of lower desirability assessments to larger domains, as well as to a first-order model, which can be used in statistical reasoning and decision making. The main result of the paper is what I call {\em Precision--Imprecision Equivalence\/}: as far as certain behavioural implications of this model are concerned, it does not matter whether the subject's underlying first-order model is assumed to be pre
 cise or imprecise.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/057.pdf</pdf>
    </paper>
    <paper>
      <id>068</id>
      <title>Examples of Independence for Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@pinon.ccu.uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Peter Walley</name>
          <email>NA</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Lower Hutt</name>
              <latitude>-41.21667</latitude>
              <longitude>174.91667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>independence</keyword>
        <keyword>conditioning</keyword>
        <keyword>convex sets of probabilities</keyword>
      </keywords>
      <abstract>In this paper we try to clarify the notion of independence for imprecise probabilities. Our main point is that there are several possible definitions of independence which are applicable in different types of situation. With this aim, simple examples are given in order to clarify the meaning of the different concepts of independence and the relationships between them.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/068.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>Computing Posterior Upper Expectations</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>convex sets of probability measures</keyword>
        <keyword>linear and linear fractional programming</keyword>
        <keyword>graphical models and directed acyclic graphs</keyword>
      </keywords>
      <abstract>This paper investigates the computation of posterior upper expectations induced by imprecise probabilities, with emphasis on the consequences of Walley's concepts of irrelevance and independence. Algorithms that simultaneously handle imprecise priors and imprecise likelihoods are derived through linear fractional programming; sequences of independent measurements are then analyzed, and a result on the limiting divergence of posterior upper probabilities is presented. Algorithms that handle irrelevance and independence relations in multivariate models are analyzed through graphical representations, inspired by the popular Bayesian network model.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/033.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Totally Monotone Core and Products of Monotone Measures</title>
      <authors>
        <author>
          <name>Dieter Denneberg</name>
          <email>denneberg@math.uni-bremen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bremen</name>
              <latitude>53.08891</latitude>
              <longitude>8.79063</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>monotone measure</keyword>
        <keyword>product measures</keyword>
        <keyword>core of cooperative game</keyword>
      </keywords>
      <abstract/>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/011.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Applications of Possibility and Evidence Theory in Civil Engineering</title>
      <authors>
        <author>
          <name>Thomas Fetz</name>
          <email>fetz@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michael Oberguggenberger</name>
          <email>michael@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Simon Pittschmann</name>
          <email>simon@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>engineering applications</keyword>
        <keyword>finite element method</keyword>
        <keyword>queueing models</keyword>
        <keyword>imprecise parameters</keyword>
        <keyword>possibility and evidence theory</keyword>
        <keyword>fuzzy probabilities</keyword>
      </keywords>
      <abstract>This article is devoted to applications of fuzzy set theory, possibility theory and evidence theory in civil engineering, presenting current work of a group of researchers at the University of Innsbruck. We argue that these methods are well suited for analyzing and processing the parameter uncertainties arising in soil mechanics and construction management. We address two specific applications here: finite element computations in foundation engineering and a queueing model in earth work.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/016.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>An Outline of a Comparative Foundation to Ambiguity Aversion</title>
      <authors>
        <author>
          <name>Paolo Ghirardato</name>
          <email>paolo@hss.caltech.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pasadena</name>
              <latitude>34.14778</latitude>
              <longitude>-118.14452</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Massimo Marinacci</name>
          <email>marinacc@economia.unibo.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Bologna</name>
              <latitude>44.49381</latitude>
              <longitude>11.33875</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ambiguity aversion</keyword>
        <keyword>choquet expected utility</keyword>
        <keyword>multiple priors</keyword>
        <keyword>lower probabilities</keyword>
        <keyword>canonical preference relations</keyword>
      </keywords>
      <abstract>The theory of subjective expected utility (SEU) has been extended in many recent works, allowing ambiguity to matter for choice. However, a fully satisfactory and general notion of ambiguity aversion, analogous to risk aversion for SEU, is still missing. This outline summarizes the findings of a much longer work of ours [9]. There, using a new preference model which encompasses most of the recent literature, we provide such a definition by building on a comparative notion of ambiguity aversion. The development of the latter is not immediate, since it is necessary to distinguish between differences in ambiguity and risk attitude. The solution we offer is very general as it only requires a richness condition on the set of consequences. Employing the comparative notion, we call `ambiguity averse' a preference relation which is `more ambiguity averse' than a SEU preference with similar risk attitude. We show that ambiguity aversion in this sense has a simple charact
 erization, especially for the specific models that are most popular in the literature. We then build on these ideas to provide a definition of unambiguous act and event. We show that for preferences which have a consistent ambiguity attitude, the sets of unambiguous acts and events have a simple and easily checked characterization. As an illustration, we consider the classical Ellsberg 3-color urn problem and find</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/034.pdf</pdf>
    </paper>
    <paper>
      <id>056</id>
      <title>Upper Approximation of Non-additive Measures by k-additive Measures --- The Case of Belief Functions</title>
      <authors>
        <author>
          <name>Michel Grabisch</name>
          <email>michel.grabisch@lcr.thomson-csf.com</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Orsay</name>
              <latitude>48.69572</latitude>
              <longitude>2.18727</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>non-additive measure</keyword>
        <keyword>fuzzy measure</keyword>
        <keyword>k-additive measure</keyword>
        <keyword>belief function</keyword>
        <keyword>upper approximation</keyword>
      </keywords>
      <abstract>In this paper we give a general necessary condition for a non-additive measure to be dominated by a k-additive measure. The dominating measure is seen as a kinear transformation of the original measure. We investigate some algebraic properties of these transformations, and study the case of belief functions. non</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/056.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>Probabilistic Satisfiability with Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Pierre Hansen</name>
          <email>pierreh@crt.umontreal.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Montreal</name>
              <latitude>45.50884</latitude>
              <longitude>-73.58781</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Brigitte Jaumard</name>
          <email>brigitt@crt.umontreal.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Montreal</name>
              <latitude>45.50884</latitude>
              <longitude>-73.58781</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marcus Poggi de Aragao</name>
          <email>poggi@inf.puc-rio.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Rio De Janeiro</name>
              <latitude>-22.90278</latitude>
              <longitude>-43.20750</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabien Chauny</name>
          <email>fabien@crt.umontreal.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Montreal</name>
              <latitude>45.50884</latitude>
              <longitude>-73.58781</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sylvain Perron</name>
          <email>sylvain@crt.umontreal.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Montreal</name>
              <latitude>45.50884</latitude>
              <longitude>-73.58781</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>satisfiability</keyword>
        <keyword>probability intervals</keyword>
        <keyword>qualitative probability</keyword>
        <keyword>polyhedra</keyword>
        <keyword>linear programming</keyword>
        <keyword>column generation</keyword>
        <keyword>nonlinear 0--1 programming</keyword>
      </keywords>
      <abstract>Treatment of imprecise probabilities within the probabilistic satisfiability approach to uncertainty in knowledge-based systems is surveyed and discussed. Both probability intervals and qualitative probabilities are considered. Analytical and numerical methods to test coherence and bound the probability of a conclusion are reviewed. They use polyhedral combinatorics and advanced methods of linear programming.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/041.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>A Generalization of the Concept of Markov Decision Process to Imprecise Probabilities</title>
      <authors>
        <author>
          <name>David Harmanec</name>
          <email>davidh@comp.nus.edu.sg</email>
          <location>
            <country>
              <code>SG</code>
              <name>Singapore</name>
            </country>
            <city>
              <name>Singapore</name>
              <latitude>1.28967</latitude>
              <longitude>103.85007</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>generalized markov decision process</keyword>
        <keyword>sequential decision making</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval utilities</keyword>
      </keywords>
      <abstract>This paper is a first step towards generalizing the concept of Markov decision process to imprecise probabilities. A concept of generalized Markov decision process is defined and a solution procedure for it presented.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/035.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Rational Decision Making With Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Jean-Yves Jaffray</name>
          <email>Jean-Yves.Jaffray@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>rationality</keyword>
        <keyword>dynamic decision making</keyword>
        <keyword>resolute choice</keyword>
      </keywords>
      <abstract>Decision criteria based on an imprecise probability representation of uncertainty have been criticized, from the normative point of view, on the grounds that they make the decision maker (DM) vulnerable to manipulations and, more generally, likely to take up a dominated strategy. This is indeed the case when the DM is both consequentialist (his choices in a subtree are not influenced by data concerning the rest of the tree) and sophisticated ( his present choice, determined by backward recursion, is best given his future choices). Renouncing consequentialism, which, as shown by Machina, is a way out of the difficulty, may seem to increase excessively the complexity of the model. We revisit the whole question, and first argue that in sequential decision situations it is possible to separate preference from choice, without abandoning the Revealed Preference Creed ; then, we propose to assume consequentialist preference and accept non-consequentialist behavior ; bu
 ilding on these assumptions, we discuss McClennen's Resolute Choice model and its interpretation involving multiple Selves ; finally, taking the decision aiding point of view, we suggest an implementation of Resolute Choice in which the consensus goal among the Selves is to select an undominated strategy.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/027figure.pdf</pdf>
    </paper>
    <paper>
      <id>074</id>
      <title>Coherent Models for Discrete Possibilistic Systems</title>
      <authors>
        <author>
          <name>Hugo Janssen</name>
          <email>hugo.janssen@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Etienne E. Kerre</name>
          <email>etienne.kerre@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibilistic markov system</keyword>
        <keyword>markov condition</keyword>
        <keyword>coherence</keyword>
        <keyword>dempster's conditioning rule</keyword>
      </keywords>
      <abstract>We consider discrete possibilistic systems for which the available information is given by one-step transition possibilities and initial possibilities. These systems can be represented by a collection of variables satisfying a possibilistic counterpart of the Markov condition. This means that, given the values assumed by a selection of variables, the possibility that a subsequent variable assumes some value is only dependent on the value taken by the most recent variable of the selection. The one-step transition possibilities are recovered by computing the conditional possibility of any two consecutive variables. Under the behavioural interpretation as marginal betting rates against events these 'conditional' possibilities and the initial possibilities should satisfy the rationality criteria of 'avoiding sure loss' and 'coherence'. We show that this is indeed the case when the conditional possibilities are defined using Dempster's conditioning rule.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/074.pdf</pdf>
    </paper>
    <paper>
      <id>059</id>
      <title>Demand for Insurance, Imprecise Probabilities and Ambiguity Aversion</title>
      <authors>
        <author>
          <name>Meglena Jeleva</name>
          <email>jeleva@ensae.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>demand for insurance</keyword>
        <keyword>coinsurance</keyword>
        <keyword>deductibles</keyword>
        <keyword>ambiguity</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>This article deals with demand for insurance under non-probabilized uncertainty: the available information allows only to locate the loss probability into a given interval. In this context, we apply a model, generalizing expected utility which involves, besides the standard utility function, a pessimism-optimism index representing the agent's attitude towards ambiguity. In this context, choices empirically observed, but impossible to explain with the vNM model, are enlightened: when the insurance premium is fair, risk averse agents can choose not to buy insurance, while with loaded premium, there are agents who buy full coverage. Choices of this type appear with both linear and non-linear contracts.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/059.pdf</pdf>
    </paper>
    <paper>
      <id>064</id>
      <title>Possibilistic Systems Within a General Information Theory</title>
      <authors>
        <author>
          <name>Cliff Joslyn</name>
          <email>joslyn@lanl.gov</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Los Alamos</name>
              <latitude>35.84951</latitude>
              <longitude>-106.28848</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibility theory</keyword>
        <keyword>random set</keyword>
        <keyword>fuzzy measure</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>general information theory</keyword>
        <keyword>possibilistic processes</keyword>
      </keywords>
      <abstract>We survey possibilistic systems theory and place it in the context of Imprecise Probabilities and General Information Theory (\git). In particular, we argue that possibilistic systems hold a distinct position within a broadly conceived, synthetic \git. Our focus is on systems and applications which are semantically grounded by empirical measurement methods (statistical counting), rather than epistemic or subjective knowledge elicitation or assessment methods. Regarding fuzzy measures as special previsions, and evidence measures (belief and plausibility measures) as special fuzzy measures, thereby we can measure imprecise probabilities directly and empirically from set-valued frequencies (random set measurement). More specifically, measurements of random intervals yield empirical fuzzy intervals. In the random set (Dempster-Shafer) context, probability and \pos\ measures stand as special plausibility measures in that their ``distributionality'' (decomposability) 
 maps directly to an ``aggregable'' structure of the focal classes of their random sets. Further, possibility measures share with imprecise probabilities the ability to better handle ``open world'' problems where the universe of discourse is not specified in advance. In addition to empirically grounded measurement methods, possibility theory also provides another crucial component of a full systems theory, namely prediction methods in the form of finite (Markov) processes which are also strictly analogous to the probabilistic forms.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/064.pdf</pdf>
    </paper>
    <paper>
      <id>076</id>
      <title>Applying Non-parametric Robust Bayesian Analysis to Non-opiniated Judicial Neutrality</title>
      <authors>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Elias Moreno</name>
          <email>emoreno@ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Maria Eglee Perez</name>
          <email>eglee@cesma.usb.ve</email>
          <location>
            <country>
              <code>VE</code>
              <name>Venezuela</name>
            </country>
            <city>
              <name>Caracas</name>
              <latitude>10.48801</latitude>
              <longitude>-66.87919</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Luis Raul Pericchi</name>
          <email>pericchi@cesma.usb.ve</email>
          <location>
            <country>
              <code>VE</code>
              <name>Venezuela</name>
            </country>
            <city>
              <name>Caracas</name>
              <latitude>10.48801</latitude>
              <longitude>-66.87919</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>discrimination</keyword>
        <keyword>elicitation</keyword>
        <keyword>law</keyword>
        <keyword>linearization</keyword>
        <keyword>moment problem</keyword>
      </keywords>
      <abstract>This paper explores the usefulness of robust Bayesian analysis in the context of an applied problem, finding priors to model judicial neutrality in an age discrimination case. We seek large classes of prior distributions without trivial bounds on the posterior probability of a key set, that is, without bounds that are independent of the data. Such an exploration shows qualitatively where the prior elicition matters most, and qualitatively how sensitive the conclusions are to specified prior changes.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/076.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Nonlinear Filtering of Convex Sets of Probability Distributions</title>
      <authors>
        <author>
          <name>John Kenney</name>
          <email>keney@ee.byu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Provo</name>
              <latitude>40.23384</latitude>
              <longitude>-111.65853</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Wynn Stirling</name>
          <email>wynn@ee.byu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Provo</name>
              <latitude>40.23384</latitude>
              <longitude>-111.65853</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>nonlinear filtering</keyword>
        <keyword>convex sets of distributions</keyword>
        <keyword>set-valued estimation</keyword>
      </keywords>
      <abstract>A solution is provided to the problem of computing a convex set of conditional probability distributions that characterize the state of a nonlinear dynamic system as it evolves in time. The estimator uses the Galerkin approximation to solve Kolmogorov's equation for the diffusion of a continuous-time nonlinear system with discrete- time measurement updates. Fitering of the state is accomplished for a convex set of distributions simultaneously, and closed-form representations of the resulting sets of means and covariances are generated.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/014.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Uncertainty and Information Measures for Imprecise Probabilities: An Overview</title>
      <authors>
        <author>
          <name>George Klir</name>
          <email>gklir@binghamton.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Binghamton</name>
              <latitude>42.09869</latitude>
              <longitude>-75.91797</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>uncertainty measures</keyword>
        <keyword>uncertainty based information</keyword>
        <keyword>evidence theory</keyword>
        <keyword>possibility theory</keyword>
        <keyword>hartley-like measure</keyword>
        <keyword>shannon-like measure</keyword>
      </keywords>
      <abstract>The paper deals with basic issues regarding the measurement of relevant types of uncertainty and uncertainty-based information in theories that represent imprecise probabilities of various types. Existing results and encountered difficulties regarding these issues, primarily in evidence theory and possibility theory, are presented. Some important open questions and unexplored areas of research in this domain are also discussed.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/050.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Imprecise Probabilities Relating to Prior Reliability Assessments</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>ikozine@binghamton.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Binghamton</name>
              <latitude>42.09869</latitude>
              <longitude>-75.91797</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>reliability assessments</keyword>
        <keyword>belief function</keyword>
        <keyword>analogy</keyword>
      </keywords>
      <abstract>The paper summarizes the author's experience in dealing with the Dempster-Shafer theory relating to reliability assessments and demonstrates how to make components and systems reliability assessments based on the theory of coherent imprecise previsions. The procedure of prior imprecise probability elicitation of components is based on analogical reasoning, and two cases of precise and imprecise probabilities of prototypes are considered. Cases of combining different reliability judgements on the same component are analyzed. The formulae obtained for systems reliability assessments allow getting the lower and upper probabilities without the presumption of a conditional independence.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/021.pdf</pdf>
    </paper>
    <paper>
      <id>069</id>
      <title>Modeling Ellsberg's Paradox in Vague-Vague Cases</title>
      <authors>
        <author>
          <name>Karen Kramer</name>
          <email>kkramer@uiuc.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Urbana-Champaign</name>
              <latitude>40.11059</latitude>
              <longitude>-88.20727</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>David Budescu</name>
          <email>dbudescu@uiuc.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Urbana-Champaign</name>
              <latitude>40.11059</latitude>
              <longitude>-88.20727</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>vagueness</keyword>
        <keyword>ambiguity</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>ellsberg's paradox</keyword>
      </keywords>
      <abstract>We explore a generalization of Ellsberg's paradox (2-color scenario) to the Vague-Vague (V-V) case, in which neither of the probabilities (urns) is specified precisely, but one urn is always more precise than the other. One hundred and seven undergraduate students compared 63 pairs of urns involving positive outcomes. The paradox is as prevalent in the V-V case, as in the standard Precise-Vague (P-V) case. The paradox occurs more often when differences between ranges of vagueness are large and occurs less often with extreme midpoints. The urn with more vagueness was avoided for moderate to high expected probabilities and preferred for low expected probabilities in P-V cases, and the opposite pattern was found for the V-V cases. Models that capture adequately the relationships between the prevalence of vagueness avoidance and the lotteries' parameters (e.g. differences between the two ranges) were fitted for the P-V and V-V cases.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/069.pdf</pdf>
    </paper>
    <paper>
      <id>047</id>
      <title>Imprecise and Indeterminate Probabilities</title>
      <authors>
        <author>
          <name>Isaac Levi</name>
          <email>levi@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>strict bayesian</keyword>
        <keyword>quasi bayesian</keyword>
        <keyword>e-admissibility</keyword>
        <keyword>e-maximality</keyword>
        <keyword>maximizing lower expectation</keyword>
      </keywords>
      <abstract>Bayesian advocates of expected utility maximization use sets of probability distributions to represent very different ideas. Strict Bayesians insist that probability judgment is numerically determinate even though the agent can represent such judgments only in imprecise terms. According to Quasi Bayesians rational agents may make indeterminate subjective probability judgments. Both kinds of Bayesians require that admissible options maximize expected utility according to some probability distribution. Quasi Bayesians permit the distribution to vary with the context of choice. Maximalists allow for choices that do not maximize expected utility against any distribution. Maximiners mandate what maximalists allow. This paper defends the quasi Bayesian view against strict Bayesians on the one hand and maximalists and maximiners on the other.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/047.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Treatment Choice Under Ambiguity Induced by Inferential Problems</title>
      <authors>
        <author>
          <name>Charles Manski</name>
          <email>cfmanski@nwu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Evanston</name>
              <latitude>42.04114</latitude>
              <longitude>-87.69006</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>identification</keyword>
        <keyword>ambiguity</keyword>
        <keyword>treatment response</keyword>
        <keyword>bounds</keyword>
        <keyword>statistical treatment rules</keyword>
      </keywords>
      <abstract>Inferential problems that arise in the empirical analysis of treatment response induce ambiguity about the identity of optimal treatment rules. This paper describes a research program that begins with general themes about decisions under ambiguity, next specializes to problems of treatment choice under ambiguity, and then shows how identification problems and statistical issues induce ambiguity in treatment choice.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/003.pdf</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>Upper Probabilities and Additivity</title>
      <authors>
        <author>
          <name>Massimo Marinacci</name>
          <email>marinacc@economia.unibo.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Bologna</name>
              <latitude>44.49381</latitude>
              <longitude>11.33875</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>upper probability</keyword>
        <keyword>symmetric capacities</keyword>
      </keywords>
      <abstract>We show that a class of upper probabilities arising in many robustness models, turns out to be additive under a fairly weak condition.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/040.pdf</pdf>
    </paper>
    <paper>
      <id>071</id>
      <title>A survey of some applications of the idea of ambiguity aversion in economics</title>
      <authors>
        <author>
          <name>Sujoy Mukerji</name>
          <email>sm5@soton.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Southampton</name>
              <latitude>50.90353</latitude>
              <longitude>-1.40420</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ambiguity aversion</keyword>
        <keyword>uncertainty</keyword>
        <keyword>knightian uncertainty</keyword>
        <keyword>non-additive probabilities</keyword>
        <keyword>capacity</keyword>
        <keyword>choquet expectation</keyword>
        <keyword>economic contracts</keyword>
        <keyword>financial markets</keyword>
        <keyword>voting</keyword>
        <keyword>auctions</keyword>
        <keyword>public goods</keyword>
      </keywords>
      <abstract>Subjective uncertainty is characterized by ambiguity if the decision maker has an imprecise knowledge of the probabilities of payoff relevant events. In such an instance, the decision maker's beliefs are better represented by a set of probability functions than by a unique probability function. An ambiguity averse decision maker adjusts his choice on the side of caution in response to his imprecise knowledge of the odds. This paper attempts a (selective) survey of some of the achievements of the research program which has analyzed important economic phenomena using a methodology that departs from the standard paradigm by explicitly allowing for ambiguity aversion. We specifically look at applications, and implications, of ambiguity aversion in four areas: design of bilateral economic contracts, the trade in financial contracts and financial markets, strategic decision making and finally, the political economy of voting.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/071.pdf</pdf>
    </paper>
    <paper>
      <id>054</id>
      <title>The Aggregation of Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower and upper probabilities</keyword>
        <keyword>confidence-weighted probabilities</keyword>
        <keyword>expert resolution</keyword>
        <keyword>consensus</keyword>
        <keyword>coherence</keyword>
        <keyword>arbitrage</keyword>
      </keywords>
      <abstract>Two methods are presented for the aggregation of imprecise probabilities elicited from a group of experts in terms of betting rates. In the first method, the experts bet with a common opponent subject to limits on their personal betting stakes, and their individual and aggregate beliefs are represented by confidence-weighted lower and upper probabilities. In the second method, the experts bet directly with each other as a means of reconciling incoherence, and their beliefs are represented by lower and upper risk neutral probabilities-i.e., products of probabilities and relative marginal utilities for money.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/054.pdf</pdf>
    </paper>
    <paper>
      <id>022</id>
      <title>Ignorance and Rational Choice</title>
      <authors>
        <author>
          <name>Klaus Nehring</name>
          <email>kdnehring@ucdavis.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Davis</name>
              <latitude>38.54491</latitude>
              <longitude>-121.74052</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>incomplete preference</keyword>
        <keyword>ignorance</keyword>
        <keyword>robustness</keyword>
        <keyword>context-dependent choice</keyword>
        <keyword>non-informative priors</keyword>
      </keywords>
      <abstract>Ignorance about the comparative likelihood of events is reflected in incompleteness of an agent's preferences over bets. We argue that determinate rational choice is still possible if optimal choice is understood as context-dependent best compromise. An axiomatic characterization of such a choice rule is described for the special case of situations of complete ignorance (maximally incomplete preferences) which can be viewed as ``reduced forms'' of general decision problems under partial ignorance.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/022.pdf</pdf>
    </paper>
    <paper>
      <id>077</id>
      <title>On the Distribution of Natural Probability Functions</title>
      <authors>
        <author>
          <name>Jeff Paris</name>
          <email>jeff@ma.man.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Manchester</name>
              <latitude>53.48095</latitude>
              <longitude>-2.23743</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paul Watton</name>
          <email>NA</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Manchester</name>
              <latitude>53.48095</latitude>
              <longitude>-2.23743</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>George Wilmers</name>
          <email>george@ma.man.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Manchester</name>
              <latitude>53.48095</latitude>
              <longitude>-2.23743</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>prior probability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>random sentences</keyword>
        <keyword>probabilistic reasoning</keyword>
        <keyword>uncertain reasoning</keyword>
      </keywords>
      <abstract>The purpose of this note is to describe the underlying insights and results obtained by the authors, and others, in a series of papers aimed at modelling the distribution of `natural' probability functions, more precisely the probability functions on $\{0,1\}$ which we encounter naturally in the real world as subjects for statistical inference, by identifying such functions with large, random, sentences of the propositional calculus. We explain how this approach produces a robust parameterised family of priors, $J_n$, with several of the properties we might have hoped for in the context, for example marginalisation, invariance under (weak) renaming, and an emphasis on multivariate probability functions exhibiting high interdependence between features.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/077.pdf</pdf>
    </paper>
    <paper>
      <id>061</id>
      <title>Towards an Operational Interpretation of Fuzzy Measures</title>
      <authors>
        <author>
          <name>Fernando Reche</name>
          <email>freche@stat.ualm.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Almeria</name>
              <latitude>36.83814</latitude>
              <longitude>-2.45974</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Antonio Salmeron</name>
          <email>asc@stat.ualm.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Almeria</name>
              <latitude>36.83814</latitude>
              <longitude>-2.45974</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy measure</keyword>
        <keyword>partial information</keyword>
        <keyword>coherence</keyword>
        <keyword>extension</keyword>
      </keywords>
      <abstract>In this paper we propose an operational interpretation of general fuzzy measures. On the basis of this interpretation, we define the concept of coherence with respect to a partial information, and propose a rule of inference similar to the natural extension</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/061.pdf</pdf>
    </paper>
    <paper>
      <id>072</id>
      <title>How Sets of Coherent Probabilities may Serve as Models for Degrees of Incoherence</title>
      <authors>
        <author>
          <name>Mark Schervish</name>
          <email>mark@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dutch book</keyword>
        <keyword>coherence</keyword>
        <keyword>epsilon-contamination model</keyword>
      </keywords>
      <abstract>We introduce two indices for the degree of incoherence in a set of lower and upper previsions: maximizing the rate of loss the incoherent Bookie experiences in a Dutch Book, or maximizing the rate of profit the Gambler achieves who makes Dutch Book against the incoherent Bookie. We report how efficient bookmaking is achieved against these two indices in the case of incoherent previsions for events on a finite partition, and for incoherent previsions that include also a simple random variable. We relate the epsilon-contamination model to efficient bookmaking in the case of the rate of profit.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/072.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Human Judgment under Sample Space Ignorance</title>
      <authors>
        <author>
          <name>Michael Smithson</name>
          <email>Michael.Smithson@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Bartos</name>
          <email>Thomas.Bartos@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Kazuhisa Takemura</name>
          <email>takemura@shako.sk.tsukuba.ac.jp</email>
          <location>
            <country>
              <code>JP</code>
              <name>Japan</name>
            </country>
            <city>
              <name>Tsukuba</name>
              <latitude>36.20000</latitude>
              <longitude>140.10000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>human judgment</keyword>
        <keyword>uncertainty</keyword>
        <keyword>ambiguity</keyword>
        <keyword>vagueness</keyword>
        <keyword>probability</keyword>
      </keywords>
      <abstract>This paper surveys results of a research program investigating human judgments of imprecise probabilities under sample space ignorance. The framework used for comparisons with human judgments is primarily due to Walley [9, 10]. The five studies reported here investigate four of Walley's prescriptions for judgment under sample space ignorance, as well as assessing the impact of the number of observations and types of events on subjective lower and upper probability estimates. The paper concludes with a synopsis of future directions for empirical research on subjective imprecise probability judgments.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/008.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Imprecise Reliability Models for the General Lifetime Distribution Classes</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sergey Gurov</name>
          <email>kir@inf.fta.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>reliability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>natural extension</keyword>
        <keyword>monotone systems</keyword>
        <keyword>mean time to failure</keyword>
        <keyword>lifetime distribution</keyword>
      </keywords>
      <abstract>To develop a general reliability theory taking into account various sources of information and a lack of satisfactory data on which estimates of system parameters can be based, the theory of imprecise probabilities can be used. The purpose of the paper is to study structural reliability based on the imprecise probability models taking into account the ageing aspect of the lifetime distributions, independence of system components, and a lack of satisfactory data. We use the new non-parametric life distribution classes which generalize the well-known increasing and decreasing failure rate distributions and can represent various judgements related to the lifetime distributions. In this paper we apply the theory of imprecise probabilities to reliability analysis of monotone systems.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/018.ps</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>Conditional Independence Relations in Possibility Theory</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibility measure</keyword>
        <keyword>possibility distribution</keyword>
        <keyword>conditioning rule</keyword>
        <keyword>natural extension</keyword>
        <keyword>conditional possibility distribution</keyword>
        <keyword>possibilistic conditional independence</keyword>
        <keyword>formal properties of conditional independence</keyword>
      </keywords>
      <abstract>The aim of this paper is to survey and briefly discuss various rules conditioning proposed in the framework of possibility theory as well as various conditional independence relations suggested for these rules. These conditioning rules and conditional independence relations are confronted with formal properties of conditional independence. Special attention is payed to the conditioning rule based on measure-theoretical approach. It is argued that this way of conditioning and the related conditional independence notion not only generalize some of presented rules and conditional independence relations, but also their properties correspond to those possessed by stochastic conditional independence.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/031.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Epistemic Independence for Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Paolo Vicig</name>
          <email>paolov@econ.univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent imprecise probability</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>logical independence</keyword>
        <keyword>factorization</keyword>
      </keywords>
      <abstract>The aim of this paper is that of studying a notion of independence for imprecise probabilities which is essentially based on the intuitive meaning of this concept. This is expressed, in the case of two events, by the reciprocal irrelevance of the knowledge of the value of each event for evaluating the other one, and has been termed epistemic independence. In order to consider more general situations in the framework of coherent imprecise probabilities, a definition of (epistemic) independence is introduced referring to arbitrary sets of logically independent partitions. Logical independence is viewed as a natural prerequisite for epistemic independence. It is then proved that the definition is always consistent, its relationship with the factorization rule is analysed, and some of its more relevant implications are discussed.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/024.pdf</pdf>
    </paper>
    <paper>
      <id>073</id>
      <title>Partial Probability: Theory and Applications</title>
      <authors>
        <author>
          <name>Frans Voorbraak</name>
          <email>fransv@wins.uva.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial probability theory</keyword>
        <keyword>partial ignorance</keyword>
        <keyword>probabilistic belief change</keyword>
        <keyword>conditioning</keyword>
        <keyword>constraining</keyword>
        <keyword>evidence combination</keyword>
        <keyword>decision under partial ignorance</keyword>
        <keyword>minimax regret</keyword>
        <keyword>satisficing</keyword>
      </keywords>
      <abstract>In this paper, we describe an approach to handling partially specified probabilistic information. We propose a formalism, called Partial Probability Theory (PPT), which allows very general representations of belief states, and we give brief treatments of problems like belief change, evidence combination, and decision making in the context of PPT. We argue that the generality of PPT provide new insights in all the mentioned problem areas. More detailed treatments of these issues can be found in several papers referred to in the text.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/073.pdf</pdf>
    </paper>
    <paper>
      <id>063</id>
      <title>Dempster-Belief Functions Are Based on the Principle of Complete Ignorance</title>
      <authors>
        <author>
          <name>Peter Wakker</name>
          <email>Wakker@MDM.MedFac.LeidenUniv.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Leiden</name>
              <latitude>52.15833</latitude>
              <longitude>4.49306</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>complete ignorance</keyword>
        <keyword>bayesianism</keyword>
        <keyword>non-additive measure</keyword>
        <keyword>ambiguity</keyword>
      </keywords>
      <abstract>This paper shows that a "principle of complete ignorance" plays a central role in decisions based on Dempster belief functions. Such belief functions occur when, in a first stage, a random message is received and then, in a second stage, a true state of nature obtains. The uncertainty about the random message in the first stage is probabilizable, in agreement with the Bayesian principles. For the uncertainty in the second stage no probabilities are given. The Bayesian and belief function approaches part ways in the processing of uncertainty in the second stage. The Bayesian approach requires that this uncertainty also be probabilized, which may require a resort to subjective information. Belief functions follow the principle of complete ignorance in the second stage, which permits strict adherence to objective inputs.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/063.pdf</pdf>
    </paper>
    <paper>
      <id>055</id>
      <title>Towards a Unified Theory of Imprecise Probability</title>
      <authors>
        <author>
          <name>Peter Walley</name>
          <email>NA</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Lower Hutt</name>
              <latitude>-41.21667</latitude>
              <longitude>174.91667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>choquet capacity</keyword>
        <keyword>coherence</keyword>
        <keyword>comparative probability</keyword>
        <keyword>desirable gambles</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>incomplete preference</keyword>
        <keyword>lower prevision</keyword>
        <keyword>lower probability</keyword>
      </keywords>
      <abstract>Belief functions, possibility measures and Choquet capacities of order 2, which are special kinds of coherent upper or lower probability, are amongst the most popular mathematical models for uncertainty and partial ignorance. I give examples to show that these models are not sufficiently general to represent some common types of uncertainty. Coherent lower previsions and sets of probability measures are considerably more general but they may not be sufficiently informative for some purposes. I discuss two other models for uncertainty, involving sets of desirable gambles and partial preference orderings. These are more informative and more general than the previous models, and they may provide a suitable mathematical setting for a unified theory of imprecise probability.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/055.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>The Theory of Interval-Probability as a Unifying Concept for Uncertainty</title>
      <authors>
        <author>
          <name>Kurt Weichselberger</name>
          <email>weichsel@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval probability</keyword>
        <keyword>uncertainty</keyword>
        <keyword>conditional probability</keyword>
        <keyword>bayes' rule</keyword>
      </keywords>
      <abstract>The concept of interval-probability is motivated by the goal to generalize classical probability so that it can be used for describing uncertainty in general. The foundations of the theory are based on a system of three axioms -- in addition to Kolmogorov's axioms -- and definitions of independence as well as of conditional probability. The resulting theory does not depend upon interpretations of the probability concept. As an example of generalizing classical results the Bayes' rule is described -- other theorems are only mentioned.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/043.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>A Logic of Extended Probability</title>
      <authors>
        <author>
          <name>Nic Wilson</name>
          <email>pnwilson@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>probabilistic logic</keyword>
        <keyword>infinitesimals</keyword>
        <keyword>order of magnitude probabilistic reasoning</keyword>
      </keywords>
      <abstract>This paper shows how the logic of gambles corresponding to Peter Walley's system of Imprecise Probability can be extended to allow gambles involving infinitesimal values and infinite values. This logic can then be used for reasoning with infinitesimal probabilities alongside conventional reasoning with linear constraints on probabilities. The proof theory is shown to be sound and complete for finite input sets.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/017.ps</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>A Credal Approach to Naive Classification</title>
      <authors>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>credal set</keyword>
        <keyword>classification</keyword>
        <keyword>naive bayesian classification</keyword>
        <keyword>bayesian networks</keyword>
      </keywords>
      <abstract>Convex sets of probability distributions are also called credal sets. They generalize probability theory with special regard to the relaxation of the precision requirement about the probability values. Classification, i.e., assigning class labels to instances described by a set of attributes, is a typical domain of application of Bayesian methods, where the naive Bayesian classifier is considered among the best tools. This paper explores the classification model obtained when the naive Bayesian classifier is extended to credal sets. Exact and effective solution procedures for classification are derived, and the related dominance criteria are discussed. A methodology to induce the classifier from data is proposed.</abstract>
      <pdf>ftp://ensmain.rug.ac.be/pub/isipta99/019.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2001</year>
    <conference>
      <date>
        <start>2001-06-26</start>
        <end>2001-06-29</end>
      </date>
      <location>
        <country>
          <code>US</code>
          <name>United States</name>
        </country>
        <city>
          <name>Ithaca</name>
          <latitude>42.40279</latitude>
          <longitude>-76.48400</longitude>
        </city>
        <university>
          <name>Cornell University</name>
          <department/>
        </university>
      </location>
    </conference>
    <paper>
      <id>060</id>
      <title>Building Classification Trees Using the Total Uncertainty Criterion</title>
      <authors>
        <author>
          <name>Joaquin Abellan</name>
          <email>jabemu@teleline.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>uncertainty</keyword>
        <keyword>imprecision</keyword>
        <keyword>non-specificity</keyword>
        <keyword>classification</keyword>
        <keyword>classification trees</keyword>
        <keyword>credal set</keyword>
      </keywords>
      <abstract>We present an application of the measure of total uncertainty on convex sets of probability distributions, also called credal sets, to the construction of classification trees. In these classification trees the probabilities of the classes in each one of its leaves is estimated by using the imprecise Dirichlet model. In this way, smaller samples give rise to wider probability intervals. Branching a classification tree can decrease the entropy associated to the classes but, at the same time, as the sample is divided among the branches the non-specificity increases. We use a total uncertainty measure (entropy + non-specificity) as branching criterion. The stopping rule is not to increase the total uncertainty. The good behavior of this procedure for standard classification problems is shown. It is important to remark that it does not suffer of overfitting, with similar results in the training and test samples.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s060.ps</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>On Decision Making under Ambiguous Prior and Sampling Information</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>generalized expected utility</keyword>
        <keyword>ambiguity</keyword>
        <keyword>linear programming chooquet expected utility</keyword>
        <keyword>gamma-minimax principle</keyword>
        <keyword>critic of robust bayesian analysis</keyword>
      </keywords>
      <abstract>The standard framework of decision theory suffers from the fact that ambiguity (non-stochastic uncertainty, indeterminacy) can not be taken properly into account. In most applications, neither the maximin paradigm (relying on complete ignorance on the states of nature) nor the classical Bayesian paradigm (assuming perfect probabilistic information on the states of nature) adequately reflects the situation under consideration. This paper extends classical decision theory to situations where prior and/or sampling information is ambiguous. It gives a framework to generalize expected utility theory to interval probability and imprecise probabilities. Firstly, a general algorithm will be presented to calculate optimal actions under ambiguous prior information. Then it is shown how to incorporate ambiguous sampling information. As a by-product, the results also lead to a question concerning coherent inference with imprecise probabilities and robust Bayesian inference.
  It will be shown that, under ambiguity, inference based on the imprecise posterior distribution may lead to suboptimal actions. Under ambiguity, the posterior distribution does no longer contain all the relevant information.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s048.ps</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Reasoning with Assertions of High Conditional Probability: Entailment with Universal Near Surety</title>
      <authors>
        <author>
          <name>Donald Bamber</name>
          <email>bamber@spawar.navy.mil</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>San Diego</name>
              <latitude>32.71533</latitude>
              <longitude>-117.15726</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Irwin Goodman</name>
          <email>goodman@spawar.navy.mil</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>San Diego</name>
              <latitude>32.71533</latitude>
              <longitude>-117.15726</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional probability</keyword>
        <keyword>second-order probability</keyword>
        <keyword>bayesian inference</keyword>
        <keyword>nonmonotonic logic</keyword>
        <keyword>rule-based systems</keyword>
        <keyword>threshold knowledge</keyword>
        <keyword>informant</keyword>
        <keyword>robustness</keyword>
        <keyword>directed graph</keyword>
      </keywords>
      <abstract>Rules having rare exceptions are best interpreted as assertions of high conditional probability. In other words, a rule \emph{If $X$ then $Y$} is interpreted as meaning that $\Pr(Y|X) \approx 1$. In this paper, such rules are regarded as statements about imprecise probabilities, and imprecise probabilities are identified with convex sets of precise probabilities. A general approach to reasoning with such rules, based on second-order probability, is advocated. Within this general approach, different reasoning methods are needed, with the selection of a specific method being dependent upon what knowledge is available about the relative tightness of the approximation $\Pr(Y|X) \approx 1$ across rules. A method of reasoning, entailment with universal near surety, is formulated for the case when \emph{no} knowledge is available concerning these relative tightnesses. Finally, it is shown that reasoning via entailment with universal near surety is equivalent to carryin
 g out a particular test on a directed graph.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s024.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>No trade in financial markets with uncertainty</title>
      <authors>
        <author>
          <name>Marcello Basili</name>
          <email>basili@unisi.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Siena</name>
              <latitude>43.32004</latitude>
              <longitude>11.33283</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fulvio Fontini</name>
          <email>fontini@unisi.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Siena</name>
              <latitude>43.32004</latitude>
              <longitude>11.33283</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>knightian uncertainty</keyword>
        <keyword>capacity</keyword>
        <keyword>choquet asset pricing rule</keyword>
        <keyword>no-trade</keyword>
      </keywords>
      <abstract>In this paper a Radner economy is considered with Uncertainty, modeled by means of the Choquet Expected Utility. Agents are splitted into two categories: optimists, who hold a concave capacity, and pessimists, who hold a convex one. A no-trade theorem is stated and proved under the assumption of common beliefs with uncertainty.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s028.doc</pdf>
    </paper>
    <paper>
      <id>001</id>
      <title>Decision trade-offs under severe info-gap uncertainty</title>
      <authors>
        <author>
          <name>Yakov Ben-Haim</name>
          <email>yakov@aluf.technion.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Haifa</name>
              <latitude>32.81841</latitude>
              <longitude>34.98850</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>information-gap uncertainty</keyword>
        <keyword>decisions under uncertainty</keyword>
        <keyword>trade-offs</keyword>
      </keywords>
      <abstract>We are concerned in this paper with the trade-offs which confront a decision maker who deals with severely deficient information and unstructured uncertainty. We employ the theory of info-gap uncertainty, described briefly in Section 2. Using info-gap models of uncertainty we derive two decision functions which express (1) immunity to failure (robustness function) and (2) immunity to windfall gain (opportunity function). These immunity functions are discussed in Section 3. In this paper we will consider three types of trade-offs: robustness vs. reward, certainty vs. windfall, and opportunity vs. robustness. These are described succintly in Section 3 and illustrated with an example in Section 4. In Section 5 we briefly mention three methods for combining info-gap and probabilistic models of uncertainty.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s001.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Non-parametric Inference about an Unknown Mean using the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Jean-Marc Bernard</name>
          <email>berj@univ-paris8.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Saint-Denis</name>
              <latitude>48.93333</latitude>
              <longitude>2.36667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>nonparametric inference</keyword>
        <keyword>bayesian inference</keyword>
        <keyword>dirichlet distribution</keyword>
        <keyword>l-dirichlet distribution</keyword>
        <keyword>prior ignorance</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>upper and lower probability</keyword>
      </keywords>
      <abstract>We observe a random sample of $n$ observations from an unknown distribution having mean $\mu$. In this paper we consider the problem of making inferences about the unknown parameter $\mu$ and other charasteristics of the unknown distribution by adopting a non-parametric and objective minded framework, in which the observations are considered bounded and discrete (finite precision measurement). We first review the Bayesian approach based on Dirichlet priors and discuss the problems encountered by the usual vague priors. An alternative approach is then proposed which models prior ignorance by an imprecise Dirichlet model (IDM) with parameter $\ps$ (Walley, 1996). The comparison of inferences produced by the IDM with the ones from more common parametric approaches gives support for setting $\ps=2$ in the IDM. The new method of inference is illustrated on Darwin's maize data.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s050.ps</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Probabilistic Logic under Coherence: Complexity and Algorithms</title>
      <authors>
        <author>
          <name>Veronica Biazzo</name>
          <email>vbiazzo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Lukasiewicz</name>
          <email>Thomas.Lukasiewicz@kr.tuwien.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Vienna</name>
              <latitude>48.20849</latitude>
              <longitude>16.37208</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuseppe Sanfilippo</name>
          <email>gsanfilippo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional probability assessments</keyword>
        <keyword>probabilistic logic</keyword>
        <keyword>g-coherence</keyword>
        <keyword>g-coherent entailment</keyword>
        <keyword>complexity and algorithms</keyword>
      </keywords>
      <abstract>We study probabilistic logic under the viewpoint of the coherence principle of de Finetti. In detail, we explore the relationship between coherence-based and classical model-theoretic probabilistic logic. Interestingly, we show that the notions of g-coherence and of g-coherent entailment can be expressed by combining notions in model-theoretic probabilistic logic with concepts from default reasoning. Using these results, we analyze the computational complexity of probabilistic reasoning under coherence. Moreover, we present new algorithms for deciding g-coherence and for computing tight g-coherent intervals, which reduce these tasks to standard reasoning tasks in model-theoretic probabilistic logic. Thus, efficient techniques for model-theoretic probabilistic reasoning can immediately be applied for probabilistic reasoning under coherence, for example, column generation techniques. We then describe two other interesting techniques for efficient model-theoretic p
 robabilistic reasoning in the conjunctive case.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s030.ps</pdf>
    </paper>
    <paper>
      <id>022</id>
      <title>Dempster's Rule of Combination in Modal Logic</title>
      <authors>
        <author>
          <name>Veselka Boeva</name>
          <email>boevi@mbox.digsys.bg</email>
          <location>
            <country>
              <code>BG</code>
              <name>Bulgaria</name>
            </country>
            <city>
              <name>Plovdiv</name>
              <latitude>42.15000</latitude>
              <longitude>24.75000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief measure</keyword>
        <keyword>dempster's rule of combination</keyword>
        <keyword>modal logic</keyword>
        <keyword>multivalued mapping</keyword>
        <keyword>orthogonal sum</keyword>
        <keyword>plausibility measure</keyword>
      </keywords>
      <abstract>A modal logic interpretation of Dempster's rule of combination is developed. It is shown that the model of modal logic,obtained by combining two finite models of modal logic, induces plausibility and belief measures, which are, in fact, the orthogonal sums of the measures, corresponding to the original models.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s022.ps</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>A simplified algorithm for inference by lower conditional probabilities</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial lower conditional probability assessments</keyword>
        <keyword>coherent inference</keyword>
        <keyword>locally strong coherence</keyword>
      </keywords>
      <abstract>Thanks to the notion of locally strong coherence, the satisfiability of proper logical conditions on subfamilies of the initial domain helps to simplify inferential processes based on lower conditional assessments. Actually, these conditions avoid also round errors that, on the other hand, appear solving numerical systems. In this paper we introduce new conditions to be applied to sets of particular pairs of events. With respect to more general conditions already proposed, they avoid an exhaustive search, so that a sensible time-complexity reduction is possible. The usefulness of these rules in inferential processes is shown by a diagnostic medical problem with thyroid pathology.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s034.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>Locally additive comparative probabilities</title>
      <authors>
        <author>
          <name>Giulianella Coletti</name>
          <email>coletti@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Romano Scozzafava</name>
          <email>romscozz@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>comparative probability</keyword>
        <keyword>conditional probability</keyword>
        <keyword>coherence conditions</keyword>
      </keywords>
      <abstract>We characterize binary relations (defined on an arbitrary family of unconditional events) that are representable by a coherent conditional probability and those that are representable by a weakly decomposable conditional measure. Both these relations are locally "additive".</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s053.pdf</pdf>
    </paper>
    <paper>
      <id>052</id>
      <title>Graphoid Properties of Epistemic Irrelevance and Independence</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Peter Walley</name>
          <email>pwalley@eudoramail.com</email>
          <location>
            <country>
              <code>NA</code>
              <name>NA</name>
            </country>
            <city>
              <name>NA</name>
              <latitude>NA</latitude>
              <longitude>NA</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>sets of probability measures</keyword>
        <keyword>lower expectation</keyword>
        <keyword>graphoid axioms</keyword>
        <keyword>independence concepts</keyword>
      </keywords>
      <abstract>This paper investigates the concepts of irrelevance and independence in connection with imprecise probability models. We study the general properties of Walley's concepts of epistemic irrelevance and epistemic independence and their relation to the graphoid axioms. Simple examples are given to show that epistemic irrelevance can violate the symmetry, contraction and intersection axioms, that epistemic independence can violate contraction and intersection, and that this accords with the intuitive notions of irrelevance and independence.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s052.ps</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Constructing Sets of Probability Measures Through Kuznetsov's Independence Condition</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>sets of probability measures</keyword>
        <keyword>lower expectation</keyword>
        <keyword>independence concepts</keyword>
        <keyword>extension</keyword>
      </keywords>
      <abstract>The purpose of this paper is to investigate a condition, suggested by Kuznetsov, to be required of independent variables. Kuznetsov's condition demands decomposable functions to be associated with decomposable expectation intervals. The paper demonstrates that Kuznetsov's condition does enlarge the universe of models based on sets of probability measures, as the condition is not equivalent to existing concepts of independence.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s011.ps</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>Lattice structure of the families of compatible frames of discernment</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>cuzzolin@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ruggero Frezza</name>
          <email>frezza@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>family of compatible frames</keyword>
        <keyword>birkhoff lattice</keyword>
        <keyword>linear dependence</keyword>
        <keyword>conflict</keyword>
      </keywords>
      <abstract>One of the central ideas in Shafer's mathematical theory of evidence is the concept of different level of knowledge of a given phenomenon, embodied into the notion of compatible frames of discernment. In this work we are going to analyze the concept of family of frames from an algebraic point of view, distinguish among finite and general families and introduce the internal operation of maximal coarsening, originating the structure of semimodular lattice. We will show the equivalence between the classical independence of frames and the independence of frames as elements of a locally finite Birkhoff lattice, eventually prefiguring a solution to the conflict problem based on a pseudo Gram-Schmidt algorithm.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s046.ps</pdf>
    </paper>
    <paper>
      <id>047</id>
      <title>Geometric analysis of belief space and conditional subspaces</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>cuzzolin@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ruggero Frezza</name>
          <email>frezza@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>theory of evidence</keyword>
        <keyword>belief space</keyword>
        <keyword>fiber bundle</keyword>
        <keyword>convex decomposition</keyword>
        <keyword>commutativity</keyword>
        <keyword>conditional subspace</keyword>
      </keywords>
      <abstract>In this paper the geometric structure of the space $\mathcal{S}_{\Theta}$ of the belief functions defined over a discrete set $\Theta$ (belief space) is analyzed. Using the Moebius inversion lemma we prove the recursive bundle structure of the belief space and show how an arbitrary belief function can be uniquely represented as a convex combination of certain elements of the fibers, giving $\mathcal{S}$ the form of a simplex. The commutativity of orthogonal sum and convex closure operator is proved and used to depict the geometric structure of conditional subspaces, i.e. sets of belief functions conditioned by a given function s. Future applications of these geometric methods to classical problems like probabilistic approximation and canonical decomposition are outlined.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s047.ps</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Independent products of numerical possibility measures</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Enrique Miranda</name>
          <email>alu426@pinon.ccu.uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibility theory</keyword>
        <keyword>upper probability</keyword>
        <keyword>coherence</keyword>
        <keyword>conditioning</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>independent product</keyword>
      </keywords>
      <abstract>Possibility measures can be given a behavioural interpretation as systems of upper betting rates. As such, they should arguably satisfy certain rationality requirements. Using a version of Walley's notion of epistemic independence suitable for possibility measures, we investigate what these requirements tell us about the formation of independent product possibility measures from given marginals.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s012.pdf</pdf>
    </paper>
    <paper>
      <id>058</id>
      <title>Belief models: an order-theoretic analysis</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief model</keyword>
        <keyword>belief revision</keyword>
        <keyword>classical propositional logic</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>order theory</keyword>
        <keyword>possibility measure</keyword>
        <keyword>system of spheres</keyword>
      </keywords>
      <abstract>I show that there is a common order-theoretic structure underlying many of the models for representing beliefs in the literature. After identifying this structure, and studying it in some detail, I show that it is useful: it can be used to generalise the coherentist study of belief dynamics (belief expansion and revision) by using an abstract order-theoretic definition of the belief spaces where the dynamics of expansion and revision takes place. Interestingly, many of the existing results for expansion and revision in the context of classical propositional logic can be proven in this much more abstract setting, and therefore remain valid for many other belief models, such as imprecise probability models.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s058.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Functions and measures with linear ordinal scales, (a-)symmetric Sugeno integral and ordinal Ky Fan distance</title>
      <authors>
        <author>
          <name>Dieter Denneberg</name>
          <email>denneberg@math.uni-bremen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bremen</name>
              <latitude>53.08891</latitude>
              <longitude>8.79063</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michel Grabisch</name>
          <email>michel.grabisch@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>quantile</keyword>
        <keyword>ky fan metric</keyword>
        <keyword>sugeno integral</keyword>
        <keyword>aggregation</keyword>
        <keyword>reflection lattice</keyword>
      </keywords>
      <abstract>We develop a purely ordinal model for aggregation operators for lattice valued functions, comprising as special cases quantiles, Ky Fan metrics and Sugeno integrals. For modelling findings of psychological experiments like the reflection effect in decision behaviour under risk or uncertainty, we introduce reflection lattices. These are complete linear lattices endowed with an order reversing bijection like the reflection at $0$ on the real interval $[-1,1]$. Furthermore the lattice operations $\vee$, $\wedge$ are modified in order to mimic addition and multiplication of numbers of both signs.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s027.ps</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>Conditional upper probabilities assigned by a class of Hausdorff outer measures</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>conditional upper probabilities</keyword>
        <keyword>hausdorff dimensional outer measures</keyword>
      </keywords>
      <abstract>Coherent conditional probabilities, in the sense of de Finetti, are given by a class of Hausdorff dimensional measures. In particular the case where conditioning events have infinite Hausdorff measure is considered. The problem of the exstensions of these conditional probabilities to the class of all subsets of [0,1] is investigated. Conditional upper probabilities, assigned by a class of Hausdorff outer measures, are considered and their properties are analised.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s040.ps</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>New semantics for quantitative possibility theory</title>
      <authors>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Henri Prade</name>
          <email>Henri.Prade@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Philippe Smets</name>
          <email>psmets@ulb.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Brussels</name>
              <latitude>50.85045</latitude>
              <longitude>4.34878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>quantitative possibility</keyword>
        <keyword>belief function</keyword>
        <keyword/>
      </keywords>
      <abstract>New semantics for numerical values given to possibility measures are provided. For epistemic possibilities, the new approach is based on the semantics of the Transferable Belief Model itself based on betting odds interpreted in a less drastic way than what subjective probabilities presupposes. It is shown that the least informative among the belief structures that are compatible with prescribed betting rates is nested, i.e. corresponds to a possibility measure. It is also proved that the idempotent conjunctive combination of two possibility measures corresponds to the hyper-cautious conjunctive combination of the belief functions induced by the possibility measures. This view differs from the subjective semantics first proposed by Giles and relying on upper and lower probability induced by non-exchangeable bets. For objective possibility degrees, the semantics is based on the most informative possibilistic approximation of a probability measure derived from a hi
 stogram. The motivation for this semantics is its capability to extend a well-known kind of confidence intervals around the mode of a distribution to a fuzzy confidence interval. We show how the idempotent disjunctive combination of possibility functions is related to the convex mixture of probability distributions.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s019.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Imprecision in a timber asset sale model:  motivation, specification, and behavioral implications</title>
      <authors>
        <author>
          <name>Mark Ducey</name>
          <email>mjducey@cisunix.unh.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>timber management</keyword>
        <keyword>investment analysis</keyword>
        <keyword>intertemporal preferences</keyword>
        <keyword>upper and lower prevision</keyword>
      </keywords>
      <abstract>Timber management involves making long-term investment decisions. However, timber prices are characterized by interannual volatility, and the future costs and revenues of management depend on a changing social, technological, and environmental context. Timber management with fluctuating prices is commonly analyzed using an asset pricing model that can be solved as a recursive dynamic program. I reformulate the model in terms of previsions for gambles, and introduce imprecision in both future prices and discount rates. Imprecision in prices and intertemporal preferences leads to imprecise buying and selling prices for timber and for timberland. The results have behavioral implications which may assist in understanding individual landowners and timber markets.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s008.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Sets of joint probability measures generated by weighted marginal focal sets</title>
      <authors>
        <author>
          <name>Thomas Fetz</name>
          <email>fetz@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>weighted focal sets</keyword>
        <keyword>random set</keyword>
        <keyword>possibility measure</keyword>
        <keyword>plausibility measure</keyword>
        <keyword>lower and upper probabilities</keyword>
        <keyword>sets of probability measures</keyword>
      </keywords>
      <abstract>This paper is devoted to the construction of sets of joint probability measures for the case that the marginal sets of probability measures are generated by weighted focal sets. Different conditions on the choice of the weights of the joint focal sets and on the probability measures on these sets lead to different types of independence such as strong independence, random set independence, fuzzy set independence and unknown interaction. As an application the upper probabilities of failure of a beam are computed.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s021.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Towards a Frequentist Interpretation of Sets of Measures</title>
      <authors>
        <author>
          <name>Pablo Fierens</name>
          <email>pifierens@ee.cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Terrence Fine</name>
          <email>tlfine@ee.cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower envelopes</keyword>
        <keyword>frequentist interpretation</keyword>
        <keyword>simulation</keyword>
      </keywords>
      <abstract>Upper and lower envelopes can be represented by a set $\cM$ of (finitely additive) measures indexed by an unknown parameter $\theta$; this also specifies the classical frequentist concept of a compound hypothesis. Envelope models have hitherto been used almost exclusively in subjective settings to model the uncertainty or strength of belief of individuals or groups. Our interest in these imprecise probability representations is as mathematical models for those objective frequentist phenomena of engineering and scientific significance where what is known may be substantial, but relative frequencies, nonetheless, lack (statistical) stability. A full probabilistic methodology needs not only an appropriate mathematical probability concept, enriched by such notions as expectation and conditioning, but also an interpretive component to identify data that is typical of the model and an estimation component to enable inference to the model from data and background knowl
 edge. Our starting point is this first task of determining typicality. Kolmogorov complexity is used as the key non-probabilistic idea to enable us to create simulation data from an envelope model in an attempt to identify ``typical'' sequences. First steps in frequentist modeling will also be taken towards inference of the set $\cM$ from frequentist data and applied to data on vowel production from an internet message source.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s003.pdf</pdf>
    </paper>
    <paper>
      <id>064</id>
      <title>Posterior previsions for the parameter of a binomial model via natural extension of a finite number of judgments</title>
      <authors>
        <author>
          <name>Vincent Fortin</name>
          <email>vinfort@ireq.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Varennes</name>
              <latitude>45.68338</latitude>
              <longitude>-73.43246</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Eric Parent</name>
          <email>parent@engref.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Bernard Bobee</name>
          <email>bernard_bobee@inrs-eau.uquebec.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Sainte-Foy</name>
              <latitude>46.75615</latitude>
              <longitude>-71.29543</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional lower previsions</keyword>
        <keyword>prior information modeling</keyword>
        <keyword>natural extension</keyword>
        <keyword>generalized bayes rule</keyword>
        <keyword>binomial model</keyword>
      </keywords>
      <abstract>In this paper, we investigate the use of assessments of conditional previsions for modeling prior information on the parameter of a binomial model as a way of obtaining non-vacuous posterior previsions via natural extension. More specifically, we argue that a useful method for obtaining an imprecise prevision for the parameter q of a binomial model, given a sample of size n showing r successes, is to assess imprecise previsions for q which are conditional on samples having sizes larger than n. Inferences obtained using this approach are compared to Walley's proposal for learning from a bag of marbles.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s064.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>Imprecise Probabilities of Engineering System Failure from Random and Fuzzy Set Reliability Analysis</title>
      <authors>
        <author>
          <name>Jim Hall</name>
          <email>jim.hall@bristol.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Bristol</name>
              <latitude>51.45523</latitude>
              <longitude>-2.59665</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jonathan Lawry</name>
          <email>j.lawry@bris.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Bristol</name>
              <latitude>51.45523</latitude>
              <longitude>-2.59665</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>reliability analysis</keyword>
        <keyword>imprecise failure probabilities</keyword>
        <keyword>random set</keyword>
        <keyword>linguistic labels</keyword>
      </keywords>
      <abstract>Reliability analysis of engineering systems conventionally represents the system state variables as precise probability distributions and generates precise estimates of the probability of system failure. It is demonstrated how this conventional approach can be extended to handle imprecise knowledge about the system state variables, represented in general as random sets, in order to generate bounds on the probability of failure. The conventional assumption of a precise limit state function is then relaxed. A new method based on linguistic covering of the state variable space with fuzzy set labels is introduced and is used to generate an imprecise limit state function from very scarce experimental data.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s017.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>An outline of a unifying statistical theory</title>
      <authors>
        <author>
          <name>Frank Hampel</name>
          <email>hampel@stat.math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>foundations of statistics</keyword>
        <keyword>frequentist approach</keyword>
      </keywords>
      <abstract>A new statistical theory is outlined which builds a bridge between frequentist and Bayesian approaches and very naturally uses upper and lower probabilities. It started with an attempt to investigate how far one can get with a frequentist approach; this approach goes beyond the Neyman-Pearson and the Fisherian theory in explicitly using intersubjective epistemic upper and lower probabilities allowing an operational frequentist interpretation (not tied to repetitions of an experiment), and in deriving what is valid of Fisher's mostly misinterpreted fiducial probabilities as a very special case within a broader framework. It formally contains the Bayes theory as an extremal special case, but at the other extreme it also allows starting with the state of total ignorance about the parameter in an objective, frequentist learning process converging to the true model, thereby solving a problem of artificial intelligence (AI). The general theory describes (rather simila
 r) optimal compromises between frequentist and Bayesian approaches within (and outside) either framework, thus also providing a new class of ``least informative priors''. There is also a connection with information theory. Key concepts are ``successful bets'', more specifically ``least unfair successful bets'', ``cautious surprises'', and ``enforced fair bets'', including ``best enforced fair bets''. The main emphasis is on prediction. When going from inference to decisions, upper and lower probabilities (which avoid sure loss) are replaced by proper probabilities (which are coherent), somewhat analogous to Smets' pignistic transformation of belief functions. Much still needs to be done, but several examples for the binomial (the ``fundamental problem of practical statistics'') have been worked out, and there are also first (rather limited) solutions for continuous one-parameter situations, including their robustness problem.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s033.pdf</pdf>
    </paper>
    <paper>
      <id>054</id>
      <title>Constructing coherent models of conditional and unconditional upper probabilities</title>
      <authors>
        <author>
          <name>Hugo Janssen</name>
          <email>hugo.janssen@pi.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Leuven</name>
              <latitude>50.87959</latitude>
              <longitude>4.70093</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>dempster's conditioning rule</keyword>
        <keyword>possibility measure</keyword>
        <keyword>upper probability</keyword>
      </keywords>
      <abstract>We investigate the coherence of a possibilistic system modelled by the joint possibility distribution function of a finite sequence of linearly ordered possibilistic variables together with the conditional possibility distribution function of any variable in this sequence, given the values assumed by all preceding variables. To ensure the coherence of this model it is necessary and sufficient that the conditional possibilities are intermediate between those calculated from the joint possibility distribution function by Dempster's rule and the natural extension rule. We then show how a coherent model of conditional and unconditional upper probabilities can be constructed, using a given coherent model and a finitely additive probability. The method used to construct this model is to form convex combinations of the finitely additive probability and the upper probabilities belonging to the given model.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s054.ps</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Different faces of the natural extension</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>igor.kozine@risoe.dk</email>
          <location>
            <country>
              <code>DK</code>
              <name>Denmark</name>
            </country>
            <city>
              <name>Roskilde</name>
              <latitude>55.64152</latitude>
              <longitude>12.08035</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability theory</keyword>
        <keyword>imprecise reliability</keyword>
        <keyword>natural extension</keyword>
        <keyword>previsions</keyword>
      </keywords>
      <abstract>The natural extension, the key concept for the construction of coherent imprecise models, can appear in different equivalent forms. Each of them has pros and cons in the context of specific applications. The use of a proper form can substantially facilitate the inference and computation of the previsions of interest. The current paper concerns four forms of the natural extension representation. It is demonstrated that all of them are equivalent and one, discussed in the last instance, is prominent solely for gambles defined on continuous possibility sets. It is proven that the solution of the natural extension problem for continuous gambles exists on the degenerate distributions. Partial information and a characteristic to be calculated can be thought as the expectations of some real-valued functions defined on the possibility space. As they are expectations, they can be expressed as functions of probability density functions and proper real-valued functions (ga
 mbles). Each piece of the partial information acts as a constraint to the probability distributions. All together the constraints define the area of all distributions over which the interval of the desired statistical characteristic will be searched. Throughout the paper the natural extension is analyzed through the prism of re-liability application.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s014.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Computing the reliability of complex systems</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>igor.kozine@risoe.dk</email>
          <location>
            <country>
              <code>DK</code>
              <name>Denmark</name>
            </country>
            <city>
              <name>Roskilde</name>
              <latitude>55.64152</latitude>
              <longitude>12.08035</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability theory</keyword>
        <keyword>imprecise reliability</keyword>
        <keyword>natural extension</keyword>
        <keyword>previsions</keyword>
      </keywords>
      <abstract>Methods for computing the reliability of complex systems described in the current paper are grounded on partial information on system components. A tool for inferring the interval-valued models is the natural extension and the upper and lower bounds of the characteristics to be interpreted as coherent upper and lower previsions. A generic algorithm to find a solution of the natural extension in a practically affordable way braking down the general problem into problems that are much easier to solve is described. In general this can be made at the cost of a lesser precision in the previsions of interest. It is also shown that for some particular cases the genuine, minimally coherent, solutions can be found through the algorithm developed. The second part of the paper is devoted to those cases when the reliability of components constituting a system is represented by identical interval-valued reliability characteristics. That is, all the components are characteriz
 ed, for example, by probabilities to failure in the same time interval, or by mean times to failure or some others. Often namely these particular cases take place in reliability analysis practice. In this respect, based on the previous works by the authors of the current paper some new findings have been disclosed and new results obtained on particular practical cases.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s015.pdf</pdf>
    </paper>
    <paper>
      <id>042</id>
      <title>Coherent Lower Previsions as Exact Functionals and their (Sigma-)Core</title>
      <authors>
        <author>
          <name>Sebastian Maass</name>
          <email>Sebastian.Maass@email.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bremen</name>
              <latitude>53.08891</latitude>
              <longitude>8.79063</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>exact functionals</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>exact cooperative games</keyword>
        <keyword>core</keyword>
      </keywords>
      <abstract>Coherent lower previsions and exact cooperative games are mathematically essentially the same. We investigate in this paper the smallest class containing these functionals resp. games. This class will be denoted to consist of exact functionals which coincide with coherent lower previsions up to normalization. We investigate the exact functionals from a functional analytic point of view, i.e. we characterize this class by a norm, present a Hahn-Banach type theorem, a powerful construction method and adopt the concept of the core resp. sigma-core from cooperative game theory.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s042.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Imprecise Identification from Incomplete Data</title>
      <authors>
        <author>
          <name>Charles Manski</name>
          <email>cfmanski@northwestern.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Evanston</name>
              <latitude>42.04114</latitude>
              <longitude>-87.69006</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joel Horowitz</name>
          <email>joel-horowitz@uiowa.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Iowa City</name>
              <latitude>41.63421</latitude>
              <longitude>-91.49905</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>identification region</keyword>
        <keyword>interval data</keyword>
        <keyword>missing data</keyword>
        <keyword>nonparametric regression</keyword>
      </keywords>
      <abstract>An incomplete data problem arises when sample realizations are not fully observable: some realizations may be entirely or partially missing; some variables may be interval-measured. Whatever the specific form of the incomplete data problem, the generic consequence is imprecise identification of the population distribution generating the data. This paper describes completed and ongoing research showing how incomplete data problems lead to imprecise identification of regressions and of parameters solving extremum problems.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s016.pdf</pdf>
    </paper>
    <paper>
      <id>062</id>
      <title>Random correspondences as bundles of random variables</title>
      <authors>
        <author>
          <name>Massimo Marinacci</name>
          <email>massimo@econ.unito.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Torino</name>
              <latitude>45.05000</latitude>
              <longitude>7.66667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Adriana Castaldo</name>
          <email>acastald@unina.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Naples</name>
              <latitude>40.83333</latitude>
              <longitude>14.25000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>random set</keyword>
        <keyword>random correspondences</keyword>
        <keyword>capacity</keyword>
      </keywords>
      <abstract>We prove results that relate random correspondences with their measurable selections, thus providing a foundation for viewing random correspondences as "bundles" of random variables.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s062.pdf</pdf>
    </paper>
    <paper>
      <id>061</id>
      <title>Epistemic Irrelevance on  Sets of Desirable  Gambles</title>
      <authors>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>set of desirable gambles</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>graphoid axioms</keyword>
        <keyword>natural extension</keyword>
      </keywords>
      <abstract>This paper studies graphoid properties for epistemic irrelevance in sets of desirable gambles. For that aim, the basic operations of conditioning and marginalization are expressed in terms of variables. Then, it is shown that epistemic irrelevance is asymmetric graphoid. The intersection property is verified in probability theory when the global probability distribution is positive in all the values. Here it is always verified due to the handling of zero probabilities in sets of gambles. It is also presented an asymmetrical D-separation principle, by which this type of independence relationships can be represented in directed acyclic graphs.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s061.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>A Multiperiod Binomial Model for Pricing Options in an Uncertain World</title>
      <authors>
        <author>
          <name>Silvia Muzzioli</name>
          <email>s.muzzioli@unimo.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Modena</name>
              <latitude>44.64783</latitude>
              <longitude>10.92539</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Costanza Torricelli</name>
          <email>torricelli@unimo.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Modena</name>
              <latitude>44.64783</latitude>
              <longitude>10.92539</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>evidence theory</keyword>
        <keyword>fuzzy set</keyword>
        <keyword>options</keyword>
        <keyword>pricing</keyword>
      </keywords>
      <abstract>The aim of this paper is to price an option in a multiperiod binomial model, when there is uncertainty on the states of the world at each node of the tree. As a consequence, also the stock price at each state takes imprecise values. Possibility distributions are used to handle this type of problems. The pricing methodology is still based on a risk neutral valuation approach, but, as a consequence of the uncertainty on the two jumps of the stock, we obtain weighted intervals for risk-neutral probabilities. The distinctive feature of our model is that it tracks back the arising of these probability intervals to the imprecision of the value of the stock price in the up and down states. This paper provides a generalization of the standard binomial option pricing model. We obtain an expected value interval for the option price within which it is possible to find a crisp representative value and an index of the uncertainty present in the model.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s041.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>A Protocol for the Elicitation of Prior Distributions</title>
      <authors>
        <author>
          <name>Gertrudes Nadler Lins</name>
          <email>gertie@uol.com.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fernando Campello de Souza</name>
          <email>fmcs@elogica.com.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>elicitation</keyword>
        <keyword>uncertainty</keyword>
        <keyword>prior knowledge</keyword>
        <keyword>prior distribution</keyword>
        <keyword>expert opinion</keyword>
        <keyword>convex sets of probability measures</keyword>
        <keyword>vagueness</keyword>
      </keywords>
      <abstract>A practical way of eliciting convex sets of probability measures on a real continuous variable is introduced, one which mathematically defines vagueness and allows for its explicit treatment when it emerges from the activity of making inferences about a parameter based on available evidence through expert opinion. In the setup of the protocol, new indexes are introduced concerning the detailing of the questionnaire.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s009.pdf</pdf>
    </paper>
    <paper>
      <id>063</id>
      <title>Uncertainty aversion with second-order probabilities and utilities</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>risk aversion</keyword>
        <keyword>uncertainty aversion</keyword>
        <keyword>non-additive probabilities</keyword>
        <keyword>risk neutral probabilities</keyword>
      </keywords>
      <abstract>Aversion to uncertainty is commonly attributed to non-additivity of subjective probabilities for ambiguous events, as in the Choquet expected utility model. This paper shows that uncertainty aversion can be parsimoniously explained by a simple model of ?partially separable? non-expected utility preferences in which the decision maker satisfies the independence axiom selectively within partitions of the state space whose elements have similar degrees of uncertainty. As such, she may behave like an expected-utility maximizer with additive probabilities for assets in the same uncertainty class, while exhibiting higher degrees of risk aversion toward assets that are more uncertain. An alternative interpretation of the same model is that the decision maker may be uncertain about her credal state (represented by second-order probabilities for her first-order probabilities and utilities), and she may be averse to that uncertainty (represented by a second-order utility 
 function). The Ellsberg and Allais paradoxes are explained by way of illustration.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s063.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Interval Discriminant Analysis: An Efficient Method to Integrate Errors In Supervised Pattern Recognition</title>
      <authors>
        <author>
          <name>Philippe Nivlet</name>
          <email>philippe.nivlet@ifp.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Rueil-Malmaison</name>
              <latitude>48.87650</latitude>
              <longitude>2.18967</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frederique Fournier</name>
          <email>frederique.fournier@ifp.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Rueil-Malmaison</name>
              <latitude>48.87650</latitude>
              <longitude>2.18967</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jean-Jacques Royer</name>
          <email>Jean-Jacques.Royer@ensg.inpl-nancy.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Vandoeuvre</name>
              <latitude>48.65000</latitude>
              <longitude>6.18333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>discriminant analysis</keyword>
        <keyword>interval arithmetic</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>In a statistical pattern recognition context, probabilistic algorithms like -par ametric or nonparametric- discriminant analysis are designed to classify, when p ossible, objects into predefined classes. Because these methods require precise input data, they cannot propagate uncertainties in the classifying process. In r eal case studies, this could lead to drastic misinterpretations of objects. We h ave thus developed an extension of these methods to directly propagate imprecise interval-form data. The computations are based on interval arithmetic, which ap pears to be an efficient tool to handle intervals. They consist in calculating s uccessively interval conditional probability density functions and interval post erior probabilities, whose definitions are closely associated with the imprecise probability theory. The algorithms eventually assign any object to a subset of classes, consistent with the data and its uncertainties. The resulting classifyi ng model
  is thus less precise, but much more realistic than the standard one. Th e efficiency of this algorithm is tested on a synthetic case study.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s020.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>Fuzzy, probabilistic and stochastic modelling of an elastically bedded beam</title>
      <authors>
        <author>
          <name>Michael Oberguggenberger</name>
          <email>michael@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Francesco Russo</name>
          <email>russo@math.univ-paris16.fr3</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Villetaneuse</name>
              <latitude>48.95833</latitude>
              <longitude>2.34167</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy set</keyword>
        <keyword>random variables</keyword>
        <keyword>stochastic differential equation</keyword>
        <keyword>parameter uncertainty</keyword>
        <keyword>civil engineering models</keyword>
      </keywords>
      <abstract>This article sets out to compare the effects of modelling uncertainty using fuzzy sets, random variables and stochastic analysis. With the aid of an example from civil engineering - the bending equation for an elastically bedded beam - we discuss what each model is or is not capable of capturing. All models may be adequately used for variability studies, but may fail to detect the effect of localized parameter fluctuations on the response of the system. In the stochastics setting, we show an instance of the linearization effect of large noise which says that under large stochastic excitations, the contributions of nonlinear terms may be annihilated.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s023.ps</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>A note on the Dutch Book method</title>
      <authors>
        <author>
          <name>Jeff Paris</name>
          <email>jeff@maths.man.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Manchester</name>
              <latitude>53.48095</latitude>
              <longitude>-2.23743</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dutch book</keyword>
        <keyword>belief function</keyword>
        <keyword>uncertain reasoning</keyword>
      </keywords>
      <abstract>The paper presents a rather general proof of the so called Dutch Book argument and shows how it may also be applied to yield the corresponding analogs of probability functions for various non-classical propositional logics, for example modal, intuitionistic, and paraconsistent logics.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s002.ps</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>Coherent Risk Measures and Upper Previsions</title>
      <authors>
        <author>
          <name>Renato Pelessoni</name>
          <email>renatop@econ.univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolov@econ.univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent risk measure</keyword>
        <keyword>imprecise prevision</keyword>
        <keyword>value-at-risk</keyword>
        <keyword>avoiding sure loss condition</keyword>
      </keywords>
      <abstract>In this paper coherent risk measures and other currently used risk measures, notably Value-at-Risk (VaR), are studied from the perspective of the theory of coherent imprecise previsions. We show that coherent risk measures are a special case of coherent upper previsions and extend their definition and several properties to arbitrary sets of risks. We also prove that Value-at-Risk does not necessarily satisfy a weaker notion of coherence called Avoiding Sure Loss (ASL), and discuss both sufficient conditions for VaR to be ASL and ways of modifying VaR into a coherent risk measure.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s029.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>Graphical Models for Conditional Independence Structures</title>
      <authors>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>graphical model</keyword>
        <keyword>conditional independence</keyword>
        <keyword>lower probability</keyword>
        <keyword>separation criteria</keyword>
      </keywords>
      <abstract>In this paper we study conditional independence structures arising from conditional probabilities and lower conditional probabilities. Such models are based on notions of stochastic independence apt to manage also those situations where zero evaluations on possible events are present: this is particularly crucial for lower probability. The "graphoid" properties of such models are investigated, and the representation problem of conditional independence structures is dealt with by generalizing classical separation criteria for undirected and directed acyclic graphs.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s043.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>A partial solution of the possibilistic marginal problem</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>marginal problem</keyword>
        <keyword>possibility distribution</keyword>
        <keyword>triangular norm</keyword>
        <keyword>conditioning</keyword>
        <keyword>conditional independence</keyword>
        <keyword>extension</keyword>
      </keywords>
      <abstract>A possibilistic marginal problem will be introduced in a way analogous to probabilistic framework, to address the question of whether or not a common extension exists for a given set of marginal distributions. Similarities and differences between possibilistic and probabilistic marginal problems will be demonstrated, concerning necessary and sufficient conditions and sets of all solutions. Finally, the operators of composition will be introduced and we will show how to use them for finding T-product extension.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s038.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>Confidence as Higher-Order Uncertainty</title>
      <authors>
        <author>
          <name>Pei Wang</name>
          <email>peiwang@mindspring.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Bloomington</name>
              <latitude>39.16532</latitude>
              <longitude>-86.52639</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>confidence</keyword>
        <keyword>evidence</keyword>
        <keyword>frequency interval</keyword>
        <keyword>revision</keyword>
        <keyword>inference</keyword>
        <keyword>deduction</keyword>
        <keyword>induction</keyword>
        <keyword>abduction</keyword>
      </keywords>
      <abstract>With conflicting evidence, a reasoning system derives uncertain conclusions. If the system is open to new evidence, it faces additionally a higher-order uncertainty, because the first-order uncertainty evaluations are uncertain themselves --- they can be changed by future evidence. A new measurement, confidence, is introduced for this higher-order uncertainty. It is defined in terms of the amount of available evidence, and interpreted and processed as the relative stability of the first-order uncertainty evaluation. Its relation with other approaches of ``reasoning with uncertainty'' is also discussed.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s005.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>The status of F-indicator-fields within the theory of interval-probability</title>
      <authors>
        <author>
          <name>Kurt Weichselberger</name>
          <email>weichsel@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword/>
      </keywords>
      <abstract>In the theory of interval-probability the F-indicator-field of the random event A describes the information that A occurs and that nothing more is known. It will be shown how combined algebraic operations can be employed to trace back the set of F-fields to the set of F-indicator-fields. This is also a demonstration of mathematical analysis applied to the objects of the theory of interval-probability.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s049.ps</pdf>
    </paper>
    <paper>
      <id>057</id>
      <title>Modified upper and lower probabilities based on imprecise likelihoods</title>
      <authors>
        <author>
          <name>Nic Wilson</name>
          <email>nic.wilson@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>upper and lower probability</keyword>
        <keyword>imprecise likelihoods</keyword>
        <keyword>large sample behaviour</keyword>
        <keyword>bernouilli trials</keyword>
      </keywords>
      <abstract>The large sample behaviour is examined for upper and lower probabilities generated by precise priors (on a finite set) and imprecise likelihoods, for equally independently distributed variables taking values in a finite set. It can easily happen that the posterior upper probability of $\theta$ will tend to $1$ in situations where the relative frequencies are tending to a measure far removed from the set of conditional measures associated with $\theta$. A number of different modifications to upper and lower probability are made in such a way that the posterior probabilities will, with probability one, become or tend to $0$ in such situations; these all involve rejecting measures which are extremely implausible given the data, and hence considering more restricted sets of likelihoods.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s057.ps</pdf>
    </paper>
    <paper>
      <id>004</id>
      <title>Characterizing Fuzzy Measures Used in Uncertainty Representation</title>
      <authors>
        <author>
          <name>Ronald Yager</name>
          <email>yager@panix.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New Rochelle</name>
              <latitude>40.91757</latitude>
              <longitude>-73.78486</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy measure</keyword>
        <keyword>cardinality based measures</keyword>
        <keyword>entropy</keyword>
        <keyword>attitude</keyword>
      </keywords>
      <abstract/>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s004.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>Statistical inference of the naive credal classifier</title>
      <authors>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal classification</keyword>
        <keyword>classification</keyword>
        <keyword>naive credal classifier</keyword>
        <keyword>naive bayesian classifier</keyword>
        <keyword>credal set</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>inference</keyword>
        <keyword>missing data</keyword>
        <keyword>incomplete samples</keyword>
      </keywords>
      <abstract>In the wish list of the characteristics of a classifier, there are a reliable approach to small data sets and a clear and robust treatment of incomplete samples. This paper copes with such difficult problems by adopting the paradigm of credal classification. By exploiting Walley's imprecise Dirichlet model, it defines how to infer the naive credal classifier from a possibly incomplete multinomial sample. The derived procedure is exact and linear in the number of attributes. The obtained classifier is robust to small data sets and to all the possible missingness mechanisms. The results of some experimental analyses that compare the naive credal classifier with naive Bayesian models support the presented approach.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s035.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>Robust discovery of tree-dependency structures</title>
      <authors>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>tree dependencies</keyword>
        <keyword>inference</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>mutual information</keyword>
        <keyword>graphical model</keyword>
        <keyword>bayesian networks</keyword>
        <keyword>robustness</keyword>
        <keyword>multinomial distribution</keyword>
        <keyword>maximum spanning tree</keyword>
      </keywords>
      <abstract>The problem of inferring dependency structures from random samples is a very fundamental topic in artificial intelligence and statistics. This paper reviews an early result from Chow and Liu on the approximation of unknown multinomial distributions by tree-dependency distributions, at the light of imprecise probabilities. Imprecision, arising here from Walley's imprecise Dirichlet model, generally makes many tree structures be plausible given the data. This paper focuses on the inference of the substructure common to all the possible trees. Such common pattern is a set of reliable dependencies. The problem of identifying the common pattern is abstracted and solved here in the general context of graph algorithms. On this basis, an algorithm is developed that infers reliable dependencies in time $O(k^{3})$, from a set of $k$ binary random variables, that converge to a tree as the sample grows. The algorithm works by computing bounds on the solutions of global opti
 mization problems. There are a number of reasons why trees are a very important special case of dependence graphs. This work appears as a significant step in the direction of discovering dependency structures under the realistic assumption of imprecise knowledge.</abstract>
      <pdf>ftp://decsai.ugr.es/pub/utai/other/smc/isipta01/s037.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2003</year>
    <conference>
      <date>
        <start>2003-07-14</start>
        <end>2003-07-17</end>
      </date>
      <location>
        <country>
          <code>CH</code>
          <name>Switzerland</name>
        </country>
        <city>
          <name>Lugano</name>
          <latitude>46.01008</latitude>
          <longitude>8.96004</longitude>
        </city>
        <university>
          <name>University of Lugano</name>
          <department/>
        </university>
      </location>
    </conference>
    <paper>
      <id>040</id>
      <title>Maximum of Entropy in Credal Classification</title>
      <authors>
        <author>
          <name>Joaquin Abellan</name>
          <email>jabemu@teleline.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>uncertainty</keyword>
        <keyword>maximum entropy</keyword>
        <keyword>imprecision</keyword>
        <keyword>non-specificity</keyword>
        <keyword>classification</keyword>
        <keyword>classification trees</keyword>
        <keyword>credal set</keyword>
      </keywords>
      <abstract>We present one application of the measure of maximum entropy for credal sets: as a total uncertainty measure to branch classification trees based on imprecise probabilities. In this paper we justify the use of maximum entropy as a global uncertainty measure for credal sets. A deduction of this measure, based on the best lower expectation of the logarithmic score is presented. We have also carried out several experiments in which credal classification trees are built taking a global uncertainty measure as basis. The results show that the error is lower when the maximum entropy is used as global uncertainty measure.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/002.pdf</pdf>
    </paper>
    <paper>
      <id>051</id>
      <title>On the Suboptimality of the Generalized Bayes Rule and Robust Bayesian Procedures from the Decision Theoretic Point of View: a Cautionary Note on Updating Imprecise Priors</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>generalized risk</keyword>
        <keyword>generalized expected loss</keyword>
        <keyword>imprecise prior risk and posterior loss</keyword>
        <keyword>robust bayesian analysis</keyword>
        <keyword>generalized bayes rule</keyword>
      </keywords>
      <abstract>This paper discusses fundamental aspects of inference with imprecise probabilities from the decision theoretic point of view. It is shown why the equivalence of prior risk and posterior loss, well known from classical Bayesian statistics, is no longer valid under imprecise priors. As a consequence, straightforward updating, as suggested by Walley's Generalized Bayes Rule or as usually done in the Robust Bayesian setting, may lead to suboptimal decision functions. As a result, it must be warned that, in the framework of imprecise probabilities, updating and optimal decision making do no longer coincide.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/004.pdf</pdf>
    </paper>
    <paper>
      <id>059</id>
      <title>Analysis of Local or Asymmetric Dependencies in Contingency Tables using the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Jean-Marc Bernard</name>
          <email>jmbernard@psycho.univ-paris5.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>contingency tables</keyword>
        <keyword>association</keyword>
        <keyword>logical model</keyword>
        <keyword>directional association model</keyword>
        <keyword>statistical inference</keyword>
        <keyword>upper and lower probability</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>prior ignorance</keyword>
        <keyword>bayesian inference</keyword>
      </keywords>
      <abstract>We consider the statistical problem of analyzing the association between two categorical variables from cross-classified data. The focus is put on measures which enable one to study the possible dependencies at a local level and to assess whether the data support some more or less strong association model. Statistical inference is envisaged using an imprecise Dirichlet model; this model answers several difficulties of frequentist and Bayesian methods.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/005.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>Some results on generalized coherence of conditional probability bounds</title>
      <authors>
        <author>
          <name>Veronica Biazzo</name>
          <email>vbiazzo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuseppe Sanfilippo</name>
          <email>gsanfilippo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertain knowledge</keyword>
        <keyword>coherence</keyword>
        <keyword>g-coherence</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>conditional probability bounds</keyword>
        <keyword>lower and upper probabilities</keyword>
        <keyword>non relevant gains</keyword>
        <keyword>basic sets</keyword>
      </keywords>
      <abstract>Based on the coherence principle of de Finetti and a related notion of generalized coherence (g-coherence), we adopt a probabilistic approach to uncertainty based on conditional probability bounds. Our notion of g-coherence is equivalent to the "avoiding uniform loss" property for lower and upper probabilities (a la Walley). Moreover, given a g-coherent imprecise assessment by our algorithms we can correct it obtaining the associated coherent assessment (in the sense of Walley and Williams). As is well known, the problem of checking g-coherence and/or propagating conditional probability bounds has, in general, an exponential complexity. Two notions which may be helpful to reduce computational effort are those of non relevant gain and basic set. Exploiting them, our algorithms can use linear systems with reduced sets of variables and/or linear constraints. In this paper we give some insights on the notions of non relevant gain and basic set. We consider several families with three conditional events, obtaining some results characterizing g-coherence in such cases. We also give some more general results.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/006.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>The Maximal Variation of Fuzzy Interval</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibility measure</keyword>
        <keyword>upper and lower probability</keyword>
        <keyword>maximal variation</keyword>
      </keywords>
      <abstract>The paper gives the solution of calculating maximal variation of fuzzy interval in the scope of the theory of imprecise probabilities. As it appears, this problem is more difficult than analogous one connected with evaluation of lower and upper expectations of fuzzy interval. This paper gives some contribution to possibility theory in the framework of probability approach.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/007.pdf</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>Inter-personal communication of precise and imprecise subjective probabilities</title>
      <authors>
        <author>
          <name>David Budescu</name>
          <email>dbudescu@s.psych.uiuc.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Urbana-Champaign</name>
              <latitude>40.11059</latitude>
              <longitude>-88.20727</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Tzur Karelitz</name>
          <email>karelitz@s.psych.uiuc.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Urbana-Champaign</name>
              <latitude>40.11059</latitude>
              <longitude>-88.20727</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>subjective probability</keyword>
        <keyword>judgement</keyword>
        <keyword>verbal probabilities</keyword>
        <keyword>linguistic probabilities</keyword>
        <keyword>inter-personal conversion</keyword>
      </keywords>
      <abstract/>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/008.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Relevance of Qualitative Constraints in Diagnostic Processes</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent inference</keyword>
        <keyword>conditional exchangeability</keyword>
        <keyword>qualitative constraints</keyword>
        <keyword>diagnosis procedures</keyword>
      </keywords>
      <abstract>This paper reviews recent results obtained in the medical diagnosis field by adding to a coherent inference process qualitative constraints. Such further considerations turn out to be significant whenever a basic lower-upper conditional probability assessment induces extension bounds too vague to take any decision. Three general types of qualitative judgements are proposed and fully described. They do not constitute a ``panacea" to solve any problematic situation, but their application can considerably improve inferences results in specific fields, as two practical applications show.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/009.pdf</pdf>
    </paper>
    <paper>
      <id>032</id>
      <title>Combining Belief Functions Issued from Dependent Sources</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>propositional logic</keyword>
        <keyword>combination</keyword>
        <keyword>dempster's rule of combination</keyword>
        <keyword>independence</keyword>
        <keyword>conflict</keyword>
        <keyword>monotonicity</keyword>
        <keyword>non-specificity</keyword>
        <keyword>idempotency</keyword>
        <keyword>associativity</keyword>
        <keyword>bayes' theorem</keyword>
      </keywords>
      <abstract>Dempster's rule for combining two belief functions assumes the independence of the sources of information. If this assumption is questionable, I suggest to use the least specific combination minimizing the conflict among the ones allowed by a simple generalization of Dempster's rule. This increases the monotonicity of the reasoning and helps us to manage situations of dependence. Some properties of this combination rule and its usefulness in a generalization of Bayes' theorem are then considered.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/011.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Nonparametric predictive comparison of two groups of lifetime data</title>
      <authors>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ke-Jian Yan</name>
          <email>K.J.Yan@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>censored data</keyword>
        <keyword>exchangeability</keyword>
        <keyword>nonparametrics</keyword>
        <keyword>prediction</keyword>
        <keyword>survival analysis</keyword>
      </keywords>
      <abstract>This paper presents the application of a recently introduced nonparametric predictive inferential method to compare two groups of data, consisting of observed event times and right-censoring times. Comparison is based on imprecise probabilities concerning one future observation per group.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/012.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Computing lower expectations with Kuznetsov's independence condition</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>sets of probability distributions</keyword>
        <keyword>lower expectation</keyword>
        <keyword>expectation intervals</keyword>
        <keyword>independence concepts</keyword>
      </keywords>
      <abstract>Kuznetsov's condition says that variables X and Y are independent when any product of bounded functions f(X) and g(Y) behaves in a certain way: the interval of expected values E[f(X)g(Y)] must be equal to the interval product E[f(X)] . E[g(Y)]. The main result of this paper shows how to compute lower expectations using Kuznetsov's condition. We also generalize Kuznetsov's condition to conditional expectation intervals, and study the relationship between Kuznetsov's conditional condition and the semi-graphoid properties.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/014.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>Geometry of upper probabilities</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>cuzzolin@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>theory of evidence</keyword>
        <keyword>belief space</keyword>
        <keyword>basic plausibility assignment</keyword>
        <keyword>plausibility space</keyword>
        <keyword>orthogonal projection</keyword>
      </keywords>
      <abstract>In this paper we adopt the geometric approach to the theory of evidence to study the geometric counterparts of the plausibility functions, or upper probabilities. The computation of the coordinate change between the two natural reference frames in the belief space allows us to introduce the dual notion of basic plausibility assignment and understand its relation with the classical basic probability assignment. The convex shape of the plausibility space $\Pi$ is recovered in analogy to what done for the belief space, and the pointwise geometric relation between a belief function and the corresponding plausibility vector is discussed. The orthogonal projection of an arbitrary belief function $s$ onto the probabilistic subspace is computed and compared with other significant entities, such as the relative plausibility and mean probability vectors.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/015.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Convenient interactive computing for coherent imprecise prevision assessment</title>
      <authors>
        <author>
          <name>James Dickey</name>
          <email>dickey@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>assessment</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>imprecise prevision</keyword>
        <keyword>coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>interactive computing</keyword>
      </keywords>
      <abstract>A generalization of deFinetti's Fundamental Theorem of Probability facilitates coherent assessment by iterated natural extension of imprecise probabilities or expectations, conditional and unconditional. Point values are generalized to assessed bounds accepted under weak coherence, allowing the input of weakened bounds. The method is realized in a convenient interactive computer program, demonstrated here and made available as open source code.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/017.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Independence with Respect to Upper and Lower Conditional Probabilities Assigned by Hausdorff Outer and Inner Measures</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>upper and lower conditional probabilities</keyword>
        <keyword>hausdorff measures</keyword>
        <keyword>disintegration property</keyword>
        <keyword>independence</keyword>
      </keywords>
      <abstract>Upper and lower conditional probabilities assigned by Hausdorff outer and inner measures are given; they are natural extensions to the class of all subsets of omega=[0,1] of finitely additive probabilities, in the sense of Dubins, assigned by a class of Hausdorff measures. A strong disintegration property is introduced when conditional probability is defined by a class of Hausdorff dimensional measures. Moreover the definitions of s-independence and s-irrelevance are given to assure that logical independence is a necessary condition of independence. The interpretation of commensurable events, in the sense of de Finetti, as sets with finite and positive Hausdorff measure and with the same Hausdorff dimension is proposed.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/018.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Towards a Chaotic Probability Model for Frequentist Probability: The Univariate Case</title>
      <authors>
        <author>
          <name>Pablo Fierens</name>
          <email>pifierens@ece.cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Terrence Fine</name>
          <email>tlfine@ece.cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>sets of measures</keyword>
        <keyword>objective</keyword>
        <keyword>frequentist interpretation</keyword>
      </keywords>
      <abstract>We adopt the same mathematical model of a set M of probability measures as is central to the theory of coherent imprecise probability. However, we endow this model with an objective, frequentist interpretation in place of a behavioral subjective one. We seek to use M to model stable physical sources of time series data that have highly irregular behavior and not to model states of belief or knowledge that are assuredly imprecise. The approach we present in this paper is to understand a set of measures model M not as a traditional compound hypothesis, in which one of the measures in M is a true description, but rather as one in which none of the individual measures in M provides an adequate description of the potential behavior of the physical source as actualized in the form of a long time series. We provide an instrumental construction of random process measures consistent with M and the highly irregular physical phenomena we intend to model by M. This construction provides us with the basic tools for simulation of our models. We present a method to estimate M from data which studies any given data sequence by analyzing it into subsequences selected by a set of computable rules. We prove results that help us to choose an adequate set of rules and evaluate the performance of the estimator.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/019.pdf</pdf>
    </paper>
    <paper>
      <id>054</id>
      <title>Subjective Probability and Lower and Upper Prevision: A New Understanding</title>
      <authors>
        <author>
          <name>Peter Gillett</name>
          <email>gillett@business.rutgers.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Piscataway</name>
              <latitude>40.55293</latitude>
              <longitude>-74.45775</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Glenn Shafer</name>
          <email>gshafer@andromeda.rutgers.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Newark</name>
              <latitude>40.73566</latitude>
              <longitude>-74.17237</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Richard Scherl</name>
          <email>rscherl@monmouth.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>West Long Branch</name>
              <latitude>40.28351</latitude>
              <longitude>-74.01828</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>subjective probability</keyword>
        <keyword>upper and lower prevision</keyword>
        <keyword>updating</keyword>
        <keyword>event tree</keyword>
      </keywords>
      <abstract>This article introduces a new way of understanding subjective probability and its generalization to lower and upper prevision. Instead of asking whether a person is willing to pay given prices for given risky payoffs, we ask whether the person believes he can make a lot of money at those prices. If not---if the person is convinced that no strategy for exploiting the prices can make him very rich in the long run---then the prices measure his subjective uncertainty about the events involved. This new understanding justifies Peter Walley's updating principle, which applies when new information is anticipated exactly. It also justifies a weaker principle that is more useful for planning because it applies even when new information is not anticipated exactly. This weaker principle can serve as a basis for flexible probabilistic planning in event trees.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/038.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Bounding the risk of lung cancer attributed to other environmental pollutants</title>
      <authors>
        <author>
          <name>Minh Ha-Duong</name>
          <email>minh.ha.duong@cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Elizabeth Casman</name>
          <email>casman@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Granger Morgan</name>
          <email>granger.morgan@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lung cancer</keyword>
        <keyword>risk</keyword>
        <keyword>transferable belief model</keyword>
        <keyword>environmental pollutant</keyword>
      </keywords>
      <abstract>This discussion paper applies bounding analysis to environmental risk analysis as suggested in Morgan (2001) "The neglected art of bounding analysis" Environ Sci Technol. 35(7):162A-164A. The goal is to bound the fraction of lung cancer occurrences not attributed to specific well-studied causes, in order to keep estimates of the less well delimited risks consistent with known risks. Available data and expert judgment are used to attribute a portion of the observed lung cancer cases to known causes such as smoking, residential radon and asbestos exposure, to describe the uncertainty surrounding these estimates, and quantify the interaction between pollutants. Then an upper bound on the risk related to diesel particulates is inferred using a coherence constraint on the total number of deaths and a principle of maximum plausibility, a concept from the field of imprecise probabilities.</abstract>
      <pdf/>
    </paper>
    <paper>
      <id>017</id>
      <title>Bayesian Robustness with Quantile Loss Functions</title>
      <authors>
        <author>
          <name>Javier Hernandez</name>
          <email>javierhs@materiales.unex.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Badajoz</name>
              <latitude>38.87789</latitude>
              <longitude>-6.97061</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jacinto Martin</name>
          <email>jrmartin@unex.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Caceres</name>
              <latitude>39.47649</latitude>
              <longitude>-6.37224</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jose Arias</name>
          <email>jparias@unex.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Caceres</name>
              <latitude>39.47649</latitude>
              <longitude>-6.37224</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alfonso Suarez</name>
          <email>alfonso.suarez@uca.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Cadiz</name>
              <latitude>36.53361</latitude>
              <longitude>-6.29944</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian robustness</keyword>
        <keyword>non-dominated alternatives</keyword>
        <keyword>bayes alternatives</keyword>
        <keyword>quantile loss functions</keyword>
        <keyword>stochastic orders</keyword>
        <keyword>quantile class of prior distributions</keyword>
        <keyword>band probability class </keyword>
      </keywords>
      <abstract>Bayes decision problems require subjective elicitation of the inputs: beliefs and preferences. Sometimes, elicitation methods may not perfectly represent the Decision Maker's judgements. Several foundations propose to overlay this problem using robust approaches. In these models, beliefs are modelled by a class of probability distributions and preferences by a class of loss functions. Thus, the solution concept is the set of non-dominated alternatives. In this paper we focus on the computation of the efficient set when the preferences are modelled by a class of convex loss functions, specifically the quantile loss functions. We illustrate the idea with examples and introduce the use of stochastic dominance in this context.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/003.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>Robust Estimators under the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Marcus Hutter</name>
          <email>marcus@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>exact, conservative, approximate, robust confidence interval estimates</keyword>
        <keyword>entropy</keyword>
        <keyword>mutual information</keyword>
      </keywords>
      <abstract>Walley's Imprecise Dirichlet Model (IDM) for categorical data overcomes several fundamental problems which other approaches to uncertainty suffer from. Yet, to be useful in practice, one needs efficient ways for computing the imprecise=robust sets or intervals. The main objective of this work is to derive exact, conservative, and approximate, robust and credible interval estimates under the IDM for a large class of statistical estimators, including the entropy and mutual information.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/021.pdf</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>How to deal with incomplete acts? A proposal</title>
      <authors>
        <author>
          <name>Jean-Yves Jaffray</name>
          <email>Jean-Yves.Jaffray@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Meglena Jeleva</name>
          <email>jeleva@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making under uncertainty</keyword>
        <keyword>partially analyzed decision</keyword>
      </keywords>
      <abstract>In some situations, a decision is best represented by an incompletely analyzed act: conditionally to a certain event, the consequences of the decision on sub-events are perfectly known and uncertainty becomes expressable through probabilities, whereas the plausibility of this event itself remains vague and the decision outcome on the complementary event is imprecisely known. In this framework, we study an axiomatic decision model and prove a representation theorem. Decision criteria must aggregate partial evaluations consisting in: i) the conditional expected utility associated with the analyzed part of the decision and ii) the best and worst outcomes of its non-analyzed part.</abstract>
      <pdf/>
    </paper>
    <paper>
      <id>018</id>
      <title>On approximating multidimensional probability distributions by compositional models</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multidimensional distributions</keyword>
        <keyword>approximation</keyword>
        <keyword>conditional independence</keyword>
        <keyword>operator of composition</keyword>
      </keywords>
      <abstract>Because of computational problems, multidimensional probability distributions must be approximated by distributions which can be defined by a reasonable number of parameters. As a rule, distributions with a special dependence structure (i.e., complying with a system of conditional independence relations) are considered; graphical Markov models and especially Bayesian networks are often used. This paper proposes application of compositional models for this puropose. In addition to a theoretical background, a heuristic algorithm is presented. Its basic idea, construction of an approximation exploiting informational content of given low-dimensional distributions in a maximal possible way, was proposed by Albert Perez as early as in 1977.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/023.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>An Update on Generalized Information Theory</title>
      <authors>
        <author>
          <name>George Klir</name>
          <email>gklir@binghamton.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Binghamton</name>
              <latitude>42.09869</latitude>
              <longitude>-75.91797</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertainty</keyword>
        <keyword>uncertainty based information</keyword>
        <keyword>generalized information theory</keyword>
      </keywords>
      <abstract>The purpose of this paper is to survey recent developments and trends in the area of generalized information theory (GIT) and to discuss some of the issues of current interest in GIT regarding the measurement of uncertainty-based information for imprecise probabilities on finite crisp sets.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/024.pdf</pdf>
    </paper>
    <paper>
      <id>036</id>
      <title>Reducing Uncertainty by Imprecise Judgements on Probability Distributions: Application to System Reliability</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>igor.kozine@risoe.dk</email>
          <location>
            <country>
              <code>DK</code>
              <name>Denmark</name>
            </country>
            <city>
              <name>Roskilde</name>
              <latitude>55.64152</latitude>
              <longitude>12.08035</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Victor Krymsky</name>
          <email>kvg@mail.rb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Ufa</name>
              <latitude>54.78517</latitude>
              <longitude>56.04562</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>probability density function</keyword>
        <keyword>reliability</keyword>
      </keywords>
      <abstract>In this paper the judgement consisting in choosing a function that is believed to dominate the true probability distribution of a continuous random variable. This kind of judgement can significantly increase precision in constructed imprecise previsions of interest, which is of great importance for applications. New formulae for computing system reliability are derived on the basis of the technique developed.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/025.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Climate projections for the 21st century using random sets</title>
      <authors>
        <author>
          <name>Elmar Kriegler</name>
          <email>kriegler@pik-potsdam.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Potsdam</name>
              <latitude>52.39886</latitude>
              <longitude>13.06566</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Hermann Held</name>
          <email>held@pik-potsdam.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Potsdam</name>
              <latitude>52.39886</latitude>
              <longitude>13.06566</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>climate change</keyword>
        <keyword>climate sensitivity</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>random set</keyword>
        <keyword>belief function</keyword>
      </keywords>
      <abstract>We apply random set theory to an analysis of future climate change. Bounds on cumulative probability are used to quantify uncertainties in natural and socio-economic factors that influence estimates of global mean temperature. We explore the link of random sets to lower envelopes of probability families bounded by cumulative probability intervals. By exploiting this link, a random set for a simple climate change model is constructed, and projected onto an estimate of global mean warming in the 21st century. Results show that warming estimates on this basis can generate very imprecise uncertainty models.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/026.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>The DecideIT Decision Tool</title>
      <authors>
        <author>
          <name>Aron Larsson</name>
          <email>aron.larsson@mhs.studit.com</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Sundsvall</name>
              <latitude>62.39129</latitude>
              <longitude>17.30630</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mats Danielson</name>
          <email>mad@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Love Ekenberg</name>
          <email>lovek@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jim Johansson</name>
          <email>jim.johansson@mh.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Sundsvall</name>
              <latitude>62.39129</latitude>
              <longitude>17.30630</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision analysis</keyword>
        <keyword>interval probability</keyword>
        <keyword>utility theory</keyword>
        <keyword>decision tools</keyword>
      </keywords>
      <abstract>The nature of much information available to decision makers is vague and imprecise, be it information for human managers in organisations or for process agents in a distributed computer environment. Several models for handling vague and imprecise information in decision situations have been suggested. In particular, various interval methods have prevailed, i.e. methods based on interval estimates of probabilities and, in some cases, interval utility estimates. Even if these approaches in general are well founded, little has been done to take into consideration the evaluation perspective and, in particular, computational aspects and implementation issues. The purpose of this paper is to demonstrate a tool for handling imprecise information in decision situations. The tool is an implementation of our earlier research focussing on finding fast algorithms for solving bilinear systems of equations together with a graphical user interface supporting the interpretation of evaluations of imprecise data.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/016.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>Exploring a Collection of Priors Arising from an Imprecise Probability Assessment Based on Linear Constraints</title>
      <authors>
        <author>
          <name>Radu Lazar</name>
          <email>lazar@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Glen Meeden</name>
          <email>glen@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>linear constraint</keyword>
        <keyword>probability assessment</keyword>
        <keyword>bayesian inference</keyword>
        <keyword>metropolis-hastings algorithm</keyword>
      </keywords>
      <abstract>Consider the situation where the available prior informa-tion is only sufficient to identify a class of possible prior dis-tributions. In such cases it would be of interest to be able to explore the behavior of functions defined on this class. Here we develop a method based on the Metropolis-Hastings al-gorithm that allows one to investigate an imprecise prior as-sessment based on linear constraints.</abstract>
      <pdf/>
    </paper>
    <paper>
      <id>052</id>
      <title>Continuous Linear Representation of Coherent Lower Previsions</title>
      <authors>
        <author>
          <name>Sebastian Maass</name>
          <email>Sebastian.Maass@web.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bremen</name>
              <latitude>53.08891</latitude>
              <longitude>8.79063</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower prevision</keyword>
        <keyword>moebius transform</keyword>
        <keyword>choquet's theorem</keyword>
        <keyword>bishop-de leeuw theorem</keyword>
        <keyword>dempster-shafer-shapley representation theorem</keyword>
      </keywords>
      <abstract>This paper studies the possibility of representing lower previsions by continuous linear functionals. We prove the existence of a linear isomorphism between the linear space spanned by the coherent lower previsions and that of an appropriate space of continuous linear functionals. Moreover, we show that a lower prevision is coherent if and only if its transform is monotone. We also discuss the interpretation of these results and the new light they shed on the theory of imprecise probabilities.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/028.pdf</pdf>
    </paper>
    <paper>
      <id>055</id>
      <title>Expected Utility with Multiple Priors</title>
      <authors>
        <author>
          <name>Fabio Maccheroni</name>
          <email>fabio.maccheroni@uni-bocconi.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Milan</name>
              <latitude>45.46427</latitude>
              <longitude>9.18951</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Massimo Marinacci</name>
          <email>massimo@econ.unito.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Torino</name>
              <latitude>45.05000</latitude>
              <longitude>7.66667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Erio Castagnoli</name>
          <email>erio.castagnoli@uni-bocconi.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Milan</name>
              <latitude>45.46427</latitude>
              <longitude>9.18951</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>preference representation</keyword>
        <keyword>subjective probability</keyword>
        <keyword>nonexpected utility</keyword>
        <keyword>integral representation</keyword>
        <keyword>multiple priors</keyword>
      </keywords>
      <abstract>A preference relation on a convex set F is considered. Necessary and sufficient conditions are given that guarantee the existence of a set of affine utility functions { u_i } on F such that the preference relation is represented by U( f ) = u_i ( f ) if f belongs to F_i where each F_i is a convex subset of F. The interpretation is simple: facing a ``non-homogeneous'' set of alternatives F, a decision maker splits it into ``homogeneous'' subsets F_i, and acts as a standard expected utility maximizer on each of them. In particular, when F is a set of simple acts, each u_i corresponds to a subjective expected utility with respect to a finitely additive probability P_i ; while when F is a set of continuous acts, each probability P_i is countably additive.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/010.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>Study of the probabilistic information of a random set</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>emiranda@correo.uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ines Couso</name>
          <email>couso@pinon.ccu.uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Pedro Gil</name>
          <email>pedro@pinon.ccu.uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>random set</keyword>
        <keyword>upper and lower probability</keyword>
        <keyword>measurable selections</keyword>
        <keyword>choquet integral</keyword>
      </keywords>
      <abstract>Given a random set coming from the imprecise observation of a random variable, we study how to model the information about the distribution of this random variable. Specifically, we investigate whether the information given by the upper and lower probabilities induced by the random set is equivalent to the one given by the class of the distributions of the measurable selections; together with sufficient conditions for this, we also give examples showing that they are not equivalent in all cases.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/029.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>An Extended Set-valued Kalman Filter</title>
      <authors>
        <author>
          <name>Darryl Morrell</name>
          <email>morrell@asu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Tempe</name>
              <latitude>33.40140</latitude>
              <longitude>-111.93130</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Wynn Stirling</name>
          <email>wynn@ee.byu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Provo</name>
              <latitude>40.23384</latitude>
              <longitude>-111.65853</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>statistical inference</keyword>
        <keyword>dynamic systems</keyword>
        <keyword>convex sets of probability measures</keyword>
        <keyword>set-valued estimation</keyword>
      </keywords>
      <abstract>Set-valued estimation offers a way to account for imprecise knowledge of the prior distribution of a Bayesian statistical inference problem. The set-valued Kalman filter, which propagates a set of conditional means corresponding to a convex set of conditional probability distributions of the state of a linear dynamic system, is a general solution for linear Gaussian dynamic systems. In this paper, the set-valued Kalman filter is extended to the non-linear case by approximating the non-linear model with a linear model that is chosen to minimize the error between the non-linear dynamics and observation models and the linear approximation. An application is presented to illustrate and interpret the estimator results.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/030.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>The Shape of Incomplete Preferences</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probabilities and utilities</keyword>
        <keyword>state-dependent utility</keyword>
      </keywords>
      <abstract>The emergence of robustness as an important consideration in Bayesian statistical models has led to a renewed interest in normative models of incomplete preferences represented by imprecise (set-valued) probabilities and utilities. This paper presents a simple axiomatization of incomplete preferences and characterizes the shape of their representing sets of probabilities and utilities. Deletion of the completeness assumption from the axiom system of Anscombe and Aumann yields preferences represented by a convex set of state-dependent expected utilities, of which at least one must be a probability/utility pair. A strengthening of the state-independence axiom is needed to obtain a representation purely in terms of a set of probability/utility pairs.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/031.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Convex imprecise previsions: basic issues and applications</title>
      <authors>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolo.vicig@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise prevision</keyword>
        <keyword>convex imprecise previsions</keyword>
        <keyword>convex natural extension</keyword>
        <keyword>risk measures</keyword>
      </keywords>
      <abstract>In this paper we study two classes of imprecise previsions, which we termed convex and centered convex previsions, in the framework of Walley's theory of imprecise previsions. We show that convex previsions are related with a concept of convex natural estension, which is useful in correcting a large class of inconsistent imprecise probability assessments. This class is characterised by a condition of avoiding unbounded sure loss. Convexity further provides a conceptual framework for some uncertainty models and devices, like unnormalised supremum preserving functions. Centered convex previsions are intermediate between coherent previsions and previsions avoiding sure loss, and their not requiring positive homogeneity is a relevant feature for potential applications. Finally, we show how these concepts can be applied in (financial) risk measurement.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/032.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Reliability analysis in geotechnics with finite elements - comparison of probabilistic, stochastic and fuzzy set methods</title>
      <authors>
        <author>
          <name>Gerd Peschl</name>
          <email>peschl@tugraz.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Graz</name>
              <latitude>47.06667</latitude>
              <longitude>15.45000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Helmut Schweiger</name>
          <email>helmut.schweiger@tugraz.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Graz</name>
              <latitude>47.06667</latitude>
              <longitude>15.45000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>finite element method</keyword>
        <keyword>probabilistic</keyword>
        <keyword>fuzzy set</keyword>
        <keyword>stochastic modelling</keyword>
        <keyword>random field</keyword>
        <keyword>spatial correlation</keyword>
      </keywords>
      <abstract>The finite element method is widely used for solving various problems in geotechnical engineering practice. The input parameters required for the calculations are generally imprecise. The paper is devoted to a comparison of probabilistic, stochastic and fuzzy set method for reliability analysis with respect to its applicability for practical problems in geotechnical engineering. Emphasis will be given by comparing the effects of modelling uncertainty using different methods, with special reference to the role of spatial correlation. After introducing some basic notions about the approaches, this article shows that the results obtained with the fuzzy set method for a simple bearing capacity problem agree with the outcomes by a probabilistic and a stochastic method. Advantages and shortcomings of either approach with respect to practical applications will be discussed.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/033.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Game-Theoretic Learning Using the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>Erik.Quaeghebeur@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>game theory</keyword>
        <keyword>fictitious play</keyword>
        <keyword>equilibria</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>learning</keyword>
      </keywords>
      <abstract>We discuss two approaches for choosing a strategy in a two-player game. We suppose that the game is played a large number of rounds, which allows the players to use observations of past play to guide them in choosing a strategy. Central in these approaches is the way the opponent's next strategy is assessed; both a precise and an imprecise Dirichlet model are used. The observations of the opponent's past strategies can then be used to update the model and obtain new assessments. To some extent, the imprecise probability approach allows us to avoid making arbitrary initial assessments. To be able to choose a strategy, the assessment of the opponent's strategy is combined with rules for selecting an optimal response to it: a so-called best response or a maximin strategy. Together with the updating procedure, this allows us to choose strategies for all the rounds of the game. The resulting playing sequence can then be analysed to investigate if the strategy choices can converge to equilibria.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/034.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>A Sensitivity Analysis for the Pricing of Call Options in a Binary Tree Model</title>
      <authors>
        <author>
          <name>Huguette Reynaerts</name>
          <email>huguette.reynaerts@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michele Vanmaele</name>
          <email>michele.vanmaele@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy set</keyword>
        <keyword>option pricing</keyword>
        <keyword>sensitivity analysis</keyword>
      </keywords>
      <abstract>The European call option prices have well-known formulae in the Cox-Ross-Rubinstein model, depending on the volatility of the underlying asset. Nevertheless it is hard to give a precise estimate of this volatility. S. Muzzioli and C. Toricelli handle this problem by using possibility distributions. In the first part of our paper we make some critical comments on their work. In the second part we present an alternative solution to the problem by performing a sensitivity analysis for the pricing of the option. This method is very general in the sense that it can be applied if one describes the uncertainty in the volatility by confidence intervals as well as if one describes it by fuzzy numbers. The conclusion is that the price of the option is not necessarily a strictly increasing function of the volatility.</abstract>
      <pdf/>
    </paper>
    <paper>
      <id>013</id>
      <title>Inference in Credal Networks with Branch-and-Bound Algorithms</title>
      <authors>
        <author>
          <name>Jose Rocha</name>
          <email>jrocha@uepg.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Ponta Grossa</name>
              <latitude>-25.09500</latitude>
              <longitude>-50.16194</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>strong independence</keyword>
        <keyword>probability intervals</keyword>
        <keyword>inference</keyword>
        <keyword>branch-and-bound algorithms</keyword>
      </keywords>
      <abstract>A credal network associates sets of probability distributions with directed acyclic graphs. Under strong independence assumptions, inference with credal networks is equivalent to a signomial program under linear constraints, a problem that is NP-hard even for categorical variables and polytree models. We describe an approach for inference with polytrees that is based on branch-and-bound optimization/search algorithms. We use bounds generated by Tessem's A/R algorithm, and consider various branch-and-bound schemes.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/036.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>Extensions of Expected Utility Theory and some Limitations of Pairwise Comparisons</title>
      <authors>
        <author>
          <name>Mark Schervish</name>
          <email>mark@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Isaac Levi</name>
          <email>levi@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayes admissible</keyword>
        <keyword>e-admissibility</keyword>
        <keyword>gamma-maximin</keyword>
        <keyword>maximality</keyword>
        <keyword>convex set</keyword>
      </keywords>
      <abstract>We contrast three decision rules that extend Expected Utility to contexts where a convex set of probabilities is used to depict uncertainty: Gamma-Maximin, Maximality, and E-admissibility. The rules extend Expected Utility theory as they require that an option is inadmissible if there is another that carries greater expected utility for each probability in a (closed) convex set. If the convex set is a singleton, then each rule agrees with maximizing expected utility. We show that, even when the option set is convex, this pairwise comparison between acts may fail to identify those acts which are Bayes for some probability in a convex set that is not closed. This limitation affects two of the decision rules but not E-admissibility, which is not a pairwise decision rule. E-admissibility can be used to distinguish between two convex sets of probabilities that intersect all the same supporting hyperplanes.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/037.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Products of Capacities Derived from Additive Measures (Extended abstract)</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>products of capacities</keyword>
        <keyword>capacity</keyword>
        <keyword>non-additive measure</keyword>
        <keyword>increasing capacities</keyword>
        <keyword>distorted measures</keyword>
      </keywords>
      <abstract>A new approach to define a product of capacities is presented. It works for capacities that are in a certain relation with additive measures, most often this means that they are somehow derived from additive measures. The product obtained is not unique, but rather, lower and upper bound are given.</abstract>
      <pdf/>
    </paper>
    <paper>
      <id>016</id>
      <title>Dynamic Programming for Discrete-Time Systems with Uncertain Gain</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@rug.ac.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>optimal control</keyword>
        <keyword>dynamic programming</keyword>
        <keyword>uncertainty</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>We generalise the optimisation technique of dynamic programming for discrete-time systems with an uncertain gain function. We assume that uncertainty about the gain function is described by an imprecise probability model, which generalises the well-known Bayesian, or precise, models. We compare various optimality criteria that can be associated with such a model, and which coincide in the precise case: maximality, robust optimality and maximinity. We show that (only) for the first two an optimal feedback can be constructed by solving a Bellman-like equation.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/013.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Decision Making with Imprecise Second-Order Probabilities</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>generalized expected utility</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>second-order uncertainty</keyword>
        <keyword>natural extension</keyword>
        <keyword>linear programming</keyword>
      </keywords>
      <abstract>In this paper we consider decision making under hierarchical imprecise uncertainty models and derive general algorithms to determine optimal actions. Numerical examples illustrate the proposed methods.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/041.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>A Second-Order Uncertainty Model of Independent Random Variables: An Example of the Stress-Strength Reliability</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>stress-strength reliability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>second-order uncertainty</keyword>
        <keyword>natural extension</keyword>
        <keyword>previsions</keyword>
        <keyword>linear programming</keyword>
      </keywords>
      <abstract>A second-order hierarchical uncertainty model of a system of independent random variables is studied in the paper. It is shown that the complex non-linear optimization problem for reducing the second-order model to the first-order one can be represented as a finite set of simple linear programming problems with a finite number of constraints. The stress-strength reliability analysis by unreliable information about statistical parameters of the stress and strength exemplifies the model. Numerical examples illustrate the proposed algorithm for computing the stress-strength reliability.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/040.pdf</pdf>
    </paper>
    <paper>
      <id>042</id>
      <title>Graphical representation of asymmetric graphoid structures</title>
      <authors>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional independence models</keyword>
        <keyword>directed acyclic graph</keyword>
        <keyword>l-separation criterion</keyword>
        <keyword>i-map</keyword>
      </keywords>
      <abstract>Independence models induced by some uncertainty measures (e.g. conditional probability, possibility) do not obey the usual graphoid properties, since they do not satisfy the symmetry property. They are efficiently representable through directed acyclic l-graphs by using L-separation criterion. In this paper, we show that in general there is not a l-graph which describes completely all the independence statements of a given model; hence we introduce in this context the notion of minimal I-map and we show how to build it, given an ordering on the variables.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/042.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>Design of Iterative Proportional Fitting Procedure for Possibility Distributions</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multidimensional possibility distributions</keyword>
        <keyword>marginal problem</keyword>
        <keyword>triangular norm</keyword>
        <keyword>iterative proportional fitting procedure</keyword>
      </keywords>
      <abstract>We design iterative proportional fitting procedure (parametrised by a continuous t-norm) for computation of multidimensional possibility distributions from its marginals and discuss its basic properties.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/043.pdf</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>Bi-elastic Neighbourhood Models</title>
      <authors>
        <author>
          <name>Anton Wallner</name>
          <email>toni@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval probability</keyword>
        <keyword>robust statistics</keyword>
        <keyword>neighbourhood models</keyword>
        <keyword>distorted probability</keyword>
        <keyword>pseudo-capacity</keyword>
        <keyword>convex and bi-elastic functions</keyword>
      </keywords>
      <abstract>We extend Buja's concept of ``pseudo-capacities'', which comprises the neighbourhood models for classical probabilities commonly used in robust statistics. Although systematically developing various directions for generalizing that model, we especially show that robust statistics can be freed from the severe restriction to 2-monotone capacities by employing the more natural framework of coherent or F-probabilities. Our main new tool for doing this is to use bi-elastic instead of convex functions.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/044.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>On the symbiosis of two concepts of conditional interval probability</title>
      <authors>
        <author>
          <name>Kurt Weichselberger</name>
          <email>weichsel@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional interval probability</keyword>
        <keyword>intuitive concept of conditional interval probability</keyword>
        <keyword>canonical concept of conditional interval probability</keyword>
        <keyword>conditioning</keyword>
        <keyword>updating</keyword>
        <keyword>theorem of total probability</keyword>
        <keyword>markov chain</keyword>
        <keyword>bayes' theorem</keyword>
        <keyword>decision theory</keyword>
      </keywords>
      <abstract>This paper argues in favor of the thesis that two different concepts of conditional interval probability are needed, in order to serve the huge variety of tasks conditional probability has in the classical setting of precise probabilities. We compare the commonly used intuitive concept of conditional interval probability with the canonical concept, and see, in particular, that the canonical concept is the appropriate one to generalize the idea of transition kernels to interval probability: only the canonical concept allows reconstruction of the original interval probability from the marginals and conditionals, as well as the powerful formulation of Bayes Theorem.</abstract>
      <pdf>http://www.carleton-scientific.com/isipta/PDF/045.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2005</year>
    <conference>
      <date>
        <start>2005-07-20</start>
        <end>2005-07-23</end>
      </date>
      <location>
        <country>
          <code>US</code>
          <name>United States</name>
        </country>
        <city>
          <name>Pittsburgh</name>
          <latitude>40.47441</latitude>
          <longitude>-79.95097</longitude>
        </city>
        <university>
          <name>Carnegie Mellon University</name>
          <department/>
        </university>
      </location>
    </conference>
    <paper>
      <id>054</id>
      <title>A New Score for Independence Based on the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Joaquin Abellan</name>
          <email>jabellan@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>independence</keyword>
        <keyword>statistical tests</keyword>
        <keyword>bayesian score</keyword>
        <keyword>chi-square test</keyword>
        <keyword>imprecise dirichlet model</keyword>
      </keywords>
      <abstract>In this paper we present a new score to determine when two categorical variables are independent. It represents a measure that can be used in classification. It is an interval-valued score that is based on the Heckerman, Geiger, and Chickering's score. We also carry out an empirical comparison with different scores to determine when two binary variables are independent. The others measures that have been considered are: the Bayesian score metric, the Bayesian information criterion (BIC), the p-value of the Chi-square test for independence and the upper entropy score based on imprecise probabilities. For the new score, we find a behaviour that it is more similar to statistical tests from small samples and to Bayesian procedures for large samples. This makes it very appropriate for some concrete types of problems.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s054.pdf</pdf>
    </paper>
    <paper>
      <id>045</id>
      <title>Fast Algorithms for Robust Classification with Bayesian Nets</title>
      <authors>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian networks</keyword>
        <keyword>missing data</keyword>
        <keyword>conservative updating rule</keyword>
        <keyword>credal classification</keyword>
      </keywords>
      <abstract>We focus on a well-known classification task with expert systems based on Bayesian networks: predicting the state of a target variable given an incomplete observation of the other variables in the network, i.e., an observation of a subset of all the possible variables. To provide conclusions robust to near-ignorance about the process that prevents some of the variables from being observed, it has recently been derived a new rule, called conservative updating. With this paper we address the problem to efficiently compute the conservative updating rule for robust classification with Bayesian networks. We show first that the general problem is NP-hard, thus establishing a fundamental limit to the possibility to do robust classification efficiently. Then we define a wide subclass of Bayesian networks that does admit efficient computation. We show this by developing a new classification algorithm for such a class, which extends substantially the limits of efficient computation with respect to the previously existing algorithm.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s045.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>Comparative Ignorance and the Ellsberg Phenomenon</title>
      <authors>
        <author>
          <name>Horacio Arlo-Costa</name>
          <email>hcosta@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jeffrey Helzner</name>
          <email>jh2239@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ellsberg's paradox</keyword>
        <keyword>comparative ignorance</keyword>
        <keyword>ambiguity aversion</keyword>
      </keywords>
      <abstract>The "Ellsberg phenomenon" has played a significant role in research on imprecise probabilities. Fox and Tversky [5] have attempted to explain this phenomenon in terms of their "comparative ignorance" hypothesis. We challenge that explanation and present empirical work suggesting an explanation that is much closer to Ellsberg's own diagnosis.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s043.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Comparing Methods for Joint Objective and Subjective Uncertainty Propagation with an example in a risk assessment</title>
      <authors>
        <author>
          <name>Cedric Baudrit</name>
          <email>baudrit@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>possibility</keyword>
        <keyword>belief function</keyword>
        <keyword>probability boxes</keyword>
        <keyword>dependency bounds</keyword>
      </keywords>
      <abstract>Probability-boxes, numerical possibility theory and belief functions have been suggested as useful tools to represent imprecise, vague or incomplete information. They are particularly appropriate in environment risk assessment where information is typically tainted with imprecision or incompleteness. Based on these notions, we present and compare four different methods to propagate objective and subjective uncertainties through multivariate functions. Lastly, we use these different techniques on an environmental real case of soil contamination by lead on an ironworks brownfield.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s021.pdf</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>Possibilistic networks with locally weighted knowledge bases</title>
      <authors>
        <author>
          <name>Salem Benferhat</name>
          <email>benferhat@cril.univ-artois.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Lens</name>
              <latitude>50.43302</latitude>
              <longitude>2.82791</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Salma Smaoui</name>
          <email>smaoui@cril.univ-artois.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Lens</name>
              <latitude>50.43302</latitude>
              <longitude>2.82791</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibilistic networks</keyword>
        <keyword>possibilistic logic</keyword>
      </keywords>
      <abstract>Possibilistic networks and possibilistic logic bases are important tools to deal with uncertain pieces of information. Both of them offer a compact representation of possibility distributions. This paper studies a new representation format, called hybrid possibilistic networks, which cover both standard possibilistic networks and possibilistic knowledge bases. An adaptation of propagation algorithm for singly (resp. multiply) connected hybrid possibilistic networks is provided.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s040.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>Electric Company Portfolio Optimization Under Interval Stochastic Dominance Constraints</title>
      <authors>
        <author>
          <name>Dan Berleant</name>
          <email>berleant@iastate.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ames</name>
              <latitude>42.03471</latitude>
              <longitude>-93.61994</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mathieu Dancre</name>
          <email>mathieu.dancre@edf.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Clamart</name>
              <latitude>48.80299</latitude>
              <longitude>2.26692</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jean-Philippe Argaud</name>
          <email>jean-philippe.argaud@edf.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Clamart</name>
              <latitude>48.80299</latitude>
              <longitude>2.26692</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gerald Sheble</name>
          <email>gsheble@iastate.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ames</name>
              <latitude>42.03471</latitude>
              <longitude>-93.61994</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>portfolio optimization</keyword>
        <keyword>risk analysis</keyword>
        <keyword>stochastic dominance</keyword>
      </keywords>
      <abstract>This paper addresses the problem of market risk management for a company in the electricity industry. When dealing with corporate volumetric exposure, there is a need for a methodology that helps to manage the aggregate risks in energy markets. The originality of the approach presented lies in the use of intervals to formulate a specific portfolio optimization problem under stochastic dominance constraints.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s053.pdf</pdf>
    </paper>
    <paper>
      <id>047</id>
      <title>Some theoretical properties of interval-valued conditional probability assessments</title>
      <authors>
        <author>
          <name>Veronica Biazzo</name>
          <email>vbiazzo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword/>
      </keywords>
      <abstract>In this paper we consider interval-valued conditional probability assessments on finite families of conditional events. Based on the</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s047.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>On eventwise aggregation of coherent lower probabilities</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower probability</keyword>
        <keyword>aggregation functions</keyword>
        <keyword>coherence</keyword>
      </keywords>
      <abstract>The paper gives sufficient and necessary conditions for eventwise aggregation of various families of lower probabilities, in particular, of coherent lower probabilities, and properties of the corresponding aggregation functions.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s008.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>Computing Lower and Upper Expectations under Epistemic Independence</title>
      <authors>
        <author>
          <name>Cassio Campos</name>
          <email>cassio@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower and upper expectations</keyword>
        <keyword>credal set</keyword>
        <keyword>credal network</keyword>
        <keyword>multilinear programming</keyword>
      </keywords>
      <abstract>This papers investigates the computation of lower/upper expectations that must cohere with a collection of probabilistic assessments and a collection of judgements of epistemic independence. New algorithms, based on multilinear programming, are presented, both for independence among events and among gambles. Separation properties of graphical models are also investigated.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s006.pdf</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>Application of a hill-climbing algorithm to exact and approximate inference in credal networks</title>
      <authors>
        <author>
          <name>Andres Cano</name>
          <email>acu@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Manuel Gomez</name>
          <email>mgomez@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>probability intervals</keyword>
        <keyword>bayesian networks</keyword>
        <keyword>strong independence</keyword>
        <keyword>hill-climbing and branch-and-bound algorithms</keyword>
      </keywords>
      <abstract>This paper proposes two new algorithms for inference in credal networks. These algorithms enable probability intervals to be obtained for the states of a given query variable. The first algorithm is approximate and uses the hill-climbing technique in the Shenoy-Shafer architecture to propagate in join trees; the second is exact and is a modification of Rocha and Cozman's branch-and-bound algorithm, but applied to general directed acyclic graphs.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s048.pdf</pdf>
    </paper>
    <paper>
      <id>056</id>
      <title>Configurations of Locally Strong Coherence in the Presence of Conditional Exchangeability</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower-upper conditional assessments</keyword>
        <keyword>conditional exchangeability</keyword>
        <keyword>locally strong coherence</keyword>
      </keywords>
      <abstract>Locally strong coherence is an helpful property for inference models based on partial lower-upper conditional probabilities. Moreover, structural constraints are usually adopted to improve vague conclusions. In this paper this two aspects are joint together by proposing logical-numerical conditions that guarantee conditional exchangeability among couples or triplets of events.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s056.pdf</pdf>
    </paper>
    <paper>
      <id>044</id>
      <title>Likelihood-Based Statistical Decisions</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>uncertainty</keyword>
        <keyword>prior ignorance</keyword>
        <keyword>minimax criterion</keyword>
        <keyword>likelihood function</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>non-additive measure</keyword>
        <keyword>completely maxitive measure</keyword>
        <keyword>shilkret integral</keyword>
        <keyword>choquet integral</keyword>
      </keywords>
      <abstract>In this paper, a nonadditive quantitative description of uncertain knowledge about statistical models is obtained by extending the likelihood function to sets and allowing the use of prior information. This description, which has the distinctive feature of not being calibrated, is called relative plausibility. It can be updated when new information is obtained, and it can be used for inference and decision making. As regards inference, the well-founded theory of likelihood-based statistical inference can be exploited, whereas decisions can be based on the minimax plausibility-weighted loss criterion. In the present paper, this decision criterion is introduced and some of its properties are studied, both from the conditional and from the repeated sampling point of view.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s044.pdf</pdf>
    </paper>
    <paper>
      <id>051</id>
      <title>Answers to Two Questions of Fishburn on Subset Comparisons in Comparative Probability Orderings</title>
      <authors>
        <author>
          <name>Robin Christian</name>
          <email>rchr019@math.auckland.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Arkadii Slinko</name>
          <email>slinko@math.auckland.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>additively representable linear orders</keyword>
        <keyword>comparative probability</keyword>
        <keyword>subjective probability</keyword>
        <keyword>subset comparisons</keyword>
      </keywords>
      <abstract>We show that every additively representable comparative probability ordering is determined by at least n-1 binary subset comparisons. We show that there are many orderings of this kind, not just the lexicographic ordering. These provide answers to two questions of Fishburn.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s051.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>Learning from multinomial data: a nonparametric predictive alternative to the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval probability</keyword>
        <keyword>multinomial data</keyword>
        <keyword>nonparametric predictive inference</keyword>
        <keyword>probability wheel</keyword>
      </keywords>
      <abstract>A new model for learning from multinomial data has recently been developed, giving predictive inferences in the form of lower and upper probabilities for a future observation. Apart from the past observations, no information on the sample space is assumed, so explicitly no assumptions are made on the number of possible categories. In this paper, we briefly present the general lower and upper probabilities corresponding to this model, and illustrate their properties via two examples taken from Walley's paper which introduced the imprecise Dirichlet model (IDM). As our approach is nonparametric, its applicability is more restricted. However, our inferences do not suffer from some disadvantages of the IDM.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s037.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>Evidential modeling for pose estimation</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>cuzzolin@cs.ucla.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Los Angeles</name>
              <latitude>33.97395</latitude>
              <longitude>-118.24841</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ruggero Frezza</name>
          <email>frezza@dei.unipd.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Padova</name>
              <latitude>45.41519</latitude>
              <longitude>11.88181</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>pose estimation</keyword>
        <keyword>training set</keyword>
        <keyword>feature-pose maps</keyword>
        <keyword>belief function</keyword>
        <keyword>evidential model</keyword>
      </keywords>
      <abstract>Pose estimation involves reconstructing the configura- tion of a moving body from images sequences. In this paper we present a general framework for pose esti- mation of unknown objects based on Shafer's eviden- tial reasoning. During learning an evidential model of the object is built, integrating different image fea- tures to improve both estimation robustness and pre- cision. All the measurements coming from one or more views are expressed as belief functions, and com- bined through Dempster's rule. The best pose esti- mate at each time step is then extracted from the resulting belief function by probabilistic approxima- tion. The choice of a sufficiently dense training set is a critical problem. Experimental results concerning a human tracking system are shown.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s028.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>n-Monotone lower previsions and lower integrals</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Enrique Miranda</name>
          <email>enrique.miranda@urjc.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Mostoles</name>
              <latitude>40.32234</latitude>
              <longitude>-3.86496</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>n-monotonicity</keyword>
        <keyword>coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>choquet integral</keyword>
        <keyword>comonotone additivity</keyword>
        <keyword>lower prevision</keyword>
        <keyword>lower integral</keyword>
      </keywords>
      <abstract>We study n-monotone lower previsions, which constitute a generalisation of n-monotone lower probabilities. We investigate their relation to the concepts of coherence and natural extension in the behavioural theory of imprecise probabilities, and improve along the way upon a number of results from the literature. Finally, we indicate how many approaches to integration in the literature fall nicely within the framework of the present study of coherent n-monotone lower previsions. This discussion allows us to characterise which types of integrals can be used to calculate the natural extension of a probability charge.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s017.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>S-Independence and S-Conditional independence with respect to Upper and Lower Conditional Probabilities Assigned by Hausdorff Outer and Inner Measures</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>independence</keyword>
        <keyword>strong independence</keyword>
        <keyword>conditional independence</keyword>
        <keyword>hausdorff outer and inner measures</keyword>
      </keywords>
      <abstract>In this paper the notion of s-irrelevance with respect to upper and lower conditional probabilities assigned by Hausdorff outer and inner measures is proved to be a sufficient condition for strong independence introduced for credal sets. An example is given to show that the converse is not true. Moreover the definition of s-conditional irrelevance is given and a generalized factorization property is proposed as necessary condition of s-conditional irrelevance. An example is given to show that s-conditional irrelevance and s-irrelevance are not related; moreover sufficient conditions are given for equivalence between s-conditional irrelevance and s-irrelevance. Finally the notion of s-irrelevance is extended to random variables.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s031.pdf</pdf>
    </paper>
    <paper>
      <id>063</id>
      <title>Computing the Join Range of a Set of Expectations</title>
      <authors>
        <author>
          <name>Charles Geyer</name>
          <email>charlie@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Radu Lazar</name>
          <email>lazar@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Glen Meeden</name>
          <email>glen@stat.umn.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Minneapolis</name>
              <latitude>44.97997</latitude>
              <longitude>-93.26384</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>linear constraint</keyword>
        <keyword>probability assessment</keyword>
        <keyword>convex family of prior</keyword>
        <keyword>polytope</keyword>
      </keywords>
      <abstract>In the theory of imprecise probability it is often of interest to find the range of the expectation of some function over a convex family of probability measures. Here we show how to find the joint range of the expectations of a finite set of functions when the underlying space is finite and the family of probability distributions is defined by finitely many linear constraints.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s063.pdf</pdf>
    </paper>
    <paper>
      <id>061</id>
      <title>Basing Probabilistic Logic on Gambles</title>
      <authors>
        <author>
          <name>Peter Gillett</name>
          <email>gillett@business.rutgers.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Piscataway</name>
              <latitude>40.55293</latitude>
              <longitude>-74.45775</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Richard Scherl</name>
          <email>rscherl@monmouth.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>West Long Branch</name>
              <latitude>40.28351</latitude>
              <longitude>-74.01828</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Glenn Shafer</name>
          <email>gshafer@andromeda.rutgers.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Newark</name>
              <latitude>40.73566</latitude>
              <longitude>-74.17237</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probabilistic logic</keyword>
        <keyword>anytime computation</keyword>
        <keyword>logic of gambles</keyword>
        <keyword>measure-theoretic and behavioral semantics</keyword>
      </keywords>
      <abstract>This article presents a probabilistic logic whose sen- tences can be interpreted as asserting the acceptabil- ity of gambles described in terms of an underlying logic. This probabilistic logic has a concrete syntax and a complete inference procedure, and it handles conditional as well as unconditional probabilities. It synthesizes Nilsson's original probabilistic logic and Frisch and Haddawy's anytime inference procedure with Wilson and Moral's logic of gambles. Two distinct semantics can be used for our prob- abilistic logic: (1) the measure-theoretic semantics used by the prior logics already mentioned and also by the more expressive logic of Fagin, Halpern, and Meggido and (2) a behavioral semantics. Under the measure-theoretic semantics, sentences of our prob- abilistic logic are interpreted as assertions about a probability distribution over interpretations of the un- derlying logic. Under the behavioral semantics, these sentences are interpreted only as asserting the accept- ability of gambles, and this suggests different direc- tions for generalization.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s061.pdf</pdf>
    </paper>
    <paper>
      <id>055</id>
      <title>Objective Imprecise Probabilistic Information, Second Order Beliefs and Ambiguity Aversion: an Axiomatization</title>
      <authors>
        <author>
          <name>Raphael Giraud</name>
          <email>Raphael.Giraud@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probabilistic information</keyword>
        <keyword>ellsberg's paradox</keyword>
        <keyword>second order beliefs</keyword>
        <keyword>ambiguity aversion</keyword>
        <keyword>non-additive probabilities</keyword>
        <keyword>choquet integral</keyword>
      </keywords>
      <abstract>We axiomatize a model of decision under objective ambiguity described by multiple probability distributions. The decision maker forms a subjective (non necessarily additive) belief about the likelihood of probability distributions and computes the average expected utility of a given act with respect to this second order belief. We show that ambiguity aversion like the one revealed by the Ellsberg paradox requires that second order beliefs be nonadditive. Some properties of the model are examined.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s055.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Towards a Unifying Theory of Logical and Probabilistic Reasoning</title>
      <authors>
        <author>
          <name>Rolf Haenni</name>
          <email>haenni@iam.unibe.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Berne</name>
              <latitude>46.94809</latitude>
              <longitude>7.44744</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>logical reasoning</keyword>
        <keyword>probabilistic reasoning</keyword>
        <keyword>uncertainty</keyword>
        <keyword>ignorance</keyword>
        <keyword>argumentation</keyword>
      </keywords>
      <abstract>Logic and probability theory have both a long history in science. They are mainly rooted in philosophy and mathematics, but are nowadays important tools in many other fields such as computer science and, in particular, artificial intelligence. Some philosophers studied the connection between logical and probabilistic reasoning, and some attempts to combine these disciplines have been made in computer science, but logic and probability theory are still widely considered to be separate theories that are only loosely connected. This paper introduces a new perspective which shows that logical and probabilistic reasoning are no more and no less than two opposite extreme cases of one and the same universal theory of reasoning called probabilistic argumentation.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s007.pdf</pdf>
    </paper>
    <paper>
      <id>059</id>
      <title>Dynamically Consistent Updating of MaxMin EU and MaxMax EU Preferences</title>
      <authors>
        <author>
          <name>Eran Hanany</name>
          <email>hananye@post.tau.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Tel Aviv</name>
              <latitude>32.08088</latitude>
              <longitude>34.78057</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Peter Klibanoff</name>
          <email>peterk@kellogg.northwestern.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Evanston</name>
              <latitude>42.04114</latitude>
              <longitude>-87.69006</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ambiguity</keyword>
        <keyword>dynamic consistency</keyword>
        <keyword>updating</keyword>
        <keyword>bayesian</keyword>
      </keywords>
      <abstract>This paper presents a new family of nonconsequentialist update rules, each member of which extends full Bayesian updating in a dynamically consistent way to the whole class of max-min EU and max-max EU preferences. Two of the main properties of these rules are: (1) an unconditionally optimally chosen act remains optimal also conditionally; (2) the set of measures in the representation of the conditional preference is obtained by applying Bayes rule to a subset of the set of measures of the unconditional preference.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s059.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Approximate Inference in Credal Networks by Variational Mean Field Methods</title>
      <authors>
        <author>
          <name>Jaime Ide</name>
          <email>jaime.ide@poli.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>variational methods</keyword>
        <keyword>inferences</keyword>
      </keywords>
      <abstract>Graph-theoretical representations for sets of probability measures (credal networks) generally display high complexity, and approximate inference seems to be a natural solution for large networks. This paper introduces a variational approach to approximate inference in credal networks: we show how to formulate mean field approximations using naive (fully factorized) and structured (tree-like) schemes. We discuss the computational advantages of the variational approach, and present examples that illustrate the mechanics of the proposal.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s050.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>A Granular Semantics for Fuzzy Measures and its Application to Climate Change Scenarios</title>
      <authors>
        <author>
          <name>Jonathan Lawry</name>
          <email>j.lawry@bris.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Bristol</name>
              <latitude>51.45523</latitude>
              <longitude>-2.59665</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jim Hall</name>
          <email>jim.hall@ncl.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Newcastle Upon Tyne</name>
              <latitude>54.97328</latitude>
              <longitude>-1.61396</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Guangtao Fu</name>
          <email>guangtao.fu@bristol.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Bristol</name>
              <latitude>51.45523</latitude>
              <longitude>-2.59665</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy measure</keyword>
        <keyword>operational semantics</keyword>
        <keyword>uncertain models</keyword>
        <keyword>emission scenarios</keyword>
      </keywords>
      <abstract>A granular based semantics for fuzzy measures is introduced in which the measure of a set of propositions approximates the probability of the disjunction of these propositions. This approximation is derived from known probabilities across a granular partition of the set of possible worlds. This interpretation is then extended to allow for the case where there is uncertainty regarding the meanings of propositions. Such a semantics is motivated by, and provides some justification for, the use of fuzzy measure to quantify the uncertainty associated with climate emissions scenarios. The use of socio-economic scenarios in climate models is discussed in the context of a possible worlds model and an example is given of the use of fuzzy measures across scenarios to aggregate global mean temperature predictions.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s002.pdf</pdf>
    </paper>
    <paper>
      <id>036</id>
      <title>Decision Making with Imprecise and Fuzzy Probabilities - a Comparison</title>
      <authors>
        <author>
          <name>Sven-Hendrik Lossin</name>
          <email>losso@web.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Hanover</name>
              <latitude>52.37052</latitude>
              <longitude>9.73322</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>fuzzy probabilities</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>The standard framework of decision theory has no answer to the question how to deal with partial or fuzzy information. In this article two frameworks are presented and compared. The first one uses fuzzy probabilities as in [Buckley 2003] and has been developed by Dubois/Prade [Dubois 1979]. The data-based case is added here. The second framework deals with imprecise probabilities as in [Walley 1991] and proposes a model similar to that by [Kofler 1976]. Furthermore the two frameworks are compared with classical statistical decision theory. It is shown that both of them are similar concerning the mathematical techniques they require but are different regarding the knowledge the decision maker has about the probabilities.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s036.pdf</pdf>
    </paper>
    <paper>
      <id>058</id>
      <title>Nonmonotonic Probabilistic Logics under Variable-Strength Inheritance with Overriding: Algorithms and Implementation in nmproblog</title>
      <authors>
        <author>
          <name>Thomas Lukasiewicz</name>
          <email>Thomas.Lukasiewicz@dis.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>model-theoretic and nonmonotonic probabilistic logics</keyword>
        <keyword>qualitative reasoning about uncertainty</keyword>
        <keyword>probability rankings</keyword>
        <keyword>algorithms for manipulating imprecise probabilities</keyword>
        <keyword>convex sets of probability measures</keyword>
      </keywords>
      <abstract>In previous work, I have introduced nonmonotonic probabilistic logics under variable-strength inheritance with overriding. They are formalisms for probabilistic reasoning from sets of strict logical, default logical, and default probabilistic sentences, which are parameterized through a value lambda in [0,1] that describes the strength of the inheritance of default probabilistic knowledge. In this paper, I continue this line of research. I present algorithms for deciding consistency of strength lambda and for computing tight consequences of strength lambda, which are based on reductions to the standard problems of deciding satisfiability and of computing tight logical consequences in model-theoretic probabilistic logic. Furthermore, I describe an implementation of these algorithms in the system nmproblog.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s058.pdf</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>On Coherent Variability Measures and Conditioning</title>
      <authors>
        <author>
          <name>Sebastian Maass</name>
          <email>sebastian.maass@math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent previsions</keyword>
        <keyword>coherent risk measure</keyword>
        <keyword>variability measures</keyword>
        <keyword>fair price</keyword>
        <keyword>conditioning</keyword>
      </keywords>
      <abstract>Coherent upper and lower previsions are becoming more and more popular as a mathematical model for robust valuations under uncertainty. Likewise, the mathematically equivalent class of coherent risk measures is attracting a lot attention in mathematical finance. In this paper, we show that a misinterpretation of upper previsions demands a closer examination of the basis of the theory of imprecise previsions. As a consequence, we obtain a new interpretation of coherent lower previsions as fair prices, a class of coherent variability measures, and a new type of conditioning for coherent lower previsions.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s029.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>On the Existence of Extremal Cones and Comparative Probability Orderings</title>
      <authors>
        <author>
          <name>Simon Marshall</name>
          <email>smar141@ec.auckland.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>NA</keyword>
      </keywords>
      <abstract>We study the recently discovered phenomenon of existence of comparative probability orderings on finite sets that violate Fishburn hypothesis - we call such orderings and the discrete cones associated with them extremal. Conder and Slinko constructed an extremal discrete cone on the set of n=7 elements and showed that no extremal cones exist on the set of n Keywords. Comparative probability ordering, Discrete cone, Quadratic residues</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s030.pdf</pdf>
    </paper>
    <paper>
      <id>062</id>
      <title>Bayesianism Without Priors, Acts Without Consequences</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>subjective probability</keyword>
        <keyword>state-dependent utility</keyword>
        <keyword>state-preference theory</keyword>
        <keyword>risk neutral probabilities</keyword>
      </keywords>
      <abstract>A generalization of subjective expected utility is presented in which the primitives are a finite set of states of the world, a finite set of strategies available to the decision maker, and allocations of money. The model does not require explicit definitions of consequences ("states of the person"), nor does it rely on counterfactual preferences, nor does it emphasize the unique separation of prior probabilities from possibly-state-dependent utilities. Rather, preferences have an additively separable representation in which the valuation of outcomes of a decision or game is implicit in the state- and strategy-dependence of utility for money. This model provides an axiomatic foundation for Bayesian decision analysis and game theory in the tradition of de Finetti and Arrow-Debreu rather than Savage. The observable parameters of beliefs are risk neutral probabilities (betting rates for money) and in situations where the decision maker has no intrinsic interest or influence over an experiment given the truth or falsity of the hypothesis, her risk neutral probabilities and preferences among strategies are updated by application of Bayes' rule without the need to identify "true" prior probabilities.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s062.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>Envelope Theorems and Dilation with Convex Conditional Previsions</title>
      <authors>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolov@econ.univ.trieste.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>convex prevision</keyword>
        <keyword>envelope theorem</keyword>
        <keyword>dilation</keyword>
        <keyword>convex risk measure</keyword>
      </keywords>
      <abstract>This paper focuses on establishing envelope theorems for convex conditional lower previsions, a recently investigated class of imprecise previsions larger than coherent imprecise conditional previsions. It is in particular discussed how the various theorems can be employed in assessing convex previsions. We also consider the problem of dilation for these kinds of imprecise previsions, and point out the role of convex previsions in measuring conditional risks.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s023.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Limits of Learning from Imperfect Observations under Prior Ignorance: the Case of the Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Alberto Piatti</name>
          <email>alberto.piatti@lu.unisi.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Trojani</name>
          <email>fabio.trojani@unisg.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Saint Gallen</name>
              <latitude>47.42391</latitude>
              <longitude>9.37477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>predictive bayesian inference</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>vacuous predictive probabilities</keyword>
        <keyword>imperfect observational mechanism</keyword>
      </keywords>
      <abstract>Consider a relaxed multinomial setup, in which there may be mistakes in observing the outcomes of the process--this is often the case in real applications. What can we say about the next outcome if we start learning about the process in conditions of prior ignorance? To answer this question we extend the imprecise Dirichlet model to the case of imperfect observations and we focus on posterior predictive probabilities for the next outcome. The results are very surprising: the posterior predictive probabilities are vacuous, irrespectively of the amount of observations we do, and however small is the probability of doing mistakes. In other words, the imprecise Dirichlet model cannot help us to learn from data when the observational mechanism is imperfect. This result seems to rise a serious question about the use of the imprecise Dirichlet model for practical applications, and, more generally, about the possibility to learn from imperfect observations under prior ignorance.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s020.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Imprecise probability models for inference in exponential families</title>
      <authors>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>Erik.Quaeghebeur@UGent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>exponential family</keyword>
        <keyword>imprecise probability models</keyword>
        <keyword>inference</keyword>
        <keyword>conjugate analysis</keyword>
        <keyword>naive credal classifier</keyword>
      </keywords>
      <abstract>When considering sampling models described by a distribution from an exponential family, it is possible to create two types of imprecise probability models. One is based on the corresponding conjugate distribution and the other on the corresponding predictive distribution. In this paper, we show how these types of models can be constructed for any (regular, linear, canonical) exponential family, such as the centered normal distribution. To illustrate the possible use of such models, we take a look at credal classification. We show that they are very natural and potentially promising candidates for describing the attributes of a credal classifier, also in the case of continuous attributes.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s019.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>Estimation of Chaotic Probabilities</title>
      <authors>
        <author>
          <name>Leandro Rego</name>
          <email>lcr26@cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Terrence Fine</name>
          <email>tlfine@ece.cornell.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ithaca</name>
              <latitude>42.40279</latitude>
              <longitude>-76.48400</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>foundations of probability</keyword>
        <keyword>church place selection rules</keyword>
        <keyword>probabilistic reasoning</keyword>
        <keyword>complexity</keyword>
      </keywords>
      <abstract>A Chaotic Probability model is a usual set of probability measures, ${\cal M}$, the totality of which is endowed with an {\em objective, frequentist interpretation} as opposed to being viewed as a statistical compound hypothesis or an imprecise behavioral subjective one. In the prior work of Fierens and Fine, given finite time series data, the estimation of the Chaotic Probability model is based on the analysis of a set of relative frequencies of events taken along a set of subsequences selected by a set of rules. Fierens and Fine proved the existence of families of causal subsequence selection rules that can make ${\cal M}$ visible, but they did not provide a methodology for finding such family. This paper provides a universal methodology for finding a family of subsequences that can make ${\cal M}$ visible such that relative frequencies taken along such subsequences are provably close enough to a measure in ${\cal M}$ with high probability.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s005.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>No Double Counting Semantics for Conditional Independence</title>
      <authors>
        <author>
          <name>Prakash Shenoy</name>
          <email>pshenoy@ku.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Lawrence</name>
              <latitude>38.97167</latitude>
              <longitude>-95.23525</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional independence</keyword>
        <keyword>no double counting semantics</keyword>
        <keyword>distinct evidence</keyword>
        <keyword>independent pieces of evidence</keyword>
        <keyword>graphoid axioms</keyword>
        <keyword>dempster-shafer's belief function theory</keyword>
        <keyword>spohn's epistemic beliefs theory</keyword>
        <keyword>possibility theory</keyword>
        <keyword>valuation based systems</keyword>
      </keywords>
      <abstract>The main goal of this paper is to describe a new semantic for conditional independence in terms of no double counting of uncertain evidence. For ease of exposition, we use probability calculus to state all results. But the results generalize easily to any calculus that fits in the framework of valuation-based systems. Thus, the results described in this paper apply also, for example, to Dempster-Shafer's (D-S) belief function theory, to Spohn's epistemic beliefs theory, and to Zadeh's possibility theory. The concept of independent (or distinct) evidence in D-S belief function theory is analogous to the concept of conditional independence for variables in probability theory.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s003.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>A Protocol for the Elicitation of Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Alane Alves Silva</name>
          <email>alaneaas@yahoo.com.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fernando Campello de Souza</name>
          <email>fmcs@hotlink.com.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>elicitation</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>inferential skill</keyword>
        <keyword>convex set</keyword>
        <keyword>imprecise dirichlet model</keyword>
      </keywords>
      <abstract>A protocol for the elicitation of imprecise probabilities based on linear programming is applied for the case of two continuous variables. Two medical experts were elicited. The resulting convex set of probability distributions was compared with the results obtained by the application of an imprecise Dirichlet model to a data base. An indicator is introduced to assess the inferential skill of the medical experts.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s025.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Generalized Conditioning in Neighbourhood Models</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval probability</keyword>
        <keyword>robust statistics</keyword>
        <keyword>non-additive probabilities</keyword>
        <keyword>neighbourhood models</keyword>
      </keywords>
      <abstract>Sets of probability measures which form neighbourhoods of additive probability measures are studied. An application of the Jeffrey's rule of conditioning to forming neighbourhoods of probability measures is proposed. Neighbourhoods that are closed for generalized conditioning according to this rule are characterized. They are shown to be exactly convex and bi-elastic neighbourhoods.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s015.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>Ordinal Subjective Foundations for Finite-domain Probability Agreement</title>
      <authors>
        <author>
          <name>Paul Snow</name>
          <email>paulusnix@cs.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Concord</name>
              <latitude>43.20814</latitude>
              <longitude>-71.53757</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>qualitative probability</keyword>
        <keyword>transitivity</keyword>
        <keyword>de finetti's conjecture</keyword>
        <keyword>scott's theorem</keyword>
      </keywords>
      <abstract>Normative study of probability-agreeing orderings of propositions, much of it rooted in a false but evocative conjecture of Bruno de Finetti, has typically sought to abstract credal rationality claims familiarly made for numerical probabilities. It is now known that some probability-disagreeing orderings, e.g. possibilistic order, syntactically restate probability-agreeing orderings, and so share in any ordinal probabilistic 'rationality.' This paper explores what remains normatively distinctive about subjective probability agreement. A multiset partial ordering, characteristic of all transitive elementary orderings, helps provide succinct, apprehensible necessary and sufficient ordinal conditions for probability agreement.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s033.pdf</pdf>
    </paper>
    <paper>
      <id>022</id>
      <title>Variable Selection in Classification Trees Based on Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Carolin Strobl</name>
          <email>carolin.strobl@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>classification trees</keyword>
        <keyword>credal classification</keyword>
        <keyword>variable selection bias</keyword>
        <keyword>attribute selection error</keyword>
        <keyword>shannon entropy</keyword>
        <keyword>entropy estimation</keyword>
      </keywords>
      <abstract>Classification trees based on imprecise probabilities provide an advancement of classical classification trees. The Gini Index is the default splitting criterion in classical classification trees, while in classification trees based on imprecise probabilities, an extension of the Shannon entropy has been introduced as the splitting criterion. However, the use of these empirical entropy measures as split selection criteria can lead to a bias in variable selection, such that variables are preferred for features other than their information content. This bias is not eliminated by the imprecise probability approach. The source of variable selection bias for the estimated Shannon entropy, as well as possible corrections, are outlined. The variable selection performance of the biased and corrected estimators are evaluated in a simulation study. Additional results from research on variable selection bias in classical classification trees are incorporated, implying further investigation of alternative split selection criteria in classification trees based on imprecise probabilities.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s022.pdf</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>Powerful algorithms for decision making under partial prior information and general ambiguity attitudes</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ambiguity attitudes</keyword>
        <keyword>choquet expected utility</keyword>
        <keyword>decision making</keyword>
        <keyword>e-admissibility</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval probability</keyword>
        <keyword>interval statistical models</keyword>
        <keyword>linear programming</keyword>
        <keyword>maxemin criterion</keyword>
        <keyword>maximality</keyword>
        <keyword>maxmin expected utility model</keyword>
        <keyword>minimality</keyword>
        <keyword>partial prior information</keyword>
        <keyword>structure dominance</keyword>
      </keywords>
      <abstract>This paper discusses decision making in the practically important situation where only partial prior information on the stochastic behavior of the states of nature expressed by imprecise probabilities (interval probability) is available. For this situation, in literature several optimality criteria have been suggested and investigated theoretically. Practical computation of optimal solutions, however, is far from being straightforward. The paper develops powerful algorithms for determining optimal actions under arbitrary ambiguity attitudes and the criterion of E-admissibility. The algorithms are based on linear programming and can be implemented by standard software.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s046.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>Decision making under incomplete data using the imprecise Dirichlet model</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lvu@utkin.usr.etu.spb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>coarse data</keyword>
        <keyword>decision making</keyword>
        <keyword>hodges-lehmann criterion</keyword>
        <keyword>imperfect measurements</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>incomplete data</keyword>
        <keyword>interval probability</keyword>
        <keyword>interval statistical models</keyword>
        <keyword>predictive probabilities</keyword>
        <keyword>set-valued observations</keyword>
        <keyword>statistical data</keyword>
        <keyword/>
      </keywords>
      <abstract>The paper presents an efficient solution to decision problems where direct partial information on the distribution of the states of nature is available, either by observations of previous repetitions of the decision problem or by direct expert judgements.\\ To process this information we use a recent generalization of Walley's imprecise Dirichlet model, allowing us also to handle incomplete observations or imprecise judgements. We derive efficient algorithms and discuss properties of the optimal solutions. In the case of precise data and pure actions we are surprisingly led to a frequency-based variant of the Hodges-Lehmann criterion, which was developed in classical decision theory as a compromise between Bayesian and minimax procedures.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s049.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>The role of coherence for the integration of different sources</title>
      <authors>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent conditional probability</keyword>
        <keyword>data fusion</keyword>
        <keyword>statistical matching</keyword>
        <keyword>inference</keyword>
      </keywords>
      <abstract>In several economic applications (e.g. marketing research, microsimulation models) there is the need to consider different data sources and to integrate the information coming from them. In this paper we show how integration problems can be managed by means of coherence for partial conditional probabilistic assessments. Coherence allows us to combine the knowledge coming from the different sources, included those (possibly) given from field experts, without necessarily assuming further hypothesis (as conditional independence). Moreover, inferences and decisions can be drawn taking in consideration also logical constraints among the variables. An example showing advantages and drawbacks of the proposed method is given.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s034.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>On an Interval-Valued Solution of the Marginal Problem</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Vladislav Bina</name>
          <email>bina@fm.vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multidimensional distributions</keyword>
        <keyword>marginal problem</keyword>
        <keyword>consistency</keyword>
        <keyword>ipfp</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>coherence</keyword>
      </keywords>
      <abstract>A version of a marginal problem considered in this paper was connected, in the 80ies of the last century, with the necessity to cope with a task of knowledge integration in probabilistic expert systems. In practical situations marginal distributions representing pieces of local knowledge were often inconsistent. The present paper introduces an interval-valued solution of a marginal problem, which always exists, even in cases when, because of inconsistencies, one cannot get a solution in a classical sense. The paper shows how famous Iterative Proportional Fitting Procedure, which is often used to get a classical solution of a consistent problem, can be exploited also when constructing an interval-valued solution, and how any solution can be optimized.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s018.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Maximal Number of Vertices of Polytopes Defined by F-Probabilities</title>
      <authors>
        <author>
          <name>Anton Wallner</name>
          <email>toni@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>NA</keyword>
      </keywords>
      <abstract>Every F-probability (= coherent probability) FF on a finite sample space Omega_k with k elements defines a set of classical probabilities in accordance with the interval limits. This set, called ``structure'' of FF, is a convex polytope having dimension Keywords. Geometry of interval probability, number of vertices of structures/cores/credal sets, combinatorial theory of polyhedra, 0/1-matrices.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s016.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>The Logical Concept of Probability and Statistical Inference</title>
      <authors>
        <author>
          <name>Kurt Weichselberger</name>
          <email>Kurt.Weichselberger@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>the theory of interval probability</keyword>
        <keyword>logical probability</keyword>
        <keyword>statistical inference</keyword>
        <keyword>inductive probability</keyword>
      </keywords>
      <abstract>A consistent concept of logical probability affords the employment of interval probability. Such a concept which attributes probability to arguments consisting of premise and conclusion, can be used to generate a system of axioms for statistical inference.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s035.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Conservative Rules for Predictive Inference with Incomplete Data</title>
      <authors>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>predictive inference</keyword>
        <keyword>statistical inference</keyword>
        <keyword>incomplete data</keyword>
        <keyword>missing data</keyword>
        <keyword>conservative inference rule</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>conditioning</keyword>
        <keyword>classification</keyword>
        <keyword>data mining</keyword>
      </keywords>
      <abstract>This paper addresses the following question: how should we update our beliefs after observing some incomplete data, in order to make credible predictions about new, and possibly incomplete, data? There may be several answers to this question according to the model of the process that creates the incompleteness. This paper develops a rigorous modelling framework that makes it clear the conditions that justify the different answers; and, on this basis, it derives a new conditioning rule for predictive inference to be used in a wide range of states of knowledge about the incompleteness process, including near-ignorance, which, surprisingly, does not seem to have received attention so far. Such a case is instead particularly important, as modelling incompleteness processes can be highly impractical, and because there are limitations to statistical inference with incomplete data: it is generally not possible to learn how incompleteness processes work by using the available data; and it may not be possible, as the paper shows, to measure empirically the quality of the predictions. Yet, these depend heavily on the assumptions made.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s038.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Arithmetic on random variables: squeezing the envelopes with new joint distribution constraints</title>
      <authors>
        <author>
          <name>Jianzhong Zhang</name>
          <email>zjz@iastate.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ames</name>
              <latitude>42.03471</latitude>
              <longitude>-93.61994</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dan Berleant</name>
          <email>berleant@iastate.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Ames</name>
              <latitude>42.03471</latitude>
              <longitude>-93.61994</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertainty</keyword>
        <keyword>arithmetic on random variables</keyword>
        <keyword>distribution envelope determination (denv)</keyword>
        <keyword>joint distribution</keyword>
        <keyword>dependency relationship</keyword>
        <keyword>copulas</keyword>
        <keyword>probability boxes</keyword>
        <keyword>linear programming</keyword>
        <keyword>partial information</keyword>
      </keywords>
      <abstract>Uncertainty is a key issue in decision analysis and other kinds of applications. Researchers have developed a numbers of approaches to address computations on uncertain quantities. When doing arithmetic operations on random variables, an important question has to be considered: the dependency relationships among the variables. In practice, we often have partial information about the dependency relationship between two random variables. This information may result from experience or system requirements. We can use this information to improve bounds on the cumulative distributions of random variables derived from the marginals whose dependency is partially known.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta05/proceedings/papers/s013.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2007</year>
    <conference>
      <date>
        <start>2007-07-16</start>
        <end>2007-07-19</end>
      </date>
      <location>
        <country>
          <code>CZ</code>
          <name>Czech Republic</name>
        </country>
        <city>
          <name>Prague</name>
          <latitude>50.08804</latitude>
          <longitude>14.42076</longitude>
        </city>
        <university>
          <name>Charles University</name>
          <department>Faculty of Mathematics and Physics</department>
        </university>
      </location>
    </conference>
    <paper>
      <id>066</id>
      <title>Credal networks for military identification problems</title>
      <authors>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ralph Bruehlmann</name>
          <email>ralph.bruehlmann@ar.admin.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Thun</name>
              <latitude>46.75118</latitude>
              <longitude>7.62166</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alberto Piatti</name>
          <email>alberto.piatti@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>information fusion</keyword>
        <keyword>sensor management</keyword>
        <keyword>tracking systems</keyword>
      </keywords>
      <abstract>Credal networks are imprecise probabilistic graphical models generalizing Bayesian networks to convex sets of probability mass functions. This makes credal networks particularly suited to capture and model expert knowledge under very general conditions, including states of qualitative and incomplete knowledge. In this paper, we present a credal network for risk evaluation in case of intrusion of civil aircrafts into a no-fly zone. The different factors relevant for this evaluation, together with an independence structure over them, are initially identified. These factors are observed by sensors, whose reliabilities can be affected by variable external factors, and even by the behavior of the intruder. A model of these observation mechanisms, and the necessary fusion scheme for the information returned by the sensors measuring the same factor, are both completely embedded into the structure of the credal network. A pool of experts, facilitated in their task by specific techniques to convert qualitative judgments into imprecise probabilistic assessments, has made possible the quantification of the network. We show the capabilities of the proposed network by means of some preliminary tests referred to simulated scenarios. Overall, we can regard this application as an useful tool to support military experts in their decision, but also as a quite general imprecise-probability paradigm for information fusion.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s066.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Uncertainty analysis in food engineering involving imprecision and randomness</title>
      <authors>
        <author>
          <name>Cedric Baudrit</name>
          <email>cbaudrit@grignon.inra.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Thiverval-Grignon</name>
              <latitude>48.85108</latitude>
              <longitude>1.91710</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Arnaud Helias</name>
          <email>arnaud.helias@grignon.inra.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Thiverval-Grignon</name>
              <latitude>48.85108</latitude>
              <longitude>1.91710</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Nathalie Perrot</name>
          <email>nathalie.perrot@grignon.inra.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Thiverval-Grignon</name>
              <latitude>48.85108</latitude>
              <longitude>1.91710</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>p-box</keyword>
        <keyword>belief function</keyword>
        <keyword>possibility</keyword>
        <keyword>food processing</keyword>
        <keyword>cheese ripening</keyword>
      </keywords>
      <abstract>During the cheese ripening, airflow pattern and climatic conditions inside cheese-ripening rooms are determinant for cheese weight losses. Due to the variation of air velocity inside ripening chambers, homogeneity in the distribution of climatic conditions is very hard to achieve at every single point of it. It is very difficult to characterize climatic distributions inside cheese-ripening rooms. Indeed, it is inconceivable to install sensors everywhere inside ripening chambers to pick up for instance temperature and relative humidity. Associated to the fact that little data have been published for the ripening cheese, we are hence faced with imprecise and incomplete knowledge. In practice, it is common that some model parameters may be represented by single probability distributions, justified by substantial data, while others are more faithfully represented by possibibility distributions due to the partial nature of available knowledge. This paper applies recent methods, designed for the joint propagation of variability and imprecision, to a cheese ripening mass loss model. Joint propagation methods provide lower \&amp; upper probability bounds of exceeding a certain value of cheese mass losses.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s027.pdf</pdf>
    </paper>
    <paper>
      <id>070</id>
      <title>Predicting the Next Pandemic: An Exercise in Imprecise Hazards</title>
      <authors>
        <author>
          <name>Mikelis Bickis</name>
          <email>bickis@snoopy.usask.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Saskatoon</name>
              <latitude>52.11679</latitude>
              <longitude>-106.63452</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ugis Bickis</name>
          <email>bickis@sympatico.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Ottawa</name>
              <latitude>45.41117</latitude>
              <longitude>-75.69812</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>survival analysis</keyword>
        <keyword>hazard function</keyword>
        <keyword>autocorrelated prior</keyword>
      </keywords>
      <abstract>Influenza pandemics have swept the world numerous times during the last few centuries. Cases of bird flu infecting humans have prompted predictions that we are due for another pandemic soon, but skeptics dismiss such prognostications as panic caused by a misunderstanding of probability. The issue can be reduced mathematically to the question of whether the pandemic process has an increasing, constant, or decreasing hazard function. Historical data on past pandemics can be used to estimate the hazard function using imprecise probabilities, giving upper and lower predictive probabilities of an imminent pandemic, given past waiting times. In order to achieve smoother estimates of the imprecise hazard function, an autocorrelated imprecise Normal prior is proposed.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s070.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Measuring Uncertainty with Imprecision Indices</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alexander Lepskiy</name>
          <email>lepskiy@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecision indices</keyword>
        <keyword>lower and upper probabilities</keyword>
        <keyword>uncertainty based information</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s030.pdf</pdf>
    </paper>
    <paper>
      <id>065</id>
      <title>Inference in Credal Networks Through Integer Programming</title>
      <authors>
        <author>
          <name>Cassio Campos</name>
          <email>cassio@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>integer programming</keyword>
      </keywords>
      <abstract>A credal network associates a directed acyclic graph with a collection of sets of probability measures; it offers a compact representation for sets of multivariate distributions. In this paper we present a new algorithm for inference in credal networks based on an integer programming reformulation. We are concerned with computation of lower/upper probabilities for a variable in a given credal network. Experiments reported in this paper indicate that this new algorithm has better performance than existing ones for some important classes of networks.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s065.pdf</pdf>
    </paper>
    <paper>
      <id>067</id>
      <title>Credal Nets with Probabilities  Estimated with an Extreme Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Andres Cano</name>
          <email>acu@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Manuel Gomez-Olmedo</name>
          <email>mgomez@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>locally specified redal networks</keyword>
        <keyword>global imprecise dirichlet model</keyword>
        <keyword>propagation algorithms</keyword>
        <keyword>probability trees</keyword>
      </keywords>
      <abstract>The propagation of probabilities in credal networks when probabilities are estimated with a global imprecise Dirichlet model is an important open problem. Only Zaffalon (2001) has proposed an algorithm for the Naive classifier. The main difficulty is that, in general, computing upper and lower probability intervals implies the resolution of an optimization of a fraction of two polynomials. In the case of the Naive Bayes, Zaffalon has shown that the function is a convex function of one parameter, but this is not true at the general case. In this paper, we propose the use of an imprecise global model, but we restrict the distributions to only two (the most extreme ones). The result is a model giving rise to the same upper and lower probabilities, when estimating the uncertainty of a future event, but in the case of estimating a conditional probability, will provide smaller intervals. Its main advantage is that the optimization problem is simpler, and available procedures can be directly applied, as the ones proposed in Cano et al. (2007).</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s067.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Comparative Probability Orders and the Flip Relation</title>
      <authors>
        <author>
          <name>Marston Conder</name>
          <email>m.conder@auckland.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dominic Searles</name>
          <email>dnsearles@gmail.com</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Arkadii Slinko</name>
          <email>a.slinko@auckland.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>comparative probability</keyword>
        <keyword>flippable pair</keyword>
        <keyword>probability elicitation</keyword>
        <keyword>subset comparisons</keyword>
        <keyword>simple game</keyword>
        <keyword>weighted majority game</keyword>
        <keyword>desirability relation</keyword>
      </keywords>
      <abstract>In this paper we study the flip relation on the set of comparative probability orders on n atoms introduced by Maclagan (1999). With this relation the set of all comparative probability orders becomes a graph G_n. Firstly, we prove that any comparative probability order with an underlying probability measure is uniquely determined by the set of its neighbours in G_n. This theorem generalises the theorem of Fishburn, Peke\v c and Reeds (2002). We show that the existence of the underlying probability measure is essential for the validity of this result. Secondly, we obtain the numerical characteristics of the flip relation in G_6. Thirdly, we prove that a comparative probability order on n atoms can have in G_n up to f{n+1} neighbours, where f(n) is the nth Fibonacci number. We conjecture that this number is maximal possible. This partly answers a question posed by Maclagan.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s011.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Multinomial nonparametric predictive inference with sub-categories</title>
      <authors>
        <author>
          <name>Frank Coolen</name>
          <email>frank.coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ca model</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>nonparametric predictive inference</keyword>
        <keyword>probability wheel</keyword>
      </keywords>
      <abstract>Nonparametric predictive inference (NPI) is a powerful tool for predictive inference under nearly complete prior ignorance. After summarizing our NPI approach for multinomial data, as presented in Coolen &amp; Augustin (2005, 2007), both for situations with and without known total number of possible categories, we illustrate how this approach can be generalized to deal with sub-categories, enabling consistent inferences at different levels of detail for the specification of observations. This approach deals with main categories and sub-categories in a logical manner, directly based on the powerful probability wheel representation for multinomial data that is central to our method and that ensures strong internal consistency properties. Detailed theory for such inferences, enabling for example more layers of sub-categories as might occur in tree-like data base structures, has yet to be developed, but is conceptually straightforward and in line with the illustrations for more basic inferences presented in this paper.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s038.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Jury size and composition - a predictive approach</title>
      <authors>
        <author>
          <name>Frank Coolen</name>
          <email>frank.coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Brett Houlding</name>
          <email>brett.houlding@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Steven Parkinson</name>
          <email>s.g.parkinson@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise beta model</keyword>
        <keyword>lower probability</keyword>
        <keyword>nonparametric predictive inference</keyword>
        <keyword>representation of subgroups</keyword>
      </keywords>
      <abstract>We consider two basic aspects of juries that must decide on guilt verdicts, namely the size of juries and their composition in situations where society consists of sub-populations. We refer to the actual jury that needs to provide a verdict as the `first jury', and as their judgement should reflect that of society, we consider an imaginary `second jury' to represent society. The focus is mostly on a lower probability of a guilty verdict by the second jury, conditional on a guilty verdict by the first jury, under suitable exchangeability assumptions between this second jury and the first jury. Using a lower probability of a guilty verdict naturally provides a `benefit of doubt to the defendant' robustness of the inference. By use of a predictive approach, no assumptions on the guilt of a defendant are required, which distinguishes this approach from those presented before. The statistical inferences used in this paper are relatively straightforward, as only cases are considered where the lower probabilities according to Coolen's Nonparametric Predictive Inference for Bernoulli random quantities and Walley's Imprecise Beta Model coincide.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s019.pdf</pdf>
    </paper>
    <paper>
      <id>056</id>
      <title>On various definitions of the variance of a fuzzy random variable</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Gijon</name>
              <latitude>43.53573</latitude>
              <longitude>-5.66152</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Susana Montes</name>
          <email>montes@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Gijon</name>
              <latitude>43.53573</latitude>
              <longitude>-5.66152</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Luciano Sanchez</name>
          <email>luciano@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Gijon</name>
              <latitude>43.53573</latitude>
              <longitude>-5.66152</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy random variable</keyword>
        <keyword>random set</keyword>
        <keyword>variance</keyword>
        <keyword>second order possibility measure</keyword>
      </keywords>
      <abstract>According to the current literature, there are two different approaches to the definition of the variance of a fuzzy random variable. In the first one, the variance is defined as a fuzzy interval, offering a gradual description of our incomplete knowledge about the variance of an underlying, imprecisely observed, classical random variable. In the second case, the variance of the fuzzy random variable is defined as a crisp number, that makes it easier to handle in further processing. In this work, we introduce yet another definition of the variance of a fuzzy random variable, in the context of the theory of imprecise probabilities. The new variance is not defined as a fuzzy or crisp number, but it is a real interval, which is a compromise between both previous definitions. Our main objectives are twofold: first, we show the interpretation of the new variance and, second, with the help of simple examples, we demonstrate the usefulness of all these definitions when applied to particular situations.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s056.pdf</pdf>
    </paper>
    <paper>
      <id>054</id>
      <title>Independence concepts in evidence theory</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Gijon</name>
              <latitude>43.53573</latitude>
              <longitude>-5.66152</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>evidence theory</keyword>
        <keyword>independence</keyword>
        <keyword>random set</keyword>
        <keyword>sets of probabilities</keyword>
      </keywords>
      <abstract>We study three conditions of independence within Evidence Theory framework. First condition refers to the selection of pairs of focal sets. The remaining two are related to the choice of a pair of elements, once a pair of focal sets has been selected. These three concepts allow us to formalize the ideas of lack of interaction between variables and between their (imprecise) observations. We illustrate the difference between both types of independence with simple examples about drawing balls from urns. We show that there are not implication relationships between both of them. We derive interesting conclusions about the relationships between the concepts of "independence in the selection'' and "random set independence''.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s054.pdf</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>On coherent immediate prediction: Connecting two theories of imprecise probability</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Filip Hermans</name>
          <email>fiher@psb.ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>game-theoretic probability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>coherence</keyword>
        <keyword>conglomerability</keyword>
        <keyword>event tree</keyword>
        <keyword>lower prevision</keyword>
        <keyword>immediate prediction</keyword>
        <keyword>prequential principle</keyword>
        <keyword>law of large numbers</keyword>
        <keyword>hoeffding's inequality</keyword>
      </keywords>
      <abstract>We give an overview of two approaches to probability theory where lower and upper probabilities, rather than probabilities, are used: Walley's behavioural theory of imprecise probabilities, and Shafer and Vovk's game-theoretic account of probability. We show that the two theories are more closely related than would be suspected at first sight, and we establish a correspondence between them that (i) has an interesting interpretation, and (ii) allows us to freely import results from one theory into the other. Our approach leads to an account of immediate prediction in the framework of Walley's theory, and we prove an interesting and quite general version of the weak law of large numbers.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s040.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>Immediate prediction under exchangeability and representation insensitivity</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Enrique Miranda</name>
          <email>enrique.miranda@urjc.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Mostoles</name>
              <latitude>40.32234</latitude>
              <longitude>-3.86496</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>Erik.Quaeghebeur@UGent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>predictive inference</keyword>
        <keyword>immediate prediction</keyword>
        <keyword>lower prevision</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>coherence</keyword>
        <keyword>exchangeability</keyword>
        <keyword>representation invariance</keyword>
        <keyword>representation insensitivity</keyword>
        <keyword>imprecise dirichlet-multinomial model</keyword>
        <keyword>johnson's sufficientness postulate</keyword>
      </keywords>
      <abstract>We consider immediate predictive inference, where a subject, using a number of observations of a finite number of exchangeable random variables, is asked to coherently model his beliefs about the next observation, in terms of a predictive lower prevision. We study when such predictive lower previsions are representation insensitive, meaning that they are essentially independent of the choice of the (finite) set of possible values for the random variables. Such representation insensitive predictive models have very interesting properties, and among such models, the ones produced by the Imprecise Dirichlet-Multinomial Model are quite special in a number of ways.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s005.pdf</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>Constructing Predictive Belief Functions from Continuous Sample Data Using Confidence Bands</title>
      <authors>
        <author>
          <name>Thierry Denoeux</name>
          <email>tdenoeux@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Astride Aregui</name>
          <email>aaregui@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dempster-shafer theory</keyword>
        <keyword>evidence theory</keyword>
        <keyword>transferable belief model</keyword>
        <keyword>p-box</keyword>
        <keyword>distribution band</keyword>
      </keywords>
      <abstract>We consider the problem of quantifying our belief in future values of a random variable X with unknown distribution P_X, based on the observation of a random sample from the same distribution. The adopted uncertainty representation framework is the Transferable Belief Model, a subjectivist interpretation of belief function theory. In a previous paper, the concept of predictive belief function at a given confidence level was introduced, and it was shown how to build such a function when X is discrete. This work is extended here to the case where X is a continuous random variable, based on step or continuous confidence bands.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s046.pdf</pdf>
    </paper>
    <paper>
      <id>042</id>
      <title>Relating Imprecise Representations of imprecise Probabilities</title>
      <authors>
        <author>
          <name>Sebastien Destercke</name>
          <email>desterck@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Cadarache</name>
              <latitude>43.71561</latitude>
              <longitude>5.77390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Eric Chojnacki</name>
          <email>eric.chojnacki@irsn.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Cadarache</name>
              <latitude>43.71561</latitude>
              <longitude>5.77390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>random set</keyword>
        <keyword>possibility distribution</keyword>
        <keyword>probability intervals</keyword>
        <keyword>p-box</keyword>
        <keyword>clouds</keyword>
      </keywords>
      <abstract>There exist many practical representations of probability families that make them easier to handle. Among them are random sets, possibility distributions, probability intervals, Ferson's p-boxes and Neumaier's clouds. Both for theoretical and practical considerations, it is important to know whether one representation has the same expressive power than other ones, or can be approximated by other ones. In this paper, we mainly study the relationships between the two latter representations and the three other ones.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s042.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>Coherence and Fuzzy Reasoning</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>upper and lower conditional previsions</keyword>
        <keyword>hausdorff outer and inner measures</keyword>
        <keyword>disintegration property</keyword>
        <keyword>fuzzy reasoning</keyword>
        <keyword>conditional possibility distribution</keyword>
      </keywords>
      <abstract>Upper and lower conditional previsions are defined by the Choquet integral with respect to the Hausdorff outer and inner measures when the conditioning events have positive and finite Hausdorff outer or inner measures in their dimension; otherwise, when conditioning events have infinite or zero Hausdorff outer or inner measures in their dimension, they are defined by a 0-1 valued finitely, but not countably additive probability. It is proven that, if we consider the restriction of the (outer) Haudorff measures to the Borel sigma-field, these (upper) conditional and unconditional previsions satisfy the disintegration property in the sense of Dubins with respect to all countable partitions of Omega This result is obtained as a consequence of the fact that non-disintegrability characterizes finitely as opposed to countably additive probability. Moreover upper and lower conditional previsions are proven to be coherent, in the sense of Walley, with the uncondintional previsions. Properties related to the coherence of upper conditional probabilities are extended to the case where information is represented by fuzzy sets. In particular, given an infinite set Omega a conditioning rule for possibility distribution is proposed so that it is coherent and it is coherent with the unconditional possibility distribution. Through this conditional possibility distribution, a conditional possibility measure with respect to the partition of all singletons of [0,1] is defined. It is proved it satisfies the conglomerative principle of de Finetti.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s037.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>Distributions over Expected Utilities in Decision Analysis</title>
      <authors>
        <author>
          <name>Love Ekenberg</name>
          <email>lovek@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mats Danielson</name>
          <email>mad@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mikael Andersson</name>
          <email>mikaela@math.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Aron Larsson</name>
          <email>aron.larsson@miun.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Sundsvall</name>
              <latitude>62.39129</latitude>
              <longitude>17.30630</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision analysis</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>imprecise utility</keyword>
        <keyword>hierarchical model</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s028.pdf</pdf>
    </paper>
    <paper>
      <id>061</id>
      <title>Multiparameter models: Probability distributions parameterized by random sets</title>
      <authors>
        <author>
          <name>Thomas Fetz</name>
          <email>Thomas.Fetz@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>random set</keyword>
        <keyword>lower and upper probabilities</keyword>
        <keyword>sets of probability measures</keyword>
        <keyword>parameterized probability measures</keyword>
        <keyword>sets of joint probability measures</keyword>
        <keyword>strong independence</keyword>
        <keyword>random set independence</keyword>
        <keyword>unknown interaction</keyword>
      </keywords>
      <abstract>This paper is devoted to the construction of sets of joint probability measures for the case that the marginal sets of probability measures are generated by probability measures with uncertain parameters where the uncertainty of these parameters is modelled by random sets. Futher we show how different conditions on the choice of the weights of the joint focal sets and on the probability measures associated to these sets lead to different sets of joint probability measures including the cases of strong independence, random set independence and unknown interaction.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s061.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>An extension of chaotic probability models to real-valued variables</title>
      <authors>
        <author>
          <name>Pablo Fierens</name>
          <email>pfierens@itba.edu.ar</email>
          <location>
            <country>
              <code>AR</code>
              <name>Argentina</name>
            </country>
            <city>
              <name>Buenos Aires</name>
              <latitude>-34.60333</latitude>
              <longitude>-58.38167</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>foundations of probability</keyword>
        <keyword>chaotic probability models</keyword>
        <keyword>frequentist interpretation</keyword>
      </keywords>
      <abstract>Previous works have presented a frequentist interpretation of sets of measures as probabilistic models which have denominated chaotic models. Those models, however, dealt only with sets of probability measures on finite algebras, that is, probability measures which can be related to variables with a finite number of possible values. In this paper, an extension of chaotic models is proposed in order to deal with the more general case of real-valued variables.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s008.pdf</pdf>
    </paper>
    <paper>
      <id>057</id>
      <title>Some results on imprecise conditional prevision assessments</title>
      <authors>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Veronica Biazzo</name>
          <email>vbiazzo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional random quantities</keyword>
        <keyword>imprecise conditional prevision assessments</keyword>
        <keyword>generalized coherence</keyword>
        <keyword>total coherence</keyword>
        <keyword>connection property</keyword>
      </keywords>
      <abstract>\begin{abstract} \noindent In this paper we consider conditional prevision assessments on random quantities with finite set of possible values. After some preliminaries, we give the notions of generalized coherence and total coherence for imprecise conditional prevision assessments on finite families of conditional random quantities. Then, we examine some results on total coherence of such conditional previsions under different assumptions for the conditioning events. We first consider the case of logically incompatible conditioning events; then, we examine the case of logical independence. Finally, we examine the general case in which there may be some logical dependencies among the conditioning events. We show that in such case the property of total coherence is generally lost, while it is always valid a connection property. By exploiting such property, we obtain suitable totally coherent sets of conditional prevision assessments. We also give a necessary and sufficient condition of total coherence for interval-valued conditional prevision assessments. \end{abstract}</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s057.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>Data-Based Decisions under Imprecise Probability and Least Favorable Models</title>
      <authors>
        <author>
          <name>Robert Hable</name>
          <email>Robert.Hable@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision theory</keyword>
        <keyword>robust statistics</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>coherent upper expectations</keyword>
        <keyword>le cam</keyword>
        <keyword>equivalence of models</keyword>
        <keyword>least favorable models</keyword>
      </keywords>
      <abstract>Data-based decision theory under imprecise probability has to deal with optimisation problems where direct solutions are often computationally intractable. Using the $\Gamma$-minimax optimality criterion, the computational effort may significantly be reduced in the presence of a least favorable model. In 1984, A. Buja derived a neccessary and sufficient condition for the existence of a least favorable model in a special case. The present article proofs that essentially the same result is valid in case of general coherent upper expectations. This is done mainly by topological arguments in combination with some of L. Le Cam's decision theoretic concepts. It is shown how least favorable models could be used to deal with situations where the distribution of the data as well as the prior is assumed to be imprecise.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s031.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Climbing the Hills of Compiled Credal Networks</title>
      <authors>
        <author>
          <name>Rolf Haenni</name>
          <email>haenni@iam.unibe.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Berne</name>
              <latitude>46.94809</latitude>
              <longitude>7.44744</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>bayesian networks</keyword>
        <keyword>credal set</keyword>
        <keyword>approximate inference</keyword>
        <keyword>logical compilation</keyword>
        <keyword>hill-climbing</keyword>
        <keyword>local search</keyword>
      </keywords>
      <abstract>This paper introduces a new approximate inference algorithm for credal networks. The algorithm consists of two major steps. It starts by representing the credal network as a compiled logical theory. The resulting structure is the basis on which the subsequent steepest-ascent hill-climbing algorithm operates. The output of the algorithm is an inner approximation of the exact lower and upper posterior probabilities.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s016.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Quantile-Filtered Bayesian Learning for the Correlation Class</title>
      <authors>
        <author>
          <name>Hermann Held</name>
          <email>held@pik-potsdam.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Potsdam</name>
              <latitude>52.39886</latitude>
              <longitude>13.06566</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian updating</keyword>
        <keyword>generalized bayes</keyword>
      </keywords>
      <abstract>We introduce a new rule for Bayesian updating of imprecise priors that are equivalent to classes of precise priors. The rule combines (a modified version of) Walley's generalized Bayes rule with a filter based on prior quantiles of the observational evidence. We introduce this new "quantile-filtered Bayesian update rule" because in many situations, Walley's generalized Bayes rule reveals counter-intuitively noninformative, dilation-type results while an alternative rule, the maximum likelihood update rule after Gilboa and Schmeidler, is not robust against imprecise priors that are contaminated with spurious information. Our new quantile-based update rule addresses the former issue and fully resolves the latter. We demonstrate the capabilities of the new rule by updating a variant of an imprecise prior that was recently further motivated by expert interviews with climate, ecosystem and economic modelers: Tchen's "correlation class" of precise priors with arbitrary correlation structure, however, prescribed precise marginals. Finally for a stylized insurance situation we demonstrate that according to our new update rule a subset of clients would be insured that is disregarded under standard generalized Bayesian updating.ized Bayesian updating.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s020.pdf</pdf>
    </paper>
    <paper>
      <id>069</id>
      <title>On the Explanatory Power of Indeterminate Probabilities</title>
      <authors>
        <author>
          <name>Jeffrey Helzner</name>
          <email>jh2239@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Horacio Arlo-Costa</name>
          <email>hcosta@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>normative descriptive indeterminate rationality</keyword>
      </keywords>
      <abstract>Building on work that we reported at ISIPTA 2005 we revisit claims made by Fox and Tversky concerning their "comparative ignorance" hypothesis for decision making under uncertainty.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s069.pdf</pdf>
    </paper>
    <paper>
      <id>045</id>
      <title>Information Processing  under Imprecise Risk with the Hurwicz criterion</title>
      <authors>
        <author>
          <name>Jean-Yves Jaffray</name>
          <email>Jean-Yves.Jaffray@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Meglena Jeleva</name>
          <email>Meglena.Jeleva@univ-paris1.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise risk</keyword>
        <keyword>hurwicz criterion</keyword>
        <keyword>resolute choice</keyword>
        <keyword>non-consequentialism</keyword>
        <keyword>learning</keyword>
      </keywords>
      <abstract>An agent has Hurwicz criterion with pessimism-optimism index alpha under imprecise risk and adopts McClennen's Resolute Choice in sequential decision situations, i.e. evaluates strategies at the root of the decision tree by the Hurwicz criterion and enforces the best strategy, thus behaving in a dynamically consistent manner. We address two questions raised by this type of behavior: (i) is information processed correctly? and (ii) to what extent do unrealized outcomes influence decisions (non-consequentialism)? Partial answers are provided by studying: (i) the random sampling of a binary variable, and finding the influence of the pessimism-optimism index to be decreasing with the sample size, and the optimal decision rule to asymptotically only depend on the relative frequencies observed; and (ii) an insurance problem in which the agent chooses his coverage at period two after observing the period one outcome (accident or no accident); when no accident happened, a seemingly irrelevant data - the first period deductible level- is found to be able to influence the second period insurance choice. We analyse this result in relation with the existence and value of the pessimism-optimism degree.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s045.pdf</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>Compositional Models of Belief Functions</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Milan Daniel</name>
          <email>milan.daniel@cs.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>basic assignment</keyword>
        <keyword>multidimensional frame of discernment</keyword>
        <keyword>operator of composition</keyword>
        <keyword>perfect sequence</keyword>
      </keywords>
      <abstract>After it has been successfully done in probability and possibility theories, the paper is the first attempt to introduce the operator of composition also for belief functions. We prove that the proposed definition preserves all the necessary properties of the operator enabling us to define compositional models as an efficient tool for multidimensional models representation.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s048.pdf</pdf>
    </paper>
    <paper>
      <id>058</id>
      <title>Enhancement of Natural Extension</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>igor.kozine@risoe.dk</email>
          <location>
            <country>
              <code>DK</code>
              <name>Denmark</name>
            </country>
            <city>
              <name>Roskilde</name>
              <latitude>55.64152</latitude>
              <longitude>12.08035</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Victor Krymsky</name>
          <email>kvg@mail.rb.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Ufa</name>
              <latitude>54.78517</latitude>
              <longitude>56.04562</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>statistical reasoning</keyword>
        <keyword>natural extension</keyword>
        <keyword>variational calculus</keyword>
        <keyword>reliability analysis</keyword>
      </keywords>
      <abstract>The theory of imprecise previsions admits the use of a wide variety of statistical evidence. Nevertheless, some existing evidence, for example, in reliability applications, cannot be utilized by models developed within its framework. In the pursuit of reducing imprecision, any available evidence should become an input to modeling. It is suggested to take a different look at the natural extension, the basic constructive step in the theory. It shown that natural extension can be viewed as a problem belonging to the realm of variational calculus, which opens up new perspectives for obtaining tighter intervals.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s058.pdf</pdf>
    </paper>
    <paper>
      <id>073</id>
      <title>Updating and Testing Beliefs: An Open Version of Bayes' Rule</title>
      <authors>
        <author>
          <name>Elmar Kriegler</name>
          <email>elmar@cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian updating</keyword>
        <keyword>prediction</keyword>
        <keyword>model accuracy</keyword>
        <keyword>epsilon-contamination model</keyword>
        <keyword>ar process</keyword>
      </keywords>
      <abstract>Developing models to describe real systems is a challenge because it is difficult to assess and control the residual between the two entities. Bayesian updating of a belief about model accuracy across an ensemble of available models can lead to spurious results, since the application of Bayes' rule presupposes that an accurate model is contained in the ensemble with certainty. We present a framework in which this assumption can be dropped. The basic idea is to extend Bayes' rule to the exhaustive, but unknown space of all models, and then contract it again to the known set of models by making best/worst case assumptions for the remaining space. We show that this approach leads to an epsilon-contamination model for the posterior belief, where the epsilon-contamination is updated along with the distribution of belief across available models. In essence, the epsilon-contamination provides an additional test on the accuracy of the overall model ensemble compared to the data, and will grow rapidly if the ensemble fails such a test. We demonstrate our concept with an example of autoregressive processes.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s073.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>On sigma-additive robust representations of convex risk measures for unbounded financial positions in the presence of uncertainty of the market model</title>
      <authors>
        <author>
          <name>Volker Kraetschmer</name>
          <email>KRAETSCH@math.tu-berlin.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Berlin</name>
              <latitude>52.52330</latitude>
              <longitude>13.41377</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>convex risk measure</keyword>
        <keyword>convex upper previsions</keyword>
        <keyword>model uncertainty</keyword>
        <keyword>sigma-additive robust representation</keyword>
        <keyword>fatou property</keyword>
        <keyword>nonsequential fatou property</keyword>
        <keyword>strong sigma-additive robust representation</keyword>
        <keyword>krein-smulian theorem</keyword>
        <keyword>greco theorem</keyword>
        <keyword>inner daniell stone theorem</keyword>
        <keyword>general dini theorem</keyword>
        <keyword>simons' lemma</keyword>
      </keywords>
      <abstract>Recently, Frittelli and Scandolo extend the notion of risk measures, originally introduced by Artzner, </abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s013.pdf</pdf>
    </paper>
    <paper>
      <id>044</id>
      <title>Estimating Probability Distributions by Observing Betting Practices</title>
      <authors>
        <author>
          <name>Caroline Lynch</name>
          <email>caroline.lynch@nuigalway.ie</email>
          <location>
            <country>
              <code>IE</code>
              <name>Ireland</name>
            </country>
            <city>
              <name>Galway</name>
              <latitude>53.27194</latitude>
              <longitude>-9.04889</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Don Barry</name>
          <email>vpa@ul.ie</email>
          <location>
            <country>
              <code>IE</code>
              <name>Ireland</name>
            </country>
            <city>
              <name>Limerick</name>
              <latitude>52.66472</latitude>
              <longitude>-8.62306</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>em algorithm</keyword>
        <keyword>bookmaker</keyword>
        <keyword>horse race</keyword>
        <keyword>markov decision process</keyword>
      </keywords>
      <abstract>A bookmaker takes bets on a two-horse race, attempting to minimise expected loss over all possible outcomes of the race. Profits are controlled by manipulation of customers' betting behaviour; in order to do this, we need some information about the probability distribution which describes how the customers will bet. We examine what information initial customers' betting behaviour provides about this probability distribution, and consider how to use this to estimate the probability distribution for remaining customers.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s044.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>An independence concept under plausibility function</title>
      <authors>
        <author>
          <name>Marcello Mastroleo</name>
          <email>mastroleo@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>totaly monotone measures</keyword>
        <keyword>plausibility</keyword>
        <keyword>conditioning</keyword>
        <keyword>independence</keyword>
      </keywords>
      <abstract>Starting from considering different definitions of conditioning for decomposable measures, in particular for totaly monotone measures (belief functions) and totaly alternating measures (plausibility functions), we provide a concept of independence which covers some natural properties. In particular, we characterize the proposed independence for plausibility functions and we check some relevant properties. Relationships with other notion studied in literature are shown.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s053.pdf</pdf>
    </paper>
    <paper>
      <id>060</id>
      <title>Coherence graphs</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>enrique.miranda@urjc.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Mostoles</name>
              <latitude>40.32234</latitude>
              <longitude>-3.86496</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>walley coherence</keyword>
        <keyword>weak coherence</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>graphical model</keyword>
        <keyword>coherence graph</keyword>
      </keywords>
      <abstract>We consider the task of proving Walley's (joint or strong) coherence of a number of probabilistic assessments, when these assessments are represented as a collection of conditional lower previsions. In order to maintain generality in the analysis, we assume to be given nearly no information about the numbers that make up the lower previsions in the collection. Under this condition, we investigate the extent to which the above global task can be decomposed into simpler and more local ones. This is done by introducing a graphical representation of the conditional lower previsions, that we call the coherence graph: we show that the coherence graph allows one to isolate some subsets of the collection whose coherence is sufficient for the coherence of all the assessments. The situation is shown to be completely analogous in the case of Walley's notion of weak coherence, for which we prove in addition that the subsets found are optimal, in the sense that they embody the maximal degree to which the task of checking weak coherence can be decomposed. In doing all of this, we obtain a number of related results: we give a new characterisation of weak coherence; we characterise, by means of a special kind of coherence graph, when the local notion of separate coherence is sufficient for coherence; and we provide an envelope theorem for collections of lower previsions whose graph is of the latter type.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s060.pdf</pdf>
    </paper>
    <paper>
      <id>076</id>
      <title>Scoring Rules, Entropy, and Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Robert Winkler</name>
          <email>rwinkler@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Victor Richmond Jose</name>
          <email>vrj@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>entropy</keyword>
        <keyword>divergence</keyword>
        <keyword>scoring rules</keyword>
        <keyword>portfolio optimization</keyword>
        <keyword>incomplete markets</keyword>
      </keywords>
      <abstract>Suppose that a risk-averse expected utility maximizer with a precise probability distribution p bets optimally against a risk neutral opponent (or equivalently invests in an incomplete market for contingent claims) whose beliefs (or prices) are described by a convex set Q of probability distributions. This utility-maximization problem is the dual of the problem of finding the distribution q in Q that minimizes a generalized divergence (relative entropy) with respect to p. A special case is that of logarithmic utility, in which the corresponding divergence is the Kullback-Leibler divergence, but we present a closed-form solution for the entire family of linear-risk-tolerance (a.k.a. HARA) utility functions and show that this corresponds to a particular parametric family of generalized divergences, which is derived from an entropy measure originally proposed by Arimoto and which is also related to a generalization of pseudospherical scoring rule originally proposed by I.J. Good. A variant of this decision problem, in which the decision maker has quasilinear utility for consumption over two periods, leads to the family of power divergences, which is related to a generalization of the power family of scoring rules.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s076.pdf</pdf>
    </paper>
    <paper>
      <id>032</id>
      <title>Imprecise probability methods for sensitivity analysis in engineering</title>
      <authors>
        <author>
          <name>Michael Oberguggenberger</name>
          <email>michael@mat1.uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Julian King</name>
          <email>csae2209@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Bernhard Schmelzer</name>
          <email>csae1209@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>reliability of structures</keyword>
        <keyword>sensitivity analysis</keyword>
        <keyword>random set</keyword>
        <keyword>fuzzy set</keyword>
        <keyword>simulation methods</keyword>
        <keyword>aerospace engineering</keyword>
      </keywords>
      <abstract>This article addresses questions of sensitivity of output values in engineering models with respect to variations in the input parameters. Such an analysis is an important ingredient in the assessment of the safety and reliability of structures. A major challenge in engineering applications lies in the fact that high computational costs have to be faced. Methods have to be developed that admit assertions about the sensitivity of the output with as few computations as possible. This article serves to explore various techniques from imprecise probability that may contribute to achieving this goal.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s032.pdf</pdf>
    </paper>
    <paper>
      <id>039</id>
      <title>Lucenos discretization methods and its application in decision making under ambiguity</title>
      <authors>
        <author>
          <name>Michael Obermeier</name>
          <email>obermeierm@web.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making under ambiguity</keyword>
        <keyword>discretization</keyword>
        <keyword>gaussian quadrature</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval probability</keyword>
        <keyword>linear programming</keyword>
        <keyword>luceno</keyword>
        <keyword>numerical integration</keyword>
      </keywords>
      <abstract>NA</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s039.pdf</pdf>
    </paper>
    <paper>
      <id>047</id>
      <title>Some Bounds for Conditional Lower Previsions</title>
      <authors>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolo.vicig@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional lower previsions</keyword>
        <keyword>product rule</keyword>
        <keyword>bayes' theorem</keyword>
        <keyword>williams coherence</keyword>
        <keyword>centered convex previsions</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s047.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Human reasoning with imprecise probabilities: Modus ponens and Denying the antecedent</title>
      <authors>
        <author>
          <name>Niki Pfeifer</name>
          <email>niki.pfeifer@sbg.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Salzburg</name>
              <latitude>47.79941</latitude>
              <longitude>13.04399</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gernot Kleiter</name>
          <email>gernot.kleiter@sbg.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Salzburg</name>
              <latitude>47.79941</latitude>
              <longitude>13.04399</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>mental probability logic</keyword>
        <keyword>modus ponens</keyword>
        <keyword>coherence</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>The 'modus ponens' is, along with 'modus tollens' and the two logically not valid counterparts 'denying the antecedent' and 'affirming the consequent', the argument form that was most often investigated in the psychology of human reasoning. The present contribution reports the results of three experiments on the probabilistic versions of 'modus ponens' and 'denying the antecedent'. In probability logic these arguments lead to conclusions with imprecise probabilities. In the 'modus ponens' tasks the participants inferred probabilities that agreed much better with the coherent normative values than in the 'denying the antecedent' tasks, a result that mirrors results found with the classical argument versions. For 'modus ponens' a surprisingly high number of lower and upper probabilities agreed perfectly with the conjugacy property (upper probabilities equal one complements of the lower probabilities). When the probabilities of the premises are imprecise the participants do not ignore irrelevant ('silent') boundary probabilities. The results show that human mental probability logic is close to predictions derived from probability logic for the most elementary argument form, but has considerable difficulties with the more complex forms involving negations.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s050.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>Learning about a Categorical Latent Variable under Prior Near-Ignorance</title>
      <authors>
        <author>
          <name>Alberto Piatti</name>
          <email>alberto.piatti@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Trojani</name>
          <email>fabio.trojani@unisg.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Saint Gallen</name>
              <latitude>47.42391</latitude>
              <longitude>9.37477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marcus Hutter</name>
          <email>marcus@hutter1.net</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>prior near-ignorance</keyword>
        <keyword>latent and manifest variables</keyword>
        <keyword>observational processes</keyword>
        <keyword>vacuous beliefs</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>It is well known that complete prior ignorance is not compatible with learning, at least in a coherent theory of (epistemic) uncertainty. What is less widely known, is that there is a state similar to full ignorance, that Walley calls near-ignorance, that permits learning to take place. In this paper we provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is latent. We argue that such a setting is by far the most common case in practice, and we show, for the case of categorical latent variables (and general manifest variables) that there is a sufficient condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied in the most common statistical problems.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s049.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>Conditioning in Chaotic Probabilities Interpreted as a Generalized Markov Chain</title>
      <authors>
        <author>
          <name>Leandro Rego</name>
          <email>leandro@de.ufpe.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>foundations of probability</keyword>
        <keyword>church place selection rules</keyword>
        <keyword>probabilistic reasoning</keyword>
        <keyword>conditioning</keyword>
        <keyword>complexity</keyword>
      </keywords>
      <abstract>We propose a new definition for conditioning in the Chaotic Probability framework. We show that the Conditional Chaotic Probability model that we propose can be given the interpretation of a generalized Markov chain. Chaotic Probabilities were introduced by Fine et al. as an attempt to model chance phenomena with a usual set of measures ${\cal M}$ endowed with an {\em objective, frequentist interpretation} instead of a compound hypothesis or behavioral subjective one. We follow the presentation of the univariate case chaotic probability model and provide an instrumental interpretation of random process measures consistent with a conditional chaotic probability source, which can be used as a tool for simulation of our model. Given a finite time series, we also present a universal method for estimation of conditional chaotic probability models that is based on the analysis of the relative frequencies taken along a set of subsequences chosen by a given set of rules.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s043.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Qualitative and Quantitative Reasoning in Hybrid Probabilistic Logic Programs</title>
      <authors>
        <author>
          <name>Emad Saad</name>
          <email>emad.saad@adu.ac.ae</email>
          <location>
            <country>
              <code>AE</code>
              <name>United Arab Emirates</name>
            </country>
            <city>
              <name>Abu Dhabi</name>
              <latitude>24.46667</latitude>
              <longitude>54.36667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probabilistic logic program</keyword>
        <keyword>probabilistic reasoning</keyword>
        <keyword>knowledge representation</keyword>
      </keywords>
      <abstract>Reasoning with qualitative and quantitative uncertainty is required in some real-world applications [8]. However, current extensions to logic programming with uncertainty support representing and reasoning with either qualitative or quantitative uncertainty. In this paper we extend the language of Hybrid Probabilistic Logic programs [36, 32], originally introduced for reasoning with quantitative uncertainty, to support both qualitative and quantitative uncertainty. We propose to combine disjunctive logic programs [12, 21] with Extended and Normal Hybrid Probabilistic Logic Programs (EHPP [32] and NHPP [36]) in a unified logic programming framework, to allow directly and intuitively to represent and reason in the presence of both qualitative and quantitative uncertainty. The semantics of the proposed languages are based on the answer set semantics and stable model semantics of extended and normal disjunctive logic programs [12, 21]. In addition, they also rely on the probabilistic answer set semantics and the stable probabilistic model semantics of EHPP [32] and NHPP [36].</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s021.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Coherent Choice Functions under Uncertainty</title>
      <authors>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mark Schervish</name>
          <email>mark@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>choice function</keyword>
        <keyword>coherence</keyword>
        <keyword>gamma-maximin</keyword>
        <keyword>maximality</keyword>
        <keyword>uncertainty</keyword>
        <keyword>state-independent utility</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s015.pdf</pdf>
    </paper>
    <paper>
      <id>062</id>
      <title>Multilinear and Integer Programming for Markov Decision Processes with Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Ricardo Shirota Filho</name>
          <email>ricardo.shirota@poli.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Felipe Trevizan</name>
          <email>felipe.trevizan@upf.edu</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Barcelona</name>
              <latitude>41.38879</latitude>
              <longitude>2.15899</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cassio Campos</name>
          <email>cassio@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Leliane Barros</name>
          <email>leliane@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>markov decision processes with imprecise probabilities</keyword>
        <keyword>maximin criterion</keyword>
        <keyword>multilinear and integer programming</keyword>
      </keywords>
      <abstract>Markov Decision Processes (MDPs) are extensively used to encode sequences of decisions with probabilistic effects. Markov Decision Processes with Imprecise Probabilities (MDPIPs) encode sequences of decisions whose effects are modeled using sets of probability distributions. In this paper we examine the computation of Gamma-maximin policies for MDPIPs using multilinear and integer programming. We discuss the application of our algorithms to ``factored'' models and to a recent proposal, Markov Decision Processes with Set-valued Transitions (MDPSTs), that unifies the fields of probabilistic and ``nondeterministic'' planning in artificial intelligence research.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s062.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>Regular finite Markov chains with interval probabilities</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>markov chain</keyword>
        <keyword>interval probability</keyword>
      </keywords>
      <abstract>In Markov chain theory a stochastic matrix $P$ is regular if some matrix power $P^n$ contains only strictly positive elements. Regularity of transition matrix of a Markov chain guarantees the existence of a unique invariant distribution which is also the limiting distribution. In the present paper a similar result is shown for the generalized Markov chain model that replaces classical probabilities with interval probabilities. We generalize the concept of regularity and show that for a regular interval transition matrix sets of probabilities corresponding to consecutive steps of a Markov chain converge to a unique limiting set of distributions that only depends on transition matrix and is independent of the initial distribution. A similar convergence result is also shown for approximations of the invariant set.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s033.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Minimax Regret Treatment Choice with Finite Samples and Missing Outcome Data</title>
      <authors>
        <author>
          <name>Joerg Stoye</name>
          <email>j.stoye@nyu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>minimax regret</keyword>
        <keyword>missing data</keyword>
        <keyword>imprecise probability models</keyword>
        <keyword>statistical decision theory</keyword>
        <keyword>partial identification</keyword>
        <keyword>treatment evaluation</keyword>
      </keywords>
      <abstract>This paper uses the minimax regret criterion to analyze choice between two treatments when one has observed a finite sample that is plagued by missing data. The analysis is entirely in terms of exact finite sample regret, as opposed to asymptotic approximations or finite sample bounds. It thus extends Manski (in press), who largely abstracts from finite sample problems, as well as Stoye (2006a), who provides finite sample results but abtracts from missing data. Core findings are: (i) Minimax regret is achieved by randomizing over two rules that were identified in the aforecited papers. (ii) For every sample size, there exists a sufficiently small (but positive) proportion of missing data such that if less data are missing, the missing data problem is ignored altogether and Stoye's (2006a) results apply. (iii) For every positive fraction of missing data, the value of additional observations drops to zero at a finite sample size. I also provide the decision problem's value function and briefly touch on optimal sample design as well as unknown propensity scores.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s025.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Finite Approximations To Coherent Choice</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>approximation</keyword>
        <keyword>e-admissibility</keyword>
        <keyword>maximality</keyword>
        <keyword>numerical analysis</keyword>
        <keyword>lower prevision</keyword>
        <keyword>sensitivity analysis</keyword>
      </keywords>
      <abstract>This paper studies and bounds the effects of approximating loss functions and credal sets, under very weak assumptions, on choice functions. In particular, the credal set is assumed to be neither convex nor closed. The main result is that the effects of approximation can be bounded, although in general, approximation of the credal set may not always be practically possible. In case of pairwise choice, I demonstrate how the situation can be improved by showing that only approximations of the extreme points of the closure of the convex hull of the credal set need to be taken into account, as expected.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s018.pdf</pdf>
    </paper>
    <paper>
      <id>074</id>
      <title>Computing expectations with p-boxes: two views of the same problem</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>desterck@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Cadarache</name>
              <latitude>43.71561</latitude>
              <longitude>5.77390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>p-box</keyword>
        <keyword>random set</keyword>
        <keyword>linear programming</keyword>
        <keyword>lower/upper expectation</keyword>
        <keyword>optimization</keyword>
      </keywords>
      <abstract>Given an imprecise probabilistic model over a continuous space, computing lower (upper) expectations is often computationally hard to achieve, even in simple cases. Building tractable methods to do so is thus a crucial point in applications. In this paper, we concentrate on p-boxes (a simple and popular model), and on lower expectations computed over non-monotone functions. For various particular cases, we propose tractable methods to compute approximations or exact values of these lower expectations. We found interesting to put in evidence and to compare two approaches: the first using general linear programming, and the second using the fact that p-boxes are special cases of random sets. We underline the complementarity of both approaches, as well as the differences.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s074.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>Linear Regression Analysis under Sets of Conjugate Priors</title>
      <authors>
        <author>
          <name>Gero Walter</name>
          <email>gero.walter@campus.lmu.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Annette Peters</name>
          <email>peters@gsf.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>airgene study</keyword>
        <keyword>analysis of variance</keyword>
        <keyword>exponential family</keyword>
        <keyword>(imprecise) conjugate priors</keyword>
        <keyword>imprecise probability models</keyword>
        <keyword>interval probability</keyword>
        <keyword>prior-data conflict</keyword>
        <keyword>regression</keyword>
        <keyword>robust bayesian inference</keyword>
      </keywords>
      <abstract>Regression is \emph{the} central concept in applied statistics for analyzing multivariate, heterogenous data: The influence of a group of variables on one other variable is quantified by the regression parameter $\beta$. In this paper, we extend standard Bayesian inference on $\beta$ in linear regression models by considering imprecise conjugated priors. Inspired by a variation and an extension of a method for inference in i.i.d.\ exponential families presented at \textsc{isipta}'05 by Quaeghebeur and de Cooman, we develop a general framework for handling linear regression models including analysis of variance models, and discuss obstacles in direct implementation of the method. Then properties of the interval-valued point estimates for a two-regressor model are derived and illustrated with simulated data. As a practical example we take a small data set from the \textsc{airgene} study and consider the influence of age and body mass index on the concentration of an inflammation marker.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s041.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>The Logical Concept of Probability: Foundation and Interpretation</title>
      <authors>
        <author>
          <name>Kurt Weichselberger</name>
          <email>Kurt.Weichselberger@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval probability</keyword>
        <keyword>evaluation of arguments</keyword>
        <keyword>concept of independence</keyword>
        <keyword>frequency interpretation</keyword>
        <keyword>symmetric theory of probability</keyword>
      </keywords>
      <abstract>The Logical concept of probability, introduced to ISIPTA 2005 in a tutorial ([3]), is based on the theory of Interval probability. Since the main feature of the Logical concept is given by the evaluation of arguments consisting of premises and conclusions, it proves necessary to define exactly which kinds of propositions can be employed hereby. If this is done, the analysis allows the definition of independent arguments by examination of the contents of premises and conclusions. If Interval probability is attributed to arguments according to the relevant axioms, a frequency interpretation becomes feasible which decisively relies on the autonomous concept of independence.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta07/proceedings/papers/s034.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2009</year>
    <conference>
      <date>
        <start>2009-07-14</start>
        <end>2009-07-18</end>
      </date>
      <location>
        <country>
          <code>GB</code>
          <name>United Kingdom</name>
        </country>
        <city>
          <name>Durham</name>
          <latitude>54.77639</latitude>
          <longitude>-1.57587</longitude>
        </city>
        <university>
          <name>Durham University</name>
          <department>Department of Mathematical Sciences</department>
        </university>
      </location>
    </conference>
    <paper>
      <id>056</id>
      <title>Iterated Random Selection as Intermediate Between Risk and Uncertainty</title>
      <authors>
        <author>
          <name>Horacio Arlo-Costa</name>
          <email>hcosta@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jeffrey Helzner</name>
          <email>jh2239@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decisions from description</keyword>
        <keyword>decisions from experience</keyword>
        <keyword>random selection</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s056.pdf</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>Closure of  Independencies under Graphoid Properties: Some Experimental Results</title>
      <authors>
        <author>
          <name>Marco Baioletti</name>
          <email>baioletti@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuseppe Busanello</name>
          <email>busanello@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional independence models</keyword>
        <keyword>graphoid properties</keyword>
        <keyword>inferential rules</keyword>
      </keywords>
      <abstract>In this paper we describe an algorithm for computing the closure with respect to graphoid properties of a set of independencies under graphoid properties. Since the computation of the complete closure is infeasible, we propose to use a procedure, called FC1, which is based on a unique inference rule and on the elimination of redundant independencies. FC1 is able to compute a reduced form of the closure, called fast closure, which is equivalent to the complete closure, but whose size is much smaller. Some experimental tests have been performed with an implementation of the procedure in order to show the computational behaviour of the algorithm. We have also compared the computational cost and the size of the fast closure with the corresponding data for the complete closure.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s046.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Category Selection for Multinomial Data</title>
      <authors>
        <author>
          <name>Rebecca Baker</name>
          <email>r.m.baker@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>predictive inference</keyword>
        <keyword>categorical data</keyword>
        <keyword>selection</keyword>
      </keywords>
      <abstract>Based on observations from a multinomial data set, a new method is presented for selecting a single category or the smallest subset of categories, where the selection criterion is a minimally required lower probability that (at least) a specific number of future observations will belong to that category or subset of categories. The inferences about the future observations are made using an extension of Coolen and Augustin's nonparametric predictive inference (NPI) model to a situation with multiple future observations.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s024.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>Aggregating Imprecise Probabilistic Knowledge</title>
      <authors>
        <author>
          <name>Alessio Benavoli</name>
          <email>alessio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>information fusion</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>independent natural extension</keyword>
        <keyword>generalized bayes rule</keyword>
      </keywords>
      <abstract>The problem of aggregating two or more sources of information containing knowledge about a same domain is considered. We propose an aggregation rule for the case where the available information is modeled by coherent lower previsions, corresponding to convex sets of probability mass functions. The consistency between aggregated beliefs and sources of information is discussed. A closed formula, which specializes our rule to a particular class of models, is also derived. Finally, an alternative explanation of Zadeh's paradox is provided.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s043.pdf</pdf>
    </paper>
    <paper>
      <id>001</id>
      <title>Tests of the Mean with Distributional Uncertainty: An Info-Gap Approach</title>
      <authors>
        <author>
          <name>Yakov Ben-Haim</name>
          <email>yakov@technion.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Haifa</name>
              <latitude>32.81841</latitude>
              <longitude>34.98850</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>binary hypothesis tests</keyword>
        <keyword>distributional uncertainty</keyword>
        <keyword>info-gap</keyword>
        <keyword>robustness</keyword>
        <keyword>tests of the mean</keyword>
        <keyword>t test</keyword>
        <keyword>chronic wasting disease</keyword>
        <keyword>false nulls</keyword>
      </keywords>
      <abstract>Statistical tests of the mean are quite common. Sometimes the analyst cannot validate the assumptions underlying the test, such as normality, symmetry, independence of measurements, etc. This causes unknown deviation of the actual sampling distribution from the distribution assumed by the test, and thus unknown size and power of the test. This distributional uncertainty makes it difficult to reliably choose the decision threshold (critical value) and sample size. We present a method for evaluating the robustness of a test to an unknown degree of distributional uncertainty, based on info-gap decision theory. Analysis of robustness is useful in evaluating effective size and power, and for selecting the decision threshold and sample-size. We study binary simple-hypothesis tests of the mean and consider both type I and type II errors. We show quantitatively that robustness to distributional uncertainty improves, at fixed nominal level of significance, as the effective level of significance deteriorates. Likewise, robustness improves as the effective power of the test deteriorates. Furthermore, we show how to choose the decision threshold and sample size in light of distributional uncertainty. We illustrate our results by application to the t-test and to a test of false nulls in epidemiology.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s001.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>On General Conditional Random Quantities</title>
      <authors>
        <author>
          <name>Veronica Biazzo</name>
          <email>vbiazzo@dmi.unict.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Catania</name>
              <latitude>37.50213</latitude>
              <longitude>15.08719</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuseppe Sanfilippo</name>
          <email>sanfilippo@unipa.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Palermo</name>
              <latitude>38.11582</latitude>
              <longitude>13.35976</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional events</keyword>
        <keyword>general conditional random quantities</keyword>
        <keyword>general conditional prevision assessments</keyword>
        <keyword>generalized compound prevision theorem</keyword>
        <keyword>iterated conditioning</keyword>
        <keyword>strong generalized compound prevision theorem</keyword>
      </keywords>
      <abstract>In the first part of this paper, recalling a general discussion on iterated conditioning given by de Finetti in the appendix of his book, vol. 2, we give a representation of a conditional random quantity $X|HK$ as $(X|H)|K$. In this way, we obtain the classical formula $\pr{(XH|K)} =\pr{(X|HK)P(H|K)}$, by simply using linearity of prevision. Then, we consider the notion of general conditional prevision $\pr(X|Y)$, where $X$ and $Y$ are two random quantities, introduced in 1990 in a paper by Lad and Dickey. After recalling the case where $Y$ is an event, we consider the case of discrete finite random quantities and we make some critical comments and examples. We give a notion of coherence for such more general conditional prevision assessments; then, we obtain a strong generalized compound prevision theorem. We study the coherence of a general conditional prevision assessment $\pr(X|Y)$ when $Y$ has no negative values and when $Y$ has no positive values. Finally, we give some results on coherence of $\pr(X|Y)$ when $Y$ assumes both positive and negative values. In order to illustrate critical aspects and remarks we examine several examples.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s031.pdf</pdf>
    </paper>
    <paper>
      <id>045</id>
      <title>Approximation of Coherent Lower Probabilities by 2-Monotone Measures</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>pareto optimal 2-monotone measure</keyword>
        <keyword>additivity on lattices</keyword>
        <keyword>simplex method</keyword>
        <keyword>imprecision indices</keyword>
      </keywords>
      <abstract>The paper investigates outer approximations of coherent lower probabilities by 2-monotone measures. We charac-terize the set of (Pareto)-optimal outer approximations and provide powerful iterative algorithms to calculate such measures.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s045.pdf</pdf>
    </paper>
    <paper>
      <id>042</id>
      <title>On the Use of a New Discrepancy Measure to Correct Incoherent Assessments and to Aggregate  Conflicting Opinions Based on Imprecise Conditional Probabilities</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuliana Regoli</name>
          <email>regoli@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Francesca Vattari</name>
          <email>vattari@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise conditional probabilities</keyword>
        <keyword>inconsistency handling</keyword>
        <keyword>aggregation opinions</keyword>
        <keyword>divergence measures</keyword>
      </keywords>
      <abstract>We give a preliminary study of a new procedure to correct incoherent imprecise conditional probability assessments. The procedure is based on parametric optimization problems which have as objective function a new discrepancy measure. We show through simple examples how the procedure of correcting incoherent assessments can be properly extended to aggregate conflicting opinions, and can be generalized to embed importance weights of each assessment.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s042.pdf</pdf>
    </paper>
    <paper>
      <id>062</id>
      <title>A Generalization of Credal Networks</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian networks</keyword>
        <keyword>credal network</keyword>
        <keyword>graphical model</keyword>
        <keyword>d-separation</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>updating</keyword>
        <keyword>likelihood function</keyword>
        <keyword>hierarchical model</keyword>
        <keyword>fuzzy probabilities</keyword>
      </keywords>
      <abstract>The likelihood approach to statistics can be interpreted as a theory of fuzzy probability. This paper presents a generalization of credal networks obtained by generalizing imprecise probabilities to fuzzy probabilities; that is, by additionally considering the relative plausibility of different values in the probability intervals.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s062.pdf</pdf>
    </paper>
    <paper>
      <id>060</id>
      <title>A Tree Augmented Classifier Based on Extreme Imprecise Dirichlet Model</title>
      <authors>
        <author>
          <name>Giorgio Corani</name>
          <email>giorgio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cassio Campos</name>
          <email>cassio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sun Yi</name>
          <email>yi@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>extreme imprecise dirichlet model</keyword>
        <keyword>classification</keyword>
        <keyword>tanc</keyword>
        <keyword>credal dominance</keyword>
      </keywords>
      <abstract>In this paper we present TANC, i.e., a tree-augmented naive credal classifier based on imprecise probabilities; it models prior near-ignorance via the Extreme Imprecise Dirichlet Model (EDM) (Cano et al., 2007) and deals conservatively with missing data in the training set, without assuming them to be missing-at-random. The EDM is an approximation of the global Imprecise Dirichlet Model (IDM), which considerably simplifies the computation of upper and lower probabilities; yet, having been only recently introduced, the quality of the provided approximation needs still to be verified. As first contribution, we extensively compare the output of the naive credal classifier (one of the few cases in which the global IDM can be exactly implemented) when learned with the EDM and the global IDM; the output of the classifier appears to be identical in the vast majority of cases, thus supporting the adoption of the EDM in real classification problems. Then, by experiments we show that TANC is more reliable than the precise TAN (learned with uniform prior), and also that it provides better performance compared to a previous (Zaffalon, 2003} TAN model based on imprecise probabilities. TANC treats missing data by considering all possible completions of the training set, but avoiding an exponential increase of the computational times; eventually, we present some preliminary results with missing data.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s060.pdf</pdf>
    </paper>
    <paper>
      <id>063</id>
      <title>Sets of Desirable Gambles and Credal Sets</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Gijon</name>
              <latitude>43.53573</latitude>
              <longitude>-5.66152</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>desirable gambles</keyword>
        <keyword>regular conditioning</keyword>
        <keyword>zero probabilities</keyword>
        <keyword>sets of probability measures</keyword>
      </keywords>
      <abstract>Sets of desirable gambles were proposed by Walley (2000) as a general theory of imprecise probability. The main reasons for this are: it is a very general model, including as particular cases most of the existing theories for imprecise probability; it has a deep and simple axiomatic justification; and mathematical definitions are natural and intuitive. However, there is still a lot of work to be done until the theory of desirable gambles is operative for its use in general reasoning tasks. This paper gives an overview of some of the fundamental concepts expressed in terms of desirable gambles in the finite case, gives a characterization of regular extension, and studies the nature of maximally coherent sets of gambles.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s063.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>Concentration Inequalities and Laws of Large Numbers under Epistemic Irrelevance</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>epistemic irrelevance</keyword>
        <keyword>law of large numbers</keyword>
        <keyword>concentration inequalities</keyword>
      </keywords>
      <abstract>This paper presents concentration inequalities and laws of large numbers under weak assumptions of irrelevance, expressed through lower and upper expectations. The results are variants and extensions of De Cooman and Miranda's recent inequalities and laws of large numbers. The proofs indicate connections between concepts of irrelevance for lower/upper expectations and the standard theory of martingales.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s034.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Imprecise Markov Chains with an Absorbing State</title>
      <authors>
        <author>
          <name>Richard Crossman</name>
          <email>r.j.crossman@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Pauline Coolen-Schrijner</name>
          <email>NA</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>absorbing state</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>markov chain</keyword>
        <keyword>time inhomogeneity</keyword>
      </keywords>
      <abstract>Several authors have presented methods for considering the behaviour of Markov chains in the generalised setting of imprecise probability. Some assume a constant transition matrix which is not known precisely, instead bounds are given for each element. Others consider a transition matrix which is neither known precisely nor assumed to be constant, though each element is known to exist within intervals that are constant over time. In both cases results have been published regarding the long-term behaviour of such chains. When a finite Markov chain is considered with a single absorbing state, however, eventual absorption is generally certain in both cases. Thus it is of interest to consider the long-term behaviour of the chain, conditioned on non-absorption, within the setting of imprecise probability. Methods have previously been presented for the case of a constant transition matrix, and submitted for the case of a non-constant transition matrix. In this paper the methods for the two cases are compared.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s025.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Credal Semantics of Bayesian Transformations</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>fabio.cuzzolin@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>credal set</keyword>
        <keyword>bayesian transformations</keyword>
        <keyword>upper and lower simplices</keyword>
        <keyword>focus</keyword>
      </keywords>
      <abstract>In this paper we propose a credal representation of the set of interval probabilities associated with a belief function, and show how it relates to several classical Bayesian transformations of belief functions through the notion of ``focus" of a pair of simplices. Starting from the interpretation of the pignistic function as the center of mass of the credal set of consistent probabilities, we prove that relative belief and plausibility of singletons and intersection probability can be described as foci of different pairs of simplices in the simplex of all probability measures. Such simplices are associated with the lower and upper probability constraints, respectively. This paves the way for the formulation of frameworks similar to the transferable belief model for lower, upper, and interval constraints.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s020.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Consistent Approximations of Belief Functions</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>fabio.cuzzolin@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>consistent belief function</keyword>
        <keyword>simplicial complex</keyword>
        <keyword>approximation</keyword>
        <keyword>lp norms</keyword>
      </keywords>
      <abstract>Consistent belief functions represent collections of coherent or non-contradictory pieces of evidence. As most operators used to update or elicit evidence do not preserve consistency, the use of consistent transformations cs[.] in a reasoning process to guarantee coherence can be desirable. Such transformations are turn linked to the problem of approximating an arbitrary belief function with a consistent one. We study here the consistent approximation problem in the case in which distances are measured using classical Lp norms. We show that, for each choice of the element we want them to focus on, the partial approximations determined by the L1 and L2 norms coincide, and can be interpreted as classical focused consistent transformations. Global L1 and L2 solutions do not in general coincide, however, nor are they associated with the highest plausibility element.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s021.pdf</pdf>
    </paper>
    <paper>
      <id>053</id>
      <title>Epistemic Irrelevance in Credal Networks: the Case of Imprecise Markov Trees</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Filip Hermans</name>
          <email>filip.hermans@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>credal network</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>strong independence</keyword>
        <keyword>imprecise markov tree</keyword>
        <keyword>separation</keyword>
        <keyword>hidden markov chain</keyword>
      </keywords>
      <abstract>We replace strong independence in credal networks with the weaker notion of epistemic irrelevance. Focusing on directed trees, we show how to combine the local credal sets in the networks into an overall joint model, and use this to construct and justify an exact message-passing algorithm that computes updated beliefs for a variable in the network. The algorithm, which is essentially linear in the number of nodes, is formulated entirely in terms of coherent lower previsions. We supply examples of the algorithm's operation, and report an application to on-line character recognition that illustrates the advantages of the model for prediction.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s053.pdf</pdf>
    </paper>
    <paper>
      <id>059</id>
      <title>Exchangeability for Sets of Desirable Gambles</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>erik.quaeghebeur@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>desirability</keyword>
        <keyword>weak desirability</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>coherence</keyword>
        <keyword>exchangeability</keyword>
        <keyword>representation</keyword>
        <keyword>natural extension</keyword>
        <keyword>updating</keyword>
      </keywords>
      <abstract>Sets of desirable gambles constitute a quite general type of uncertainty model with an interesting geometrical interpretation. We study exchangeability assessments for such models, and prove a counterpart of de Finetti's finite representation theorem. We show that this representation theorem has a very nice geometrical interpretation. We also lay bare the relationships between the representations of updated exchangeable models, and discuss conservative inference (natural extension) under exchangeability.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s059.pdf</pdf>
    </paper>
    <paper>
      <id>055</id>
      <title>Representing and Solving Factored Markov Decision Processes with Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Karina Delgado</name>
          <email>kvd@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Leliane Barros</name>
          <email>leliane@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ricardo Shirota</name>
          <email>ricardo.shirota@poli.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise markov decision processes (mdpips)</keyword>
        <keyword>probabilistic planning and ppddl</keyword>
        <keyword>knowledge representation languages</keyword>
        <keyword>multilinear programming</keyword>
      </keywords>
      <abstract>This paper investigates Factored Markov Decision Processes with Imprecise Probabilities; that is, Markov Decision Processes where transition probabilities are imprecisely specified, and where their specification does not deal directly with states, but rather with factored representations of states. We first define a Factored MDPIP, based on a multilinear formulation for MDPIPs; then we propose a novel algorithm for generation of Gamma-maximin policies for Factored MDPIPs. We also developed a representation language for Factored MDPIPs (based on the standard PPDDL language); finally, we describe experiments with a problem of practical significance, the well-known System Administrator Planning problem.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s055.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>The Role of Generalised p-Boxes in Imprecise Probability Models</title>
      <authors>
        <author>
          <name>Sebastien Destercke</name>
          <email>sdestercke@gmail.com</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Montpellier</name>
              <latitude>43.61092</latitude>
              <longitude>3.87723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>generalized p-boxes</keyword>
        <keyword>comonotonic clouds</keyword>
        <keyword>fusion</keyword>
        <keyword>conditioning</keyword>
        <keyword>propagation</keyword>
      </keywords>
      <abstract>Recently, we have introduced an uncertainty representation generalising imprecise cumulative distributions to any (pre-)ordered space as well as possibility distributions: generalised p-boxes. This representation has many attractive features, as it remains quite simple while having an interesting interpretation in terms of lower and upper confidence bounds over nested sets. However, the merits this representation in various uncertainty treatments still has to be evaluated. This is the topic of this paper, in which the handling of information modelled by generalised p-boxes is studied, from the point of view of elicitation, propagation, conditioning and fusion.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s009.pdf</pdf>
    </paper>
    <paper>
      <id>051</id>
      <title>Boundary Linear Utility and Sensitivity of Decisions with Imprecise Utility Trade-off  Parameters</title>
      <authors>
        <author>
          <name>Malcolm Farrow</name>
          <email>malcolm.farrow@newcastle.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Newcastle Upon Tyne</name>
              <latitude>54.97328</latitude>
              <longitude>-1.61396</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michael Goldstein</name>
          <email>michael.goldstein@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>robust decisions</keyword>
        <keyword>imprecise utility</keyword>
        <keyword>utility hierarchies</keyword>
        <keyword>mutual utility independence</keyword>
        <keyword>boundary linear utility</keyword>
        <keyword>sensitivity analysis</keyword>
      </keywords>
      <abstract>In earlier work we have developed methods for analysing decision problems based on multi-attribute utility hierarchies, structured by mutual utility independence, which are not precisely specified due to unwillingness or inability of an individual or group to agree on precise values for the trade-offs between the various attributes. Our analysis is based on whatever limited collection of preferences we may assert between attribute collections. In this paper we show how to assess the robustness of our selected decision using the properties of boundary linear utility.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s051.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Multivariate Models and Confidence Intervals: A Local Random Set Approach</title>
      <authors>
        <author>
          <name>Thomas Fetz</name>
          <email>Thomas.Fetz@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>confidence intervals</keyword>
        <keyword>non-parametric models of uncertainty</keyword>
        <keyword>random set</keyword>
        <keyword>fuzzy set</keyword>
        <keyword>upper probability</keyword>
        <keyword>independence</keyword>
        <keyword>unknown interaction</keyword>
        <keyword>frechet bounds</keyword>
      </keywords>
      <abstract>This article is devoted to the propagation of families of confidence intervals obtained by non-parametric methods through multivariate functions comprising the semantics of confidence limits. At fixed confidence level, local random sets are defined whose aggregation admits the calculation of upper probabilities of events. In the multivariate case, a number of ways of combinations is highlighted to encompass independence and unknown interaction using random set independence and Fr\'echet bounds. For all cases we derive formulas for the corresponding upper probabilities and elaborate how they relate. The methods are exemplified by means of an example from structural mechanics.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s038.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>A Minimum Distance Estimator in an  Imprecise Probability Model - Computational Aspects and Applications</title>
      <authors>
        <author>
          <name>Robert Hable</name>
          <email>Robert.Hable@uni-bayreuth.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bayreuth</name>
              <latitude>49.94806</latitude>
              <longitude>11.57833</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>minimum distance estimator</keyword>
        <keyword>empirical measure</keyword>
        <keyword>r project for statistical computing</keyword>
      </keywords>
      <abstract>The present article considers estimating a parameter $\theta$ in an imprecise probability model $(\overline{P}_{\theta})_{\theta\in\Theta}$ which consists of coherent upper previsions $\overline{P}_{\theta}$. After the definition of a minimum distance estimator in this setup and a summarization of its main properties, the focus lies on applications. It is shown that approximate minimum distances on the discretized sample space can be calculated by linear programming. After a discussion of some computational aspects, the estimator is applied in a simulation study consisting of two different models. Finally, the estimator is applied on a real data set in a linear regression model.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s005.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>How Can We Get New Knowledge?</title>
      <authors>
        <author>
          <name>Frank Hampel</name>
          <email>hampel@stat.math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>background knowledge</keyword>
        <keyword>new information</keyword>
        <keyword>conflict of evidence</keyword>
        <keyword>change of model/paradigm</keyword>
        <keyword>common sense thinking</keyword>
        <keyword>scientific breakthrough</keyword>
        <keyword>philosophical foundations of inductive inference</keyword>
        <keyword>real life examples</keyword>
      </keywords>
      <abstract>The paper discusses the (common, important, and yet neglected) situation of a (strong or full) conflict of evidence in scientific and everyday inference (which may lead to valuable new knowledge and even an unexpected scientific breakthrough). It analyses the structure and role of the background knowledge we are using and may have to change, and the many aspects of new information and its interpretation. A number of real life examples follows, which also bring up some more subtle points of inductive thinking.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s037.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>Dutch Books and Combinatorial Games</title>
      <authors>
        <author>
          <name>Peter Harremoes</name>
          <email>P.Harremoes@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>combinatorial game</keyword>
        <keyword>dutch book theorem</keyword>
        <keyword>exchangable sequences</keyword>
        <keyword>game theory</keyword>
        <keyword>surreal number</keyword>
      </keywords>
      <abstract/>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s049.pdf</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>Characterizing Factuality in Normal Form Sequential Decision Making</title>
      <authors>
        <author>
          <name>Nathan Huntley</name>
          <email>nathan.huntley@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>counterfactual</keyword>
        <keyword>partial ordering</keyword>
        <keyword>optimality</keyword>
        <keyword>decision trees</keyword>
        <keyword>choice function</keyword>
        <keyword>backward induction</keyword>
      </keywords>
      <abstract>We prove necessary and sufficient conditions on choice functions for factuality to hold in normal form sequential decision problems. We find that factuality is sufficient for backward induction to work. However, choice must be induced by a total preorder for factuality to hold. Hence, many of the optimality criteria used in imprecise probability theory (such as interval dominance, maximality, and E-admissibility) are counterfactual under normal form decision making.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s029.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Almost Probabilistic Assignments and Conditional Independence (a contribution to Dempster-Shafer theory of evidence)</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dempster-shafer theory</keyword>
        <keyword>multidimensionality</keyword>
        <keyword>operator of composition</keyword>
        <keyword>conditional independence</keyword>
        <keyword>semigraphoids</keyword>
      </keywords>
      <abstract>In the paper we introduce a family of almost probabilistic basic assignments, which slightly extends probabilistic (by most of other authors called Bayesian) basic assignments. This extension incorporates all the distributions that can be created from low-dimensional probabilistic basic assignments by application of the operator of composition, and simultaneously preserves the property of probabilistic basic assignments concerning the number of focal elements: it does not exceed cardinality of the frame of discernment. The other goal of the paper is to propagate a new way of definition of conditional independence relation in D-S theory. It follows ideas of P.~P.~Shenoy who defined the notion of conditional independence for valuation-based system based on his operation of ``combination''. Here we do the same but using the operator of ``composition''. The notion of independence we get in this way seems to meet better the general requirements on conditional independence relation for basic assignments.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s011.pdf</pdf>
    </paper>
    <paper>
      <id>032</id>
      <title>On the Behavior of the Robust Bayesian Combination Operator and the Significance of Discounting</title>
      <authors>
        <author>
          <name>Alexander Karlsson</name>
          <email>alexander.karlsson@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ronnie Johansson</name>
          <email>ronnie.johansson@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sten Andler</name>
          <email>sten.f.andler@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>robust bayesian combination</keyword>
        <keyword>credal set</keyword>
        <keyword>discounting</keyword>
        <keyword>information fusion</keyword>
      </keywords>
      <abstract>We study the combination problem for credal sets via the robust Bayesian combination operator. We extend Walley's notion of degree of imprecision and introduce a measure for degree of conflict between two credal sets. Several examples are presented in order to explore the behavior of the robust Bayesian combination operator in terms of imprecision and conflict. We further propose a discounting operator that suppresses a source given an interval of reliability weights, and highlight the importance of using such weights whenever additional information about the reliability of a source is available.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s032.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>Affinity and Continuity of Credal Set Operator</title>
      <authors>
        <author>
          <name>Tomas Kroupa</name>
          <email>kroupa@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal set</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>superdifferential</keyword>
        <keyword>hausdorff metric</keyword>
      </keywords>
      <abstract>The credal set operator is studied as a set-valued mapping that assigns the set of dominating probabilities to a coherent lower prevision on some set of gambles. It is shown that this mapping is affine on certain classes of coherent lower previsions, which enables to find a decomposition of credal sets. Continuity of the credal set operator is investigated on finite universes with the aim of approximating credal sets.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s041.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Imprecise Probabilities from Imprecise Descriptions of Real Numbers</title>
      <authors>
        <author>
          <name>Jonathan Lawry</name>
          <email>j.lawry@bristol.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Bristol</name>
              <latitude>51.45523</latitude>
              <longitude>-2.59665</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ines Gonzalez-Rodriguez</name>
          <email>ines.gonzalez@unican.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Santander</name>
              <latitude>43.46472</latitude>
              <longitude>-3.80444</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Yongchuan Tang</name>
          <email>tyongchuan@gmail.com</email>
          <location>
            <country>
              <code>US</code>
              <name>China</name>
            </country>
            <city>
              <name>Hangzhou</name>
              <latitude>30.29365</latitude>
              <longitude>120.16142</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>label semantics</keyword>
        <keyword>prototype theory</keyword>
        <keyword>random set</keyword>
        <keyword>lower and upper distributions</keyword>
        <keyword>second order distributions</keyword>
      </keywords>
      <abstract>A prototype theory interpretation of the label semantics framework is proposed as a possible model of imprecise descriptions of real numbers. It is shown that within this framework conditioning given imprecise descriptions of a real variable naturally results in imprecise probabilities. An inference method is proposed from data in the form of a set of imprecise descriptions, which naturally suggests an algorithm for estimating lower and upper probabilities given imprecise data values.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s016.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Reasoning with Imprecise Probabilistic Knowledge on Enzymes for Rapid Screening of Potential Substrates or Inhibitor Structures</title>
      <authors>
        <author>
          <name>Weiru Liu</name>
          <email>w.liu@qub.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Belfast</name>
              <latitude>54.58333</latitude>
              <longitude>-5.93333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Anbu Yue</name>
          <email>a.yue@qub.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Belfast</name>
              <latitude>54.58333</latitude>
              <longitude>-5.93333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>David Timson</name>
          <email>d.timson@qub.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Belfast</name>
              <latitude>54.58333</latitude>
              <longitude>-5.93333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probabilistic knowledge</keyword>
        <keyword>probabilistic logic program</keyword>
        <keyword>prediction</keyword>
        <keyword>substrate structure</keyword>
        <keyword>enzymes</keyword>
        <keyword>rapid screening</keyword>
      </keywords>
      <abstract>In many real world applications, there is a need to model and reason with imprecise probabilistic knowledge. In this paper, we discuss how to model imprecise probabilistic knowledge obtained from experiments in biological sciences on enzymes for rapid screening of potential substrate or inhibitor structures. Each imprecise probabilistic knowledge base is modelled as a probabilistic logic program (PLP). To predict a meaningful substrate structure, we have developed a framework (and a tool) in which a user (bioscientist) can query against a PLP (or a collection of PLPs), can examine how relevant a PLP is for answering a query, and can select a query result that is more satisfactory. This framework is implemented by integrating an optimizer in MatLab to solve the optimization problems subject to linear constraints. A preliminary version of the tool was demonstrated in the ECAI08 Demo session. Experimental results on evaluating the tool with probabilistic knowledge on enzymes for rapid screening of potential substrates or inhibitor structures demonstrate that this tool has a great potential to be used in many similar areas for the initial screening of compound structures in drug discovery.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s015.pdf</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>Noise Quantization via Possibilistic Filtering</title>
      <authors>
        <author>
          <name>Kevin Loquin</name>
          <email>loquin@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Olivier Strauss</name>
          <email>strauss@lirmm.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Montpellier</name>
              <latitude>43.61092</latitude>
              <longitude>3.87723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>signal processing</keyword>
        <keyword>kernel methods</keyword>
        <keyword>possibility distribution</keyword>
        <keyword>noise quantization</keyword>
      </keywords>
      <abstract>In this paper, we propose a novel approach for quantifying the noise level at each location of a digital signal. This method is based on replacing the conventional kernel-based approach extensively used in signal filtering by an approach involving another kind of kernel: a possibility distribution. Such an approach leads to interval-valued resulting methods instead of punctual ones. We show, on real and synthetic data sets, that the length of the obtained interval and the local noise level are highly correlated. This method is non-parametric and advantages over other methods since no assumption about the nature of the noise has to be hypothesized, except its local ergodicity.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s048.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>Nonparametric Predictive Multiple Comparisons with  Censored Data and Competing Risks</title>
      <authors>
        <author>
          <name>Tahani Maturi</name>
          <email>tahani.maturi@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Pauline Coolen-Schrijner</name>
          <email>NA</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>competing risks</keyword>
        <keyword>early termination of experiment</keyword>
        <keyword>nonparametric predictive inference</keyword>
        <keyword>precedence testing</keyword>
        <keyword>progressive censoring</keyword>
        <keyword>right-censored data</keyword>
      </keywords>
      <abstract>Statistical inference for lifetime data often involves right-censoring, with a variety of possible causes. This paper provides an overview of nonparametric predictive inference for comparison of multiple groups of data including right-censored observations. Different right-censoring schemes discussed are early termination of a lifetime experiment, progressive censoring and competing risks. Theoretical results are briefly stated, detailed justifications are presented elsewhere. The methods are illustrated and discussed via examples with data from the literature.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s017.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>Object Association in the TBM Framework, Application to Vehicle Driving Aid</title>
      <authors>
        <author>
          <name>David Mercier</name>
          <email>david.mercier@univ-artois.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Bethune</name>
              <latitude>50.53333</latitude>
              <longitude>2.63333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Eric Lefevre</name>
          <email>eric.lefevre@univ-artois.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Bethune</name>
              <latitude>50.53333</latitude>
              <longitude>2.63333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Daniel Jolly</name>
          <email>daniel.jolly@univ-artois.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Bethune</name>
              <latitude>50.53333</latitude>
              <longitude>2.63333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>obstacle tracking</keyword>
        <keyword>association step</keyword>
        <keyword>belief function</keyword>
        <keyword>transferable belief model</keyword>
      </keywords>
      <abstract>The problem tackled in this paper deals with obstacle tracking in the context of vehicle driving aid, especially the association step, which consists in associating perceived objects with known objects detected at a previous time. A contribution in the modeling of this association problem in the belief function framework is introduced. By interpreting belief functions as weighted opinions according to the Transferable Belief Model semantics, pieces of information regarding the association of known objects and perceived objects can be expressed in a common global space of association to be combined by the conjunctive rule of combination, and a decision making process using the pignistic transformation can be made. This approach is validated on real data.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s010.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Natural Extension as a Limit of Regular Extensions</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower prevision</keyword>
        <keyword>weak and strong coherence</keyword>
        <keyword>natural extension</keyword>
        <keyword>regular extension</keyword>
        <keyword>desirable gambles</keyword>
      </keywords>
      <abstract>This paper is devoted to the extension of conditional assessments that satisfy some consistency criteria, such as weak or strong coherence, to further domains. In particular, we characterise the natural extension of a number of conditional lower previsions on finite spaces, by showing that it can be calculated as the limit of a sequence of conditional lower previsions defined by regular extension. Our results are valid for conditional lower previsions with non-linear domains, and allow us to give an equivalent formulation of the notion of coherence in terms of credal sets.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s012.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Duality Between Maximization of Expected Utility and Minimization of Relative Entropy When Probabilities are Imprecise</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Victor Richmond Jose</name>
          <email>vrj@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Robert Winkler</name>
          <email>rwinkler@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision theory</keyword>
        <keyword>decision analysis</keyword>
        <keyword>relative entropy</keyword>
        <keyword>utility theory</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>portfolio optimization</keyword>
      </keywords>
      <abstract>In this paper we model the problem faced by a risk-averse decision maker with a precise subjective probability distribution who bets against a risk-neutral opponent or invests in a financial market where the beliefs of the opponent or the representative agent in the market are described by a convex set of imprecise probabilities. The problem of finding the portfolio of bets or investments that maximizes the decision maker's expected utility is shown to be the dual of the problem of finding the distribution within the set that minimizes a measure of divergence, i.e., relative entropy, with respect to the decision maker's distribution. In particular, when the decision maker's utility function is drawn from the commonly used exponential/logarithmic/power family, the solutions of two generic utility maximization problems are shown to correspond exactly to the minimization of divergences drawn from two commonly-used parametric families that both generalize the Kullback-Leibler divergence. We also introduce a new parameterization of the exponential/logarithmic/power utility functions that allows the power parameter to vary continuously over all real numbers and which is a natural and convenient parameterization for modeling utility gains relative to a non-zero status quo wealth position</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s003.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>The Pari-Mutuel Model</title>
      <authors>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolo.vicig@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>pari-mutuel model</keyword>
        <keyword>risk measures</keyword>
        <keyword>natural extension</keyword>
        <keyword>dilation</keyword>
        <keyword>2-monotonicity</keyword>
      </keywords>
      <abstract>We explore generalizations of the pari-mutuel model (PMM), a formalization of an intuitive way of assessing an upper probability from a precise one. We discuss a naive extension of the PMM considered in insurance and generalize the natural extension of the PMM introduced by P. Walley and other related formulae. The results are subsequently given a risk measurement interpretation: in particular it is shown that a known risk measure, Tail Value at Risk (TVaR), is derived from the PMM, and a coherent risk measure more general than TVaR from its imprecise version. We analyze further the conditions for coherence of a related risk measure, Conditional Tail Expectation. Explicit formulae for conditioning the PMM and conditions for dilation or imprecision increase are also supplied and discussed.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s028.pdf</pdf>
    </paper>
    <paper>
      <id>044</id>
      <title>Interpretation and Computation of Alpha-Junctions for Combining Belief Functions</title>
      <authors>
        <author>
          <name>Frederic Pichon</name>
          <email>frederic.pichon@thalesgroup.com</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Palaiseau</name>
              <latitude>48.71667</latitude>
              <longitude>2.25000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thierry Denoeux</name>
          <email>tdenoeux@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>transferable belief model</keyword>
        <keyword>dempster-shafer theory</keyword>
        <keyword>belief function</keyword>
        <keyword>information fusion</keyword>
        <keyword>uncertain reasoning</keyword>
      </keywords>
      <abstract>The alpha-junctions are the associative, commutative and linear operators for belief functions with a neutral element. This family of rules includes as particular cases the unnormalized Dempster's rule and the disjunctive rule. Until now, the alpha-junctions suffered from two main limitations. First, they did not have an interpretation in the general case. Second, it was difficult to compute a combination by an alpha-junction. In this paper, an interpretation for these rules is proposed. It is shown that the alpha-junctions correspond to a particular form of knowledge about the truthfulness of the sources providing the belief functions to be combined. Simple means to compute a combination by an alpha-junction are also laid bare. These means are based on generalizations of mechanisms that exist to compute the combination by the unnormalized Dempster's rule.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s044.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>On Solutions of Stochastic Differential Equations with Parameters Modelled by Random Sets</title>
      <authors>
        <author>
          <name>Bernhard Schmelzer</name>
          <email>bernhard.schmelzer@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>stochastic differential equation</keyword>
        <keyword>random set</keyword>
        <keyword>set-valued stochastic process</keyword>
        <keyword>first entrance time</keyword>
      </keywords>
      <abstract>We consider ordinary stochastic differential equations whose coefficients depend on parameters. Conditions are given under which modelling the parameter uncertainty by compact-valued random sets leads to set-valued stochastic processes. Finally, we define analogues of first entrance times for set-valued processes.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s023.pdf</pdf>
    </paper>
    <paper>
      <id>039</id>
      <title>Coefficients of Ergodicity for Imprecise Markov Chains</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Robert Hable</name>
          <email>Robert.Hable@uni-bayreuth.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bayreuth</name>
              <latitude>49.94806</latitude>
              <longitude>11.57833</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>markov chain</keyword>
        <keyword>imprecise markov chain</keyword>
        <keyword>coefficient of ergodicity</keyword>
        <keyword>lower expectation</keyword>
        <keyword>upper expectation</keyword>
      </keywords>
      <abstract>Coefficients of ergodicity are an important tool in measuring convergence of Markov chains. We explore possibilities to generalise the concept to imprecise Markov chains. We find that this can be done in at least two different ways, which both have interesting implications in the study of convergence of imprecise Markov chains. Thus we extend the existing definition of the uniform coefficient of ergodicity and define a new so-called weak coefficient of ergodicity. The definition is based on the endowment of a structure of a metric space to the class of imprecise probabilities. We show that this is possible to do in some different ways, which turn out to coincide.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s039.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>Buying and Selling Prices under Risk, Ambiguity and Conflict</title>
      <authors>
        <author>
          <name>Michael Smithson</name>
          <email>Michael.Smithson@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paul Campbell</name>
          <email>paul.campbell@abs.gov.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ambiguity</keyword>
        <keyword>conflict</keyword>
        <keyword>prices</keyword>
        <keyword>risk aversion</keyword>
        <keyword>buying</keyword>
        <keyword>selling</keyword>
      </keywords>
      <abstract>This paper reports an empirical study of buying and selling prices for three kinds of gambles: Risky (with known probabilities), ambiguous (with lower and upper probabilities), and conflictive (with disagreeing probability assessments). The latter two types of gambles were constructed so that the variances in their probabilities were approximately equal, thereby ensuring that uncertainty type was not confounded with variance. Participants devaluated both ambiguous and conflictive gambles relative to risky gambles with equivalent expected utilities, but the ambiguous and conflictive valuation means did not significantly differ. Moreover, the endowment effect (the gap between buying and selling prices) was exaggerated for these two kinds of gambles in comparison with risky gambles. Conflictive gambles also were found to be devalued less than ambiguous gambles, relative to their risky counterparts. Several self-report measures of attitudes towards uncertainty and risk were included as potential predictors of pricing. The most effective predictors were a measure of instrumental risk orientation and a functional impulsivity scale. Instrumental risk positively predicted valuation of ambiguous and conflictive gambles but not of risky gambles. Functional impulsivity positively predicted valuation of risky gambles but neither of the other two kinds. No individual differences measures predicted relative devaluation.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s006.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Statistical Inference for Interval Identified Parameters</title>
      <authors>
        <author>
          <name>Joerg Stoye</name>
          <email>j.stoye@nyu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial identification</keyword>
        <keyword>bounds</keyword>
        <keyword>confidence regions</keyword>
        <keyword>hypothesis testing</keyword>
        <keyword>uniform inference</keyword>
        <keyword>moment inequalities</keyword>
        <keyword>subjective expectations</keyword>
      </keywords>
      <abstract>This papers analyzes the construction of confidence intervals for a parameter that is "interval identified," that is, the sampling process only reveals upper and lower bounds on it even in the limit. Analysis of inference for such parameters requires one to reconsider some fundamental issues. To begin, it is not clear which object -- the parameter or the set of parameter values characterized by the bounds -- should be asymptotically covered by a confidence region. Next, some straightforwardly constructed confidence intervals encounter severe problems because sampling distributions of relevant quantities can change discontinuously as parameter values change, leading to problems that are familiar from the pre-testing and model selection literatures. I carry out the relevant analyses for the simple model under consideration, but also emphasize the generality of problems encountered and connect developments to general themes in the larger and rapidly developing literature on inference under partial identification. Results are illustrated with an application to the Survey of Economic Expectations.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s050.pdf</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>Shifted Dirichlet Distributions as Second-Order Probability Distributions that Factors into Marginals</title>
      <authors>
        <author>
          <name>David Sundgren</name>
          <email>dsn@hig.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Gaevle</name>
              <latitude>60.67452</latitude>
              <longitude>17.14174</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Love Ekenberg</name>
          <email>lovek@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mats Danielson</name>
          <email>mad@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>second-order probability distribution</keyword>
        <keyword>dirichlet distribution</keyword>
        <keyword>beta distribution</keyword>
        <keyword>kullback-leibler divergence</keyword>
        <keyword>relative entropy</keyword>
        <keyword>product of marginal distributions</keyword>
      </keywords>
      <abstract>In classic decision theory it is assumed that a decision-maker can assign precise numerical values corresponding to the true value of each consequence, as well as precise numerical probabilities for their occurrences. In attempting to address real-life problems, where uncertainty in the input data prevails, some kind of representation of imprecise information is important. Second-order distributions, probability distributions over probabilities, is one way to achieve such a representation. However, it is hard to intuitively understand statements in a multi-dimensional space and user statements must be provided more locally. But the information-theoretic interplay between joint and marginal distributions may give rise to unwanted effects on the global level. We consider this problem in a setting of second-order probability distributions and find a family of distributions that normalised over the probability simplex equals its own product of marginals. For such distributions, there is no flow of information between the joint distributions and the marginal distributions other than that trivial fact that the variables belong to the probability simplex.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s040.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Multi-Criteria Decision Making with a Special Type of Information About Importance of Groups of Criteria</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multi-criteria decision making</keyword>
        <keyword>desirable gambles</keyword>
        <keyword>dempster-shafer theory</keyword>
        <keyword>judgment</keyword>
        <keyword>preference</keyword>
        <keyword>pareto set</keyword>
      </keywords>
      <abstract>An axiomatic approach for solving a multi-criteria decision making problem is studied in the paper, which generally allows reducing a set of Pareto optimal solutions. The information about criteria in the problem is represented as the decision maker judgments of a special type. The judgments have a clear behavior interpretation and can be used in various decision problems. It is shown in the paper how to combine the judgments and to use them for reducing the Pareto set when they are provided by several decision makers. Two global criteria of decision making are introduced for comparing of decision alternatives. The first criterion based on the lower expectation, the second one is based on determining the belief and plausibility functions in the framework of Dempster-Shafer theory and uses the "threshold" probability for the final decision making. The numerical examples illustrate the proposed approach.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s027.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>Combining Imprecise Bayesian and Maximum Likelihood Estimation for Reliability Growth Models</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Svetlana Zatenko</name>
          <email>s_lana2004@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian inference</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>linear regression</keyword>
        <keyword>lower and upper probability distributions</keyword>
        <keyword>maximum likelihood estimation</keyword>
        <keyword>reliability growth models</keyword>
      </keywords>
      <abstract>A new framework is explored for combining imprecise Bayesian methods with likelihood inference, and it is presented in the context of reliability growth models. The main idea of the framework is to divide a set of the model parameters of interest into two subsets related to fundamentally different aspects of the overall model, and to combine Walley's idea of imprecise Bayesian models related to one of the subsets of the model parameters with maximum likelihood estimation for the other subset. In accordance with the first subset and statistical data, the imprecise Bayesian model is constructed, which provides lower and upper predictive probability distributions depending on the second subset of parameters. These further parameters are then estimated by a maximum likelihood method, based on a novel proposition for maximum likelihood estimation over sets of distributions following from imprecise Bayesian models for the other subset of parameters. Use of this hybrid method is illustrated for reliability growth models and regression models, and some essential topics that need to be addressed in order to fully justify and further develop this framework are discussed.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s026.pdf</pdf>
    </paper>
    <paper>
      <id>052</id>
      <title>On Conditional Independence in Evidence Theory</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@vse.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>evidence theory</keyword>
        <keyword>random set independence</keyword>
        <keyword>conditional independence</keyword>
        <keyword>conditional noninteractivity</keyword>
        <keyword>markov properties</keyword>
      </keywords>
      <abstract>The goal of this paper is to introduce a new concept of conditional independence in evidence theory, to prove its formal properties, and to show in what sense it is superior to the concept introduced previously by other authors.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s052.pdf</pdf>
    </paper>
    <paper>
      <id>061</id>
      <title>Bayes Linear Analysis of Imprecision in Computer Models, with Application to Understanding Galaxy Formation</title>
      <authors>
        <author>
          <name>Ian Vernon</name>
          <email>i.r.vernon@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michael Goldstein</name>
          <email>michael.goldstein@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian inference</keyword>
        <keyword>computer models</keyword>
        <keyword>calibration</keyword>
        <keyword>imprecise model discrepancy</keyword>
        <keyword>implausibility</keyword>
        <keyword>galaxy formation</keyword>
        <keyword>graphical representation of model imprecision</keyword>
      </keywords>
      <abstract>Imprecision arises naturally in the context of computer models and their relation to reality. An imprecise treatment of general computer models is presented, illustrated with an analysis of a complex galaxy formation simulation known as Galform. The analysis involves several different types of uncertainty, one of which (the Model Discrepancy) comes directly from expert elicitation regarding the deficiencies of the model. The Model Discrepancy is therefore treated within an Imprecise framework to reflect more accurately the beliefs of the expert concerning the discrepancy between the model and reality. Due to the conceptual complexity and computationally intensive nature of such a Bayesian imprecise uncertainty analysis, Bayes Linear Methodology is employed which requires consideration of only expectations and variances of all uncertain quantities. Therefore incorporating an Imprecise treatment within a Bayes Linear analysis is shown to be relatively straightforward. The impact of an imprecise assessment on the input space of the model is determined through the use of an Implausibility measure.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s061.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Threat and Control in Military Decision Making</title>
      <authors>
        <author>
          <name>Christofer Waldenstrom</name>
          <email>christofer.waldenstrom@fhs.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Love Ekenberg</name>
          <email>lovek@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mats Danielson</name>
          <email>mad@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>military decision making</keyword>
        <keyword>threat</keyword>
        <keyword>worst case</keyword>
        <keyword>expected value</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>This paper presents a model of how military commanders estimate the threat posed by the enemy in a tactical situation, and how they employ own forces to control that threat. The model is based on interviews with nine commanders from the Swedish navy and the purpose is to find automatic and adequate methods for reasoning about strategic issues based on the long-time experience of highly qualified military officers. The results show that the number of enemy units, the types of enemy units, the behavior of the enemy units, and the uncertainties regarding the number, types, and behavior determines the threat in a tactical situation. The own course of action works as a threat altering function to control that threat. When the commander should decide on a course of action, we suggest that it should be selected so it minimizes the expected threat.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta09/proceedings/papers/s030.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2011</year>
    <conference>
      <date>
        <start>2011-07-25</start>
        <end>2011-07-28</end>
      </date>
      <location>
        <country>
          <code>AT</code>
          <name>Austria</name>
        </country>
        <city>
          <name>Innsbruck</name>
          <latitude>47.26266</latitude>
          <longitude>11.39454</longitude>
        </city>
        <university>
          <name>University of Innsbruck</name>
          <department>Unit for Engineering Mathematics</department>
        </university>
      </location>
    </conference>
    <paper>
      <id>032</id>
      <title>Likelihood-Based Naive Credal Classifier</title>
      <authors>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giorgio Corani</name>
          <email>giorgio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>classification</keyword>
        <keyword>naive credal classifier</keyword>
        <keyword>naive bayesian classifier</keyword>
        <keyword>likelihood-based learning</keyword>
      </keywords>
      <abstract>The naive credal classifier extends the classical naive Bayes classifier to imprecise probabilities, substituting the imprecise Dirichlet model for the uniform prior. As an alternative to the naive credal classifier, we present a likelihood-based approach, which extends in a novel way the naive Bayes towards imprecise probabilities, by considering any possible quantification (each one defining a naive Bayes classifier) apart from those assigning to the available data a probability below a given threshold level. Besides the available supervised data, in the likelihood evaluation we also consider the instance to be classified, for which the value of the class variable is assumed missing-at-random. We obtain a closed formula to compute the dominance according to the maximality criterion for any threshold level. As there are currently no well-established metrics for comparing credal classifiers which have considerably different determinacy, we compare the two classifiers when they have comparable determinacy, finding that in those cases they generate almost equivalent classifications.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s032.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>The Description/Experience Gap in the Case of Uncertainty</title>
      <authors>
        <author>
          <name>Horacio Arlo-Costa</name>
          <email>hcosta@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Varun Dutt</name>
          <email>vdutt@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cleotilde Gonzalez</name>
          <email>coty@cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jeffrey Helzner</name>
          <email>jh2239@columbia.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertainty</keyword>
        <keyword>descriptive</keyword>
        <keyword>normative</keyword>
        <keyword>decisions from experience/description</keyword>
      </keywords>
      <abstract>We present empirical evidence indicating the existence of a description/experience gap for decisions under uncertainty. The nature of the gap is different than the one arising in the case of risk but both phenomena depend essentially on the use of limited sampling in experience. While subjects are ambiguity averse in description they are robustly ambiguity seeking in experience. A probabilistic explanation of this effect is provided as well as conjectures about the possibility of studying the effect with descriptive theories like Cumulative Prospect Theory.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s020.pdf</pdf>
    </paper>
    <paper>
      <id>047</id>
      <title>Nonparametric predictive inference for subcategory data</title>
      <authors>
        <author>
          <name>Rebecca Baker</name>
          <email>r.m.baker@dunelm.org.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Pauline Coolen-Schrijner</name>
          <email/>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>classification</keyword>
        <keyword>multinomial data</keyword>
        <keyword>nonparametric predictive inference</keyword>
        <keyword>subcategories</keyword>
      </keywords>
      <abstract>Nonparametric predictive inference (NPI) is a framework for statistical inference in the absence of prior knowledge. We present NPI for multinomial data with subcategories, motivated by the hierarchical structure of many multinomial data sets. We consider situations with known and with unknown numbers of subcategories, and present lower and upper probabilities for general events involving one future observation. We present properties of the model and an algorithm to derive an approximation to the maximum entropy distribution.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s047.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Structural Reliability Assessment with Fuzzy Probabilities</title>
      <authors>
        <author>
          <name>Michael Beer</name>
          <email>cvebm@nus.edu.sg</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Liverpool</name>
              <latitude>53.40547</latitude>
              <longitude>-2.98054</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mingqiang Zhang</name>
          <email>mingqiang@nus.edu.sg</email>
          <location>
            <country>
              <code>SG</code>
              <name>Singapore</name>
            </country>
            <city>
              <name>Singapore</name>
              <latitude>1.28967</latitude>
              <longitude>103.85007</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ser Tong Quek</name>
          <email>cveqst@nus.edu.sg</email>
          <location>
            <country>
              <code>SG</code>
              <name>Singapore</name>
            </country>
            <city>
              <name>Singapore</name>
              <latitude>1.28967</latitude>
              <longitude>103.85007</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Scott Ferson</name>
          <email>scott@ramas.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Setauket</name>
              <latitude>40.93510</latitude>
              <longitude>-73.11844</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>fuzzy probabilities</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>failure probability</keyword>
        <keyword>reliability analysis</keyword>
      </keywords>
      <abstract>The prediction of the behavior and reliability of engineering structures and systems is often plagued by uncertainty and imprecision caused by sparse data, poor measurements and linguistic information. Accounting for such limitations complicates the mathematical modeling required to obtain realistic results in engineering analyses. The framework of imprecise probabilities provides a mathematical basis to deal with these problems which involve both probabilistic and non-probabilistic sources of uncertainty. A common feature of the various concepts of imprecise probabilities is the consideration of an entire set of probabilistic models in one analysis. But there are differences between the concepts in the mathe-matical description of this set and in the theoretical connection to the probabilistic models involved. This study is focused on fuzzy probabilities, which combine a probabilistic characterization of variability with a fuzzy characterization of imprecision. We discuss how fuzzy modeling can allow a more nuanced approach than interval-based concepts. The application in engineering is demonstrated by means of two examples.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s003.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>A discussion on learning and prior ignorance for sets of priors in the one-parameter exponential family</title>
      <authors>
        <author>
          <name>Alessio Benavoli</name>
          <email>alessio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>prior near-ignorance</keyword>
        <keyword>set of distributions</keyword>
        <keyword>exponential family of distributions</keyword>
        <keyword>imperfect observations</keyword>
      </keywords>
      <abstract>For a conjugate likelihood-priors model in the one-parameter exponential family of distributions, we show that, by letting the parameters of the conjugate exponential prior vary in suitable sets, it is possible to define a set of conjugate priors M which guarantees prior near-ignorance without producing vacuous inferences. This result is obtained following both a behavioural and a sensitivity analysis interpretation of prior near-ignorance. We also discuss the problem of the incompatibility of learning and prior near-ignorance for sets of priors in the one-parameter exponential family of distributions in the case of imperfect observations. In particular, we prove that learning and prior near-ignorance are compatible under an imperfect observation mechanism if and only if the support of the priors in M is the whole real axis.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s027.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>Two for the Price of One: Info-Gap Robustness of the 1-Test Algorithm</title>
      <authors>
        <author>
          <name>Yakov Ben-Haim</name>
          <email>yakov@technion.ac.il</email>
          <location>
            <country>
              <code>IL</code>
              <name>Israel</name>
            </country>
            <city>
              <name>Haifa</name>
              <latitude>32.81841</latitude>
              <longitude>34.98850</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>testing</keyword>
        <keyword>design</keyword>
        <keyword>info-gap</keyword>
      </keywords>
      <abstract>Analysts in many domains must choose a design, a strategy, or an intervention without being able to test all relevant alternatives. We consider a situation in which one of two alternatives must be chosen, while only one alternative can be tested prior to decision. A well known probabilistic algorithm assures probability greater than 1/2 of choosing the better system based on a single test, even in the absence of prior knowledge of the probability distribution of the systems' attributes. If this distribution is known then the algorithm can be tuned to achieve probability of success substantially exceeding 1/2. If the distribution is poorly known, then info-gap theory can robustify the algorithm. Using the info-gap robustness function we show that robust-satisficing algorithms may differ from the nominally optimal algorithm when the attribute distribution is uncertain.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s002.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Dirichlet Model Versus Expert Knowledge</title>
      <authors>
        <author>
          <name>Diogo Bezerra</name>
          <email>dicbezerra@hotmail.com</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Caruaru</name>
              <latitude>-8.28333</latitude>
              <longitude>-35.97611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fernando Campello de Souza</name>
          <email>fmcs@hotlink.com.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Recife</name>
              <latitude>-8.05389</latitude>
              <longitude>-34.88111</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>linear programming</keyword>
        <keyword>elicitation</keyword>
        <keyword>portfolio  selection</keyword>
        <keyword>financial</keyword>
      </keywords>
      <abstract>Decision theory is used to choose a portfolio. Elicitation methods was used based on the utility function and from expert opinion thus, enabling the creation of a utility function for the investor and another for the a priori distribution on economic indicators. The model chosen for an investment portfolio was formulated based on decision theory, incorporating aspects of systematic and unsystematic risk. The model was developed so as to structure an effcient way to understand the application of decision theory in the financial market as well as the application of the Imprecise Dirichlet Model-IDM. The IDM allows the use of imprecise probability. Finally, the IDM was compared to the Markowitz method and also, to the decision model, using only expert opinion, considering an allocation over time to verify which of the three models was the best one. The final conclusion is that expert opinion should not be neglected in her compiling a portfolio.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s018.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>The Description of Least Favorable Pairs in Huber-Strassen Theory, Finite Case</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Taganrog</name>
              <latitude>47.23617</latitude>
              <longitude>38.89688</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>2-monotone capacities</keyword>
        <keyword>least favorable pairs</keyword>
        <keyword>huber-strassen theory</keyword>
        <keyword>kullback-leibler divergence</keyword>
      </keywords>
      <abstract>In this paper we provide the algebraic description of the minmax problem solutions, which are considered in Huber-Strassen theory providing effective algorithms of searching least favorable pairs. This investigation gives also new insights to understanding well-known algorithms for maximizing Shannon entropy and other functionals.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s006.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Comparing Binary and Standard Probability Trees in Credal Networks Inference</title>
      <authors>
        <author>
          <name>Andres Cano</name>
          <email>acu@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Manuel Gomez-Olmedo</name>
          <email>mgomez@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Andres Masegosa</name>
          <email>andrew@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>inference algorithms</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>bayesian networks</keyword>
        <keyword>variable elimination</keyword>
        <keyword>probability trees</keyword>
      </keywords>
      <abstract>This paper proposes the use of Binary Probability Trees in the propagation of credal networks. Standard and binary probability trees are suitable data structures for representing potentials because they allow to control the accuracy of inference algorithms by means of a threshold parameter. The choice of this threshold is a trade-off between accuracy and computing time. Binary trees enable the representation of finer-grained independences than probability trees. This leads to more efficient algorithms for credal networks with variables with more than two states. The paper shows experiments comparing binary and standard probability trees in order to demonstrate their performance.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s024.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Incoherence correction strategies in statistical matching</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>capot@dipmat.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>statistical matching</keyword>
        <keyword>incoherence</keyword>
        <keyword>inference</keyword>
        <keyword>specialized discrepancy measure</keyword>
      </keywords>
      <abstract>We deal with the statistical matching problem and in particular we study the problem related to the managing of inconsistencies. In fact, when logical relations among the variables are present incoherence can arise in the probability evaluations. The aim of this paper is to remove such incoherences by using different methods. Specific precise distances minimization or least committal imprecise probability extensions are adopted. We compare these methods using an exemplifying practical example that brings to light the peculiarities of the statistical matching problem.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s014.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Regression with Imprecise Data: A Robust Approach</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Andrea Wiencierz</name>
          <email>Andrea.Wiencierz@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>robust regression</keyword>
        <keyword>imprecise data</keyword>
        <keyword>nonparametric statistics</keyword>
        <keyword>likelihood inference</keyword>
        <keyword>imprecise probability distributions</keyword>
        <keyword>survey data</keyword>
        <keyword>informative coarsening</keyword>
        <keyword>complex uncertainty</keyword>
        <keyword>interval dominance</keyword>
        <keyword>identification region</keyword>
      </keywords>
      <abstract>We introduce a robust regression method for imprecise data, and apply it to social survey data. Our method combines nonparametric likelihood inference with imprecise probability, so that only very weak assumptions are needed and different kinds of uncertainty can be taken into account. The proposed regression method is based on interval dominance: interval estimates of quantiles of the error distribution are used to identify plausible descriptions of the relationship of interest. In the application to social survey data, the resulting set of plausible descriptions is relatively large, reflecting the amount of uncertainty inherent in the analyzed data set.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s015.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>Building Imprecise Classification Trees With Entropy Ranges</title>
      <authors>
        <author>
          <name>Richard Crossman</name>
          <email>R.J.Crossman@warwick.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Warwick</name>
              <latitude>52.33333</latitude>
              <longitude>-1.58333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joaquin Abellan</name>
          <email>jabellan@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>classification trees</keyword>
        <keyword>nonparametric predictive inference</keyword>
      </keywords>
      <abstract>One method for building classification trees is to choose split variables by maximising expected entropy. This can be extended through the application of imprecise probability by replacing instances of expected entropy with the maximum possible expected entropy over credal sets of probability distributions. Such methods may not take full advantage of the opportunities offered by imprecise probability theory. In this paper, we change focus from maximum possible expected entropy to the full range of expected entropy. We then choose one or more potential split variables using an interval comparison method. This method is presented with specific reference to the case of ordinal data, and we present algorithms that maximise and minimise entropy within the credal sets of probability distributions which are generated by the NPI method for ordinal data.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s028.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Lp consonant approximation of belief functions in the mass space</title>
      <authors>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>Fabio.Cuzzolin@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>consonant belief functions</keyword>
        <keyword>(outer) consonant approximation</keyword>
        <keyword>geometric approach</keyword>
        <keyword>mass space</keyword>
        <keyword>lp norms</keyword>
      </keywords>
      <abstract>In this paper we pose the problem of approximating an arbitrary belief function (b.f.) with a consonant one, in a geometric framework in which belief functions are represented by the vectors of their basic probabilities, or "mass space". Given such a vector mb, the consonant b.f. which minimizes an appropriate distance function from mb can be sought. We consider here the classical L1, L2 and Lp norms. As consonant belief functions live in a collection of simplices in the mass space, partial approximations on each individual simplex have to be computed in order to find the overall approximation. Interpretations of the obtained approximations in terms of basic probabilities are proposed, and the results compared with those of previous approaches, in particular outer consonant approximation.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s025.pdf</pdf>
    </paper>
    <paper>
      <id>041</id>
      <title>Non-conflicting and Conflicting Parts of Belief Functions</title>
      <authors>
        <author>
          <name>Milan Daniel</name>
          <email>milan.daniel@cs.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>dempster-shafer theory</keyword>
        <keyword>dempster's semigroup</keyword>
        <keyword>conflict between belief functions</keyword>
        <keyword>uncertainty</keyword>
        <keyword>non-conflicting part of belief function</keyword>
        <keyword>conflicting part of belief function</keyword>
      </keywords>
      <abstract>Non-conflicting and conflicting parts of belief functions are introduced in this study. The unique decomposition of a belief function defined on a two-element frame of discernment to non-conflicting and indecisive conflicting belief function is presented. Several basic statements about algebra of belief functions on a general finite frame of discernment are introduced and unique non-conflicting part of a BF on an $n$-element frame of discernment is presented here.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s041.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>State sequence prediction in imprecise hidden Markov models</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise hidden markov model</keyword>
        <keyword>optimal state sequence</keyword>
        <keyword>maximality</keyword>
        <keyword>coherent lower prevision</keyword>
        <keyword>credal network</keyword>
        <keyword>epistemic irrelevance</keyword>
      </keywords>
      <abstract>We present an efficient exact algorithm for estimating state sequences from outputs (or observations) in imprecise hidden Markov models (iHMM), where both the uncertainty linking one state to the next, and that linking a state to its output, are represented using coherent lower previsions. The notion of independence we associate with the credal network representing the iHMM is that of epistemic irrelevance. We consider as best estimates for state sequences the (Walley--Sen) maximal sequences for the posterior joint state model (conditioned on the observed output sequence), associated with a gain function that is the indicator of the state sequence. This corresponds to (and generalises) finding the state sequence with the highest posterior probability in HMMs with precise transition and output probabilities (pHMMs). We argue that the computational complexity is at worst quadratic in the length of the Markov chain, cubic in the number of states, and essentially linear in the number of maximal state sequences. For binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s021.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Independent natural extension for sets of desirable gambles</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>epistemic irrelevance</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>independent natural extension</keyword>
        <keyword>strong product</keyword>
        <keyword>coherent set of desirable gambles</keyword>
      </keywords>
      <abstract>We investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence. We provide formulas for the smallest such joint, called their independent natural extension, and study its main properties. The independent natural extension of maximal sets of gambles allows us to define the strong product of sets of desirable gambles. Finally, we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s011.pdf</pdf>
    </paper>
    <paper>
      <id>049</id>
      <title>Modelling uncertainties in limit state functions</title>
      <authors>
        <author>
          <name>Thomas Fetz</name>
          <email>Thomas.Fetz@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probability of failure</keyword>
        <keyword>limit state functions</keyword>
        <keyword>parameterized probability measures</keyword>
        <keyword>random set</keyword>
        <keyword>random set independence</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>strong independence</keyword>
      </keywords>
      <abstract>In this paper uncertainties in limit state functions $g$ as arising in engineering problems are modelled by adding additional parameters and by introducing parameterized probability density functions which describe the uncertainties of these new additional parameters and of the basic variables of $g$. This will lead to a function $p_f(a,b)$ for the probability of failure depending on parameters $a$ and $b$ corresponding to the two parameterized density functions. Further the parameters $a$ and $b$ are assumed to be uncertain. Using intervals, sets or random sets to model their uncertainty results in upper probabilities $\overline{p}_f$ of failure. In this context we also discuss different notions of independence such as strong independence, epistemic irrelevance and random set independence and present a simple engineering example.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s049.pdf</pdf>
    </paper>
    <paper>
      <id>043</id>
      <title>Coherent conditional probabilities and proper scoring rules</title>
      <authors>
        <author>
          <name>Angelo Gilio</name>
          <email>gilio@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giuseppe Sanfilippo</name>
          <email>sanfilippo@unipa.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Palermo</name>
              <latitude>38.11582</latitude>
              <longitude>13.35976</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional probability assessments</keyword>
        <keyword>coherence</keyword>
        <keyword>penalty criterion</keyword>
        <keyword>proper scoring rules</keyword>
        <keyword>conditional scoring rules</keyword>
        <keyword>weak dominance</keyword>
        <keyword>strong dominance</keyword>
        <keyword>admissibility</keyword>
        <keyword>bregman divergence</keyword>
        <keyword>g-coherence</keyword>
        <keyword>total coherence</keyword>
        <keyword>imprecise probability assessments</keyword>
      </keywords>
      <abstract>In this paper we study the relationship between the notion of coherence for conditional probability assessments on a family of conditional events and the notion of admissibility with respect to scoring rules. By extending a recent result given in literature for unconditional events, we prove, for any given strictly proper scoring rule s, the equivalence between the coherence of a conditional probability assessment and its admissibility with respect to s. In this paper we focus our analysis on the case of continuous bounded scoring rules. In this context a key role is also played by Bregman divergence and by a related theoretical aspect. Finally, we briefly illustrate a possible way of defining (generalized) coherence of interval-valued probability assessments by exploiting the notion of admissibility given for precise probability assessments.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s043.pdf</pdf>
    </paper>
    <paper>
      <id>050</id>
      <title>Potential Surprises</title>
      <authors>
        <author>
          <name>Frank Hampel</name>
          <email>hampel@stat.math.ethz.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Zurich</name>
              <latitude>47.36667</latitude>
              <longitude>8.55000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>foundations of statistics</keyword>
        <keyword>historical concepts</keyword>
        <keyword>(potential) surprises</keyword>
        <keyword>background knowledge or believe</keyword>
        <keyword>combining of backgrounds</keyword>
        <keyword>updating of backgrounds</keyword>
        <keyword>merging or contrasting of backgrounds</keyword>
        <keyword>practical application of mathematical models</keyword>
        <keyword>real life examples</keyword>
      </keywords>
      <abstract>After a brief historical overview over various approaches to the foundations of statistics, the very general, simple and basic concept of (potential) surprises is introduced, which may be subjective or objective and goes beyond previous approaches by I.J. Good and by the author. The surprises are conditional on the background knowledge or belief of the person experiencing it; the updating of the so-called background, and the merging or, if not possible, the contrasting of different backgrounds by two or more persons (otherwise they talk past each other) are very important operations in practice. A number of examples from real life, in complement to two previous, more qualitative papers, are given.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s050.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>Dynamic Programming and Subtree Perfectness for Deterministic Discrete-Time Systems with Uncertain Rewards</title>
      <authors>
        <author>
          <name>Nathan Huntley</name>
          <email>nathan.huntley@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>optimal control</keyword>
        <keyword>dynamic programming</keyword>
        <keyword>deterministic discrete-time systems</keyword>
        <keyword>backward induction</keyword>
        <keyword>subtree perfectness</keyword>
        <keyword>choice function</keyword>
      </keywords>
      <abstract>We generalise de Cooman and Troffaes's sufficient condition for dynamic programming to work for deterministic discrete-time systems. To do so, we use the general framework developed by Huntley and Troffaes, for decision trees with arbitrary rewards and arbitrary choice functions. Whence, we allow deterministic discrete-time systems with arbitrary rewards and an arbitrary composition operator on rewards. We show that the principle of optimality reduces to two much simpler conditions on the choice function. We establish necessary and sufficient conditions on choice functions for deterministic discrete-time systems to be solvable by backward induction, that is, for dynamic programming to work. Finally, we also discuss subtree perfectness---which is a stronger form of dynamic consistency---for these systems, and show that, in general, decision criteria from imprecise probability theory violate it, even though dynamic programming may work.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s026.pdf</pdf>
    </paper>
    <paper>
      <id>004</id>
      <title>A  Note on Local Computations in Dempster-Shafer Theory of Evidence</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multidimensional models</keyword>
        <keyword>graphical model</keyword>
        <keyword>conditional independence</keyword>
        <keyword>factorization</keyword>
        <keyword>computations</keyword>
      </keywords>
      <abstract>When applying any technique of multidimensional models to problems of practice one has always to cope with two problems: it is necessary to have a possibility to represent the models with a `reasonable' number of parameters and to have a sufficiently efficient computational procedures at one's disposal. When considering graphical Markov models in probability theory, both these conditions are fulfilled; various computational procedures for decomposable models are based on the ideas of local computations, whose theoretical foundations were laid by Lauritzen and Spiegelhalter. The presented contribution studies a possibility of transferring these ideas from probability theory into Dempster-Shafer theory of evidence. The paper recalls decomposable models, discusses connection of the model structure with the corresponding system of conditional independence relations, and shows that under special additional conditions one can locally compute specific basic assignments which can be considered to be conditional.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s004.pdf</pdf>
    </paper>
    <paper>
      <id>045</id>
      <title>Overcoming some limitations of imprecise reliability models</title>
      <authors>
        <author>
          <name>Igor Kozine</name>
          <email>igko@man.dtu.dk</email>
          <location>
            <country>
              <code>DK</code>
              <name>Denmark</name>
            </country>
            <city>
              <name>Kongens Lyngby</name>
              <latitude>55.77044</latitude>
              <longitude>12.50378</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Victor Krymsky</name>
          <email>vikrymsky@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Ufa</name>
              <latitude>54.78517</latitude>
              <longitude>56.04562</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise reliability</keyword>
        <keyword>variational calculus</keyword>
        <keyword>bounded failure rate</keyword>
      </keywords>
      <abstract>The application of imprecise reliability models is often hindered by the rapid growth in imprecision that occurs when many components constitute a system and by the fact that time to failure is bounded from above. The latter results in the necessity to explicitly introduce an upper bound on time to failure which is in reality a rather arbitrary value. The practical meaning of the models of this kind is brought to question. We suggest an approach that overcomes the issue of having to impose an upper bound on time to failure and makes the calculated lower and upper reliability measures more precise. The main assumption consists in that failure rate is bounded. Langrage method is used to solve the non-linear program. Finally, an example is provided.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s045.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>Partially identified prevalence estimation under misclassification using the Kappa coefficient</title>
      <authors>
        <author>
          <name>Helmut Kuechenhoff</name>
          <email>kuechenhoff@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Anne Kunz</name>
          <email>anne.kunz@ibe.med.uni-muenchen</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial identification</keyword>
        <keyword>sensitivity analysis</keyword>
        <keyword>prevalence estimation</keyword>
        <keyword>kappa coefficient</keyword>
        <keyword>misclassification</keyword>
        <keyword>identification region</keyword>
        <keyword>ignorance region</keyword>
      </keywords>
      <abstract>We discuss prevalence estimation under misclassication. That is we are concerned with the estimation of a proportion of units having a certain property (being diseased, showing deviant behavior, etc.) from a random sample when the true variable of interest cannot be observed, but a related proxy variable (e.g. the outcome of a diagnostic test) is available. If the misclassification probabilities were known then unbiased prevalence estimation would be possible. We focus on the frequent case where the misclassification probabilities are unknown but two independent replicate measurements have been taken. While in the traditional precise probabilistic framework a correction from this information is not possible due to non-identifiability, the imprecise probability methodology of partial identification and systematic sensitivity analysis allows to obtain valuable insights into possible bias due to misclassification. We derive tight identification intervals and corresponding confidence regions for the true prevalence, based on the often reported kappa coeficient, which condenses the information of the replicates by measuring agreement between the two measurements. Our method is illustrated in several theoretical scenarios and in an example from oral health on prevalence of caries in children.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s031.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>A study on updating belief functions for parameter uncertainty representation in Nuclear Probabilistic Risk Assessment</title>
      <authors>
        <author>
          <name>Tu Duong Le Duy</name>
          <email>tu_duong.le_duy@utt.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Clamart</name>
              <latitude>48.80299</latitude>
              <longitude>2.26692</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dominique Vasseur</name>
          <email>dominique.vasseur@edf.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Clamart</name>
              <latitude>48.80299</latitude>
              <longitude>2.26692</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mathieu Couplet</name>
          <email>mathieu.couplet@edf.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Clamart</name>
              <latitude>48.80299</latitude>
              <longitude>2.26692</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Laurence Dieulle</name>
          <email>laurence.dieulle@utt.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Troyes</name>
              <latitude>48.30000</latitude>
              <longitude>4.08333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Christophe Berenguer</name>
          <email>christophe.berenguer@utt.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Troyes</name>
              <latitude>48.30000</latitude>
              <longitude>4.08333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>parameter uncertainty</keyword>
        <keyword>belief function</keyword>
        <keyword>generalized bayesian theorem</keyword>
        <keyword>nuclear risk assessment</keyword>
      </keywords>
      <abstract>Probabilistic Risk Assessments (PRA) are used to achieve a safe design and operation of Nuclear Power Plants. The impact of uncertainties which may affect PRA results must thus be taken into account in the decision making process. These uncertainties due to the lack of data have been recently seen as mainly epistemic ones and it has been recommended to characterize them by the belief functions of Dempster-Shafer Theory rather than a presumed single probability distribution. The current construction of these functions is based on the data provided by PRA data handbooks using traditional statistical tools like Maximum Likelihood Estimation (MLE). However, this approach is only appropriate when data coming from the operating feedback observations are sufficiently large as required in the MLE approach. Furthermore, when wishing to incorporate other sources of information, such as expert's opinions, the pooling data of MLE has limits to account for these kinds of information. Therefore, in order to overcome this problem, two alternative perspectives based on the Dempster's rule of combination and the Generalized Bayesian Theorem for constructing and updating the belief functions in a more effective way will be presented in this paper. These two approaches will be studied for the use in the context of PRA. The comparison of these two approaches with the current method is carried out through a practical example. Some conclusions about the application of these approaches will be drawn.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s037.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Robust Equilibria under Linear Tracing Procedure</title>
      <authors>
        <author>
          <name>Hailin Liu</name>
          <email>hailinl@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>equilibrium refinement</keyword>
        <keyword>linear tracing procedure</keyword>
        <keyword>stability</keyword>
        <keyword>robustness</keyword>
        <keyword>sets of probabilities</keyword>
      </keywords>
      <abstract>In Harsanyi and Selten's equilibrium selection theory, the linear tracing procedure has been used to model the hypothetical reasoning process of expectation formation. This paper reconsiders the linear tracing procedure from the perspective of the relationship between priors and Nash equilibria. A prior belongs to the source set of a Nash equilibrium if the linear tracing procedure based on this prior leads to that equilibrium. We show that for any Nash equilibrium, its source set is always nonempty and closed, but not generally convex. This paper also constructs an approach of iterative application of the linear tracing procedure to the auxiliary games that are used to model the hypothetical reasoning under the procedure. We present a notion of robustness of Nash equilibria based on this idea, by replacing uncertainty modelled by a single probability measure with uncertainty modelled by sets of probability measures. This approach attempts to capture the fact that players may not be sufficiently confident in the available information in order to single out one probability distribution that represents their initial beliefs about the other players' possible strategy choices.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s012.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>Bounds for Self-consistent CDF Estimators for Univariate and Multivariate Censored Data</title>
      <authors>
        <author>
          <name>Xuecheng Liu</name>
          <email>xuecheng.liu@umontreal.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Montreal</name>
              <latitude>45.50884</latitude>
              <longitude>-73.58781</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alain Vandal</name>
          <email>alain.vandal@aut.ac.nz</email>
          <location>
            <country>
              <code>NZ</code>
              <name>New Zealand</name>
            </country>
            <city>
              <name>Auckland</name>
              <latitude>-36.86667</latitude>
              <longitude>174.76667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval censoring</keyword>
        <keyword>maximal clique</keyword>
        <keyword>clique matrix</keyword>
        <keyword>self-consistent estimator</keyword>
        <keyword>bounds</keyword>
        <keyword>npmle</keyword>
        <keyword>mixture nonuniqueness</keyword>
      </keywords>
      <abstract>In this paper, lower bounds and upper bounds are given for the mass assigned to a set of maximal cliques in self-consistent estimates of CDF NPMLEs for multivariate (including univariate) interval censored data under the assumption that the censoring mechanism is ignorable for the purpose of likelihood inference. The bounds are applied to give upper bounds of the diameter and size of the polytope of CDF NPMLEs for multivariate censored data.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s010.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>A Fully Polynomial Time Approximation Scheme for Updating Credal Networks of Bounded Treewidth and Number of Variable States</title>
      <authors>
        <author>
          <name>Denis Maua</name>
          <email>denis@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cassio Campos</name>
          <email>cassio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probabilistic graphical models</keyword>
        <keyword>credal network</keyword>
        <keyword>approximation scheme</keyword>
        <keyword>valuation algebra</keyword>
      </keywords>
      <abstract>Credal networks lift the precise probability assumption of Bayesian networks, enabling a richer representation of uncertainty in the form of closed convex sets of probability measures. The increase in expressiveness comes at the expense of higher computational costs. In this paper we present a new algorithm which is an extension of the well-known variable elimination algorithm for computing posterior inferences in extensively specified credal networks. The algorithm efficiency is empirically shown to outperform a state-of-the-art algorithm. We then provide the first fully polynomial time approximation scheme for inference in credal networks with bounded treewidth and number of states per variable.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s035.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Conglomerable Natural Extension</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conglomerability</keyword>
        <keyword>natural extension</keyword>
        <keyword>desirable gambles</keyword>
        <keyword>coherent lower prevision</keyword>
      </keywords>
      <abstract>We study the weakest conglomerable model that is implied by desirability or probability assessments: the \emph{conglomerable natural extension}. We show that taking the natural extension of the assessments while imposing conglomerability -the procedure adopted in Walley's theory- does not yield, in general, the conglomerable natural extension (but it does so in the case of the marginal extension). Iterating this process produces a sequence of models that approach the conglomerable natural extension, although it is not known, at this point, whether it is attained in the limit. We give sufficient conditions for this to happen in some special cases, and study the differences between working with coherent sets of desirable gambles and coherent lower previsions. Our results indicate that it might be necessary to re-think the foundations of Walley's theory of coherent conditional lower previsions for infinite partitions of conditioning events.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s030.pdf</pdf>
    </paper>
    <paper>
      <id>048</id>
      <title>Imprecise Probabilities in Non-cooperative Games</title>
      <authors>
        <author>
          <name>Robert Nau</name>
          <email>robert.nau@duke.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Durham (US)</name>
              <latitude>35.93698</latitude>
              <longitude>-78.90477</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>previsions</keyword>
        <keyword>lower and upper probabilities</keyword>
        <keyword>correlated equilibrium</keyword>
        <keyword>risk neutral probabilities</keyword>
        <keyword>risk neutral equilibrium</keyword>
      </keywords>
      <abstract>Game-theoretic solution concepts such as Nash equilibrium are commonly used to model strategic behavior in terms of precise probability distributions over outcomes. However, there are many potential sources of imprecision in beliefs about the outcome of a game: incomplete knowledge of payoff functions, non-uniqueness of equilibria, heterogeneity of prior probabilities, unobservable background risk, and distortions of revealed beliefs due to risk aversion, among others. This paper presents a unified approach for dealing with these issues, in which the typical solution of a game is a convex set of probability distributions that, unlike Nash equilibria, may be correlated between players. In the most general case, where players are risk averse, the probabilities do not represent beliefs alone. Rather they must be interpreted as products of subjective probabilities and relative marginal utilities for money.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s048.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Characterizing joint distributions of random sets with an application to set-valued stochastic processes</title>
      <authors>
        <author>
          <name>Bernhard Schmelzer</name>
          <email>bernhard.schmelzer@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>random set</keyword>
        <keyword>choquet's theorem</keyword>
        <keyword>capacity functional</keyword>
        <keyword>joint distribution</keyword>
        <keyword>daniell-kolmogorov theorem</keyword>
      </keywords>
      <abstract>By the Choquet theorem, distributions of random closed sets can be characterized by a certain class of set functions called capacity functionals. In this paper a generalization to the multivariate case is presented, that is, it is proved that the joint distribution of finitely many random sets can be characterized by a set function fulfilling certain properties. Furthermore, we use this result to formulate an existence theorem for set-valued stochastic processes.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s013.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>Forecasting with Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mark Schervish</name>
          <email>mark@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>brier score</keyword>
        <keyword>coherence</keyword>
        <keyword>dominance</keyword>
        <keyword>e-admissibility</keyword>
        <keyword>gamma-maximin</keyword>
        <keyword>proper scoring rules</keyword>
      </keywords>
      <abstract>We review de Finetti's two coherence criteria for determinate probabilities: coherence1 defined in terms of previsions for a set of random variables that are undominated by the status quo -- previsions immune to a sure-loss -- and coherence2 defined in terms of forecasts for events undominated in Brier score by a rival forecast. We propose a criterion of IP-coherence2 based on a generalization of Brier score for IP-forecasts that uses 1-sided, lower and upper, probability forecasts. However, whereas Brier score is a strictly proper scoring rule for eliciting determinate probabilities, we show that there is no real-valued strictly proper IP-score. Nonetheless, with respect to either of two decision rules -- Gamma-Maximin or (Levi's) E-admissibility + Gamma-Maximin -- we give a lexicographic strictly proper IP-scoring rule that is based on Brier score.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s009.pdf</pdf>
    </paper>
    <paper>
      <id>001</id>
      <title>Never Say 'Not:' Impact of Negative Wording in Probability Phrases on Imprecise Probability Judgments</title>
      <authors>
        <author>
          <name>Michael Smithson</name>
          <email>Michael.Smithson@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>David Budescu</name>
          <email>budescu@fordham.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Stephen Broomell</name>
          <email>broomell@gmail.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>University Park</name>
              <latitude>40.79611</latitude>
              <longitude>-77.86278</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Han-Hui Por</name>
          <email>hanhui.p@gmail.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>New York</name>
              <latitude>40.71427</latitude>
              <longitude>-74.00597</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>subjective probability</keyword>
        <keyword>probability expression</keyword>
        <keyword>elicitation</keyword>
        <keyword>conjugate</keyword>
        <keyword>risk communication</keyword>
        <keyword>climate change</keyword>
      </keywords>
      <abstract>A reanalysis of Budescu et al.'s (2009) data on numerical interpretations of the Intergovernmental Panel on Climate Change (IPCC 2007) fourth report's verbal probability expressions (PE's) revealed that negative wording has deleterious effects on lay judgements. Budescu et al. asked participants to interpret PE's in IPCC report sentences, by asking them to provide lower, ``best'' and upper estimates of the probabilities that they thought the authors intended. There were four experimental conditions, determining whether participants were given any numerical guidelines for translating the PE's into numbers. The first analysis presented here focuses on six sentences in Budescu et al. that used the PE ``very likely'' or ``very unlikely''. A mixed beta regression (Verkuilen \&amp; Smithson, in press) modelling the three numerical estimates simultaneously revealed a less regressive mean and less dispersion for positive than for negative wording in all three estimates. Negative wording therefore resulted in more regressive estimates and less consensus regardless of experimental condition. The second analysis focuses on two statements that were positive-negative duals. Appropriate pairs of responses were assessed for conjugacy and additivity. A mixed beta regression model of these three variables revealed that the $\underline P (A)$ and $\overline P (A^c)$ pairs adhered most closely to conjugacy. Also, the greatest dispersion occurred for $\underline P (A) + \overline P (A^c)$, followed by $P(A) + P (A^c)$. These results were driven by the dispersion in the estimates for the negatively-worded statement. This paper also describes the effects of the experimental conditions on conjugacy and dispersion.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s001.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Discrete Second-order Probability Distributions that Factor into Marginals</title>
      <authors>
        <author>
          <name>David Sundgren</name>
          <email>dsn@hig.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Gaevle</name>
              <latitude>60.67452</latitude>
              <longitude>17.14174</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>discrete probability</keyword>
        <keyword>second-order probability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>multivariate polya distribution</keyword>
        <keyword>conjugate prior</keyword>
        <keyword>compound hypergeometric likelihood</keyword>
      </keywords>
      <abstract>In realistic decision problems there is more often than not uncertainty in the background information. As for representation of uncertain or imprecise probability values, second-order probability, i.e. probability distributions over probabilities, offers an option. With a subjective view of probability second-order probability would seem to be impractical since it is hard for a person to construct a second-order distributions that reflects his or her beliefs. From the perspective of probability as relative frequency the task of constructing or updating a second-order probability distribution from data is somewhat easier. Here a very simple model for updating lower bounds of probabilities is employed. But the difficulties in choosing second-order distributions may be further alleviated if structural properties are considered. Either some of the probability values are dependent in some way, e.g. that they are known to be almost equal, or they are not dependent in any other way than what follows from that the values sum to one. In this work we present the unique family of discrete second-order probability distributions that correspond to the case where dependence is limited. These distributions are shown to have the property that the joint distributions are equal to normalised products of marginal distributions. The distribution family introduced here is a generalisation of a special case of the multivariate Polya distribution and is shown to be conjugate prior to a compound hypergeometric distribution.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s038.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>Probability boxes on totally preordered spaces for multivariate modelling</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>sdestercke@gmail.com</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Montpellier</name>
              <latitude>43.61092</latitude>
              <longitude>3.87723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>p-box</keyword>
        <keyword>natural extension</keyword>
        <keyword>multivariate</keyword>
        <keyword>elicitation</keyword>
        <keyword>independence</keyword>
        <keyword>frechet</keyword>
        <keyword>lower prevision</keyword>
      </keywords>
      <abstract>Probability boxes (pairs of cumulative distribution functions) are among the most popular models used in imprecise probability theory. In this paper, we provide new efficient tools to construct multivariate p-boxes and develop algorithms to draw inferences from them. For this purpose, we formalise and extend the theory of p-boxes using lower previsions. We allow p-boxes to be defined on arbitrary totally preordered spaces, hence thereby also admitting multivariate p-boxes. We discuss the construction of multivariate p-boxes under various independence assumptions. An example demonstrates the practical feasibility of our results.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s034.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>Robust detection of exotic infectious diseases in animal herds: A comparative study of two decision methodologies under severe uncertainty</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>John Paul Gosling</name>
          <email>johnpaul.gosling@fera.gsi.gov.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Sand Hutton</name>
              <latitude>54.01776</latitude>
              <longitude>-0.93832</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>exotic disease</keyword>
        <keyword>lower prevision</keyword>
        <keyword>info-gap</keyword>
        <keyword>maximality</keyword>
        <keyword>minimax</keyword>
        <keyword>robustness</keyword>
        <keyword>inspection</keyword>
        <keyword>protocol</keyword>
      </keywords>
      <abstract>When animals are transported and pass through customs, some of them may have dangerous infectious diseases. Typically, due to the cost of testing, not all animals are tested: a reasonable selection must be made. How to test effectively, yet avoid cataclysmic events? First, we extend a model proposed in the literature for the detection of invasive species to suit our purpose. Secondly, we explore and compare two decision methodologies on the problem at hand, namely, info-gap theory and imprecise probability theory, both of which are designed to handle severe uncertainty. We show that, under rather general conditions, every info-gap solution is maximal with respect to a suitably chosen imprecise probability model, and that therefore, perhaps surprisingly, the set of maximal options can be inferred at least partly---and sometimes entirely---from an info-gap analysis.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s033.pdf</pdf>
    </paper>
    <paper>
      <id>044</id>
      <title>Robustness of Natural Extension</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Robert Hable</name>
          <email>Robert.Hable@uni-bayreuth.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Bayreuth</name>
              <latitude>49.94806</latitude>
              <longitude>11.57833</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower prevision</keyword>
        <keyword>linear programming</keyword>
        <keyword>robustness</keyword>
        <keyword>regularity</keyword>
        <keyword>natural extension</keyword>
        <keyword>sensitivity</keyword>
        <keyword>perturbation</keyword>
      </keywords>
      <abstract>How sensitive is the natural extension of an upper prevision against small perturbations in the assessments? We revise some basic results from the theory of systems of linear inequalities and equalities, and linear programming, and apply them to the theory of upper previsions. We find that stability is most easily characterized through a regularity condition on the constraints of the primal problem. We then study stability, and the existence of stable representations, in detail. We find necessary and sufficient conditions for the usual representations of natural extension to be stable, and necessary and sufficient conditions for natural extension to have a stable representation at all. We show that, by arbitrary small perturbation, we can force stability of the usual representations.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s044.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Interval-valued regression and classification models in the framework of machine learning</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief function</keyword>
        <keyword>classification</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval-valued observations</keyword>
        <keyword>machine learning</keyword>
        <keyword>p-box</keyword>
        <keyword>regression</keyword>
        <keyword>risk functional</keyword>
        <keyword>support vector machine</keyword>
      </keywords>
      <abstract>This paper presents a new approach for constructing regression and classification models for interval-valued data. The risk functional is considered under a set of probability distributions, resulting from the application of a chosen inferential method to the data, such that the bounding distributions of the set depend on the regression and classification parameter. Two extreme (`pessimistic' and `optimistic') strategies of decision making are presented. The method is appicable with a wide variety of inferential methods and risk functionals, in addition to the general theory the specific optimisation problems for several scenarios are formulated and discussed. In particular, the extension of the support vector machine method for the case of interval-valued data is presented.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s008.pdf</pdf>
    </paper>
    <paper>
      <id>040</id>
      <title>Conditioning, conditional independence and irrelevance in evidence theory</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>evidence theory</keyword>
        <keyword>multidimensional models</keyword>
        <keyword>conditioning rule</keyword>
        <keyword>conditional independence</keyword>
        <keyword>conditional irrelevance</keyword>
      </keywords>
      <abstract>The goal of the paper is to reveal the relationships between recently introduced concept of conditional independence in evidence theory and those (dependent on the choice of conditioning rule) of conditional irrelevance.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s040.pdf</pdf>
    </paper>
    <paper>
      <id>046</id>
      <title>On Prior-Data Conflict in Predictive Bernoulli Inferences</title>
      <authors>
        <author>
          <name>Gero Walter</name>
          <email>gero.walter@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Frank Coolen</name>
          <email>Frank.Coolen@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian inference</keyword>
        <keyword>generalized iluck-models</keyword>
        <keyword>imprecise beta-binomial model</keyword>
        <keyword>imprecise weighting</keyword>
        <keyword>predictive inference</keyword>
        <keyword>prior-data conflict</keyword>
      </keywords>
      <abstract>By its capability to deal with the multidimensional nature of uncertainty, imprecise probability provides a powerful methodology to sensibly handle prior-data conflict in Bayesian inference. When there is strong conflict between sample observations and prior knowledge the resulting posterior model naturally should be much more imprecise than in the situation of mutual agreement or compatibility. Focusing presentation on the prototypical example of Bernoulli trials, we discuss the ability of different approaches to deal with prior-data conflict. We study a generalized Bayesian setting, including Walley's Imprecise Beta-Binomial model and his extension to handle prior data conflict (called pdc-IBBM here). We investigate alternative shapes of prior parameter sets, chosen in a way that shows improved behaviour in the case of prior-data conflict and their influence on the posterior predictive distribution. Thereafter we present a new approach, consisting of an imprecise weighting of two originally separate inferences, one of which is based on an informative imprecise prior whereas the other one is based on an uninformative imprecise prior. This approach deals with prior-data conflict in a fascinating way.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s046.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Utility-Based Accuracy Measures to Empirically Evaluate Credal Classifiers</title>
      <authors>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giorgio Corani</name>
          <email>giorgio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Denis Maua</name>
          <email>denis@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal classification</keyword>
        <keyword>indeterminacy</keyword>
        <keyword>empirical evaluations</keyword>
        <keyword>discounted accuracy</keyword>
        <keyword>utility</keyword>
        <keyword>risk aversion</keyword>
      </keywords>
      <abstract>Predictions made by imprecise-probability models are often indeterminate (that is, set-valued). Measuring the quality of an indeterminate prediction by a single number is important to fairly compare different models, but a principled approach to this problem is currently missing. In this paper we derive a measure to evaluate the predictions of credal classifiers from a set of assumptions. The measure turns out to be made of an objective component, and another that is related to the decision-maker's degree of risk-aversion. We discuss when the measure can be rendered independent of such a degree, and provide insights as to how the comparison of classifiers based on the new measure changes with the number of predictions to be made. Finally, we empirically study the behavior of the proposed measure.</abstract>
      <pdf>http://leo.ugr.es/sipta/isipta11/proceedings/papers/s016.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2013</year>
    <conference>
      <date>
        <start>2013-07-02</start>
        <end>2013-07-05</end>
      </date>
      <location>
        <country>
          <code>FR</code>
          <name>France</name>
        </country>
        <city>
          <name>Compiegne</name>
          <latitude>49.41794</latitude>
          <longitude>2.82606</longitude>
        </city>
        <university>
          <name>University of Technology Compiegne</name>
          <department>Heuristic and Diagnostic Methods for Complex Systems</department>
        </university>
      </location>
    </conference>
    <paper>
      <id>001</id>
      <title>Inclusion/exclusion principle for belief functions</title>
      <authors>
        <author>
          <name>Felipe Aguirre</name>
          <email>felipe.aguirre@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Christelle Jacob</name>
          <email>jacob@isae.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>sebastien.destercke@utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>didier.dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mohamed Sallak</name>
          <email>mohamed.sallak@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>reliability</keyword>
        <keyword>evidence theory</keyword>
        <keyword>inclusion/exclusion principle</keyword>
        <keyword>boolean formulas</keyword>
      </keywords>
      <abstract>The inclusion-exclusion principle is a well-known property of set cardinality and probability measures, that is instrumental to solve some problems such as the evaluation of systems reliability or of uncertainty over Boolean formulas. However, when using sets and probabilities conjointly, this principle no longer holds in general. It is therefore useful to know in which cases it is still valid. This paper investigates this question when uncertainty is modelled by belief functions. After exhibiting necessary and sufficient conditions for the principle to hold, we illustrate its use on some applications, i.e. reliability analysis and uncertainty over Boolean formulas.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s001.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>Classification of Temporal Data by Imprecise Dynamic Models</title>
      <authors>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Rocco De Rosa</name>
          <email>roccomilano@tiscali.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Milan</name>
              <latitude>45.46427</latitude>
              <longitude>9.18951</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessandro Giusti</name>
          <email>alessandrog@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cuzzolin</name>
          <email>fabio.cuzzolin@brookes.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Oxford</name>
              <latitude>51.75222</latitude>
              <longitude>-1.25596</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>imprecise hidden markov model</keyword>
        <keyword>credal classification</keyword>
      </keywords>
      <abstract>A new procedure to classify temporal data with im- precise hidden Markov models is proposed. A differ- ent model is learned from each sequence by coupling the imprecise Dirichlet model with the EM algorithm. As a descriptor of the model associated to a sequence, we consider the expected value of the manifest vari- able in the limit of stationarity of the Markov chain. For imprecise models, only the bounds of this descrip- tor can be evaluated. In practice, the sequence, which can be regarded as a trajectory in the features space, is summarized by this method as a hyperbox in the same space. These static but interval-valued data are classified by a credal extension of the k-NN algorithm. Experiments on human action recognition data show that the method achieves the required robustness and outperforms other imprecise methods.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s002.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>The description of extreme 2-monotone measures</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Moscow</name>
              <latitude>55.75222</latitude>
              <longitude>37.61556</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Igor Rozenberg</name>
          <email>I.Rozenberg@gismps.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Moscow</name>
              <latitude>55.75222</latitude>
              <longitude>37.61556</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>2-monotone measures</keyword>
        <keyword>extreme point</keyword>
        <keyword>additivity on lattices</keyword>
        <keyword>filters</keyword>
        <keyword>partially ordered sets</keyword>
        <keyword>multilinear extension</keyword>
      </keywords>
      <abstract>The paper is devoted to the description of extreme points in the set of 2-monotone measures. We describe them using lattices on which an extreme 2-monotone measure is additive. We also propose the way of generation extreme monotone measures based on the aggregation of extreme measures with the help of multilinear extension. We describe also the class of extreme 2-monotone measures that are additive on the filter on which a 2-monotone measure has a positive values.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s003.pdf</pdf>
    </paper>
    <paper>
      <id>004</id>
      <title>On the Robustness of Imprecise Probability Method</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>cattaneo@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>robustness</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>bayesian analysis</keyword>
        <keyword>credibility</keyword>
        <keyword>decision making</keyword>
        <keyword>indecision</keyword>
        <keyword>sensitivity analysis</keyword>
        <keyword>imprecise dirichlet model</keyword>
      </keywords>
      <abstract>Imprecise probability methods are often claimed to be robust, or more robust than conventional methods. In particular, the higher robustness of the resulting methods seems to be the principal argument supporting the imprecise probability approach to statistics over the Bayesian one. The goal of the present paper is to investigate the robustness of imprecise probability methods, and in particular to clarify the terminology used to describe this fundamental issue of the imprecise probability approach.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s004.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>An approach to uncertainty in probabilistic assignments with application to vibro-acoustic problems</title>
      <authors>
        <author>
          <name>Alice Cicirello</name>
          <email>ac685@cam.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Cambridge</name>
              <latitude>52.20331</latitude>
              <longitude>0.12486</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Robin Langley</name>
          <email>rsl21@cam.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Cambridge</name>
              <latitude>52.20331</latitude>
              <longitude>0.12486</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertainties in probabilistic assignments</keyword>
        <keyword>hybrid fe/sea method</keyword>
        <keyword>reliability analysis</keyword>
        <keyword>maximum entropy distribution</keyword>
        <keyword>vibro-acoustic analysis</keyword>
      </keywords>
      <abstract>In this paper a novel imprecise probability description is applied to vibro-acoustic problems in engineering. Frequently little data is available concerning the variability of the key input parameters required for a predictive analysis. This has led to widespread use of several uncertainty descriptions. The hybrid Finite Element/Statistical Energy Analysis (FE/SEA) approach to the analysis of vibro-acoustic systems is based on subdividing a system into: (i) SEA components which incorporate a non-parametric model of uncertainty and (ii) FE components with parametric uncertainty. This approach, combined with the Laplace asymptotic method, allows the evaluation of the failure probability. A novel strategy for establishing bounds on the failure probability when an imprecise probability model (based on expressing the probability density function of a random variable in the form of a maximum entropy distribution with bounded parameters) is employed is presented. The approach is illustrated by application to a built-up plate system.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s005.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>Bayesian-like inference, complete disintegrability and complete conglomerability in coherent conditional possibility theory</title>
      <authors>
        <author>
          <name>Giulianella Coletti</name>
          <email>coletti@dmi.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Davide Petturiti</name>
          <email>davide.petturiti@dmi.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>complete disintegrability</keyword>
        <keyword>complete conglomerability</keyword>
        <keyword>coherence</keyword>
        <keyword>t-conditional possibility</keyword>
        <keyword>possibilistic likelihood function</keyword>
        <keyword>finite maxitivity</keyword>
      </keywords>
      <abstract>In this paper we consider Bayesian-like inference processes involving coherent $T$-conditional possibilities assessed on infinite sets of conditional events. For this, a characterization of coherent assessments of possibilistic prior and likelihood is carried on. Since we are working in a finitely maxitive setting, the notions of complete disintegrability and of complete conglomerability are also studied and their relevance in the infinite version of the possibilistic Bayes formula is highlighted.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s006.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Conditional not-additive measures and fuzzy sets</title>
      <authors>
        <author>
          <name>Giulianella Coletti</name>
          <email>coletti@dmi.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>vantaggi@dmmm.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>natural extension</keyword>
        <keyword>conditional plausibilities</keyword>
        <keyword>t-conditional possibility</keyword>
        <keyword>generalized bayesian inference</keyword>
        <keyword>fuzzy set</keyword>
      </keywords>
      <abstract>Consistency of partial assessments with different frameworks (probability, possibility, plausibility) is studied. We are interested in inferential processes like the Bayesian one, with particular attention when a part of the information is expressed in natural language and can be modeled by a possibilistic or a plausibilistic likelihood.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s007.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Is the mode a lower prevision?</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Luciano Sanchez</name>
          <email>luciano@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>expectation</keyword>
        <keyword>median</keyword>
        <keyword>mode</keyword>
        <keyword>lower prevision</keyword>
        <keyword>desirability</keyword>
        <keyword>preference</keyword>
      </keywords>
      <abstract>We introduce the notion of mode-desirability of a gamble, that generalizes the idea of non-negativeness of the mode of a random variable. The lower and upper previsions induced by this new definition coincide with the minimum and maximum values of the set of modes of a gamble, when the credal set is a singleton, but they only bound them in the general case. The reason why the minimum and the maximum of the set of modes can not be written, in general, by means of a pair of lower and upper previsions is discussed.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s008.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>Independence for Sets of Full Conditional Measures, Sets of Lexicographic Probabilities, and Sets of Desirable Gambles</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>sets of probability measures</keyword>
        <keyword>full conditional measures</keyword>
        <keyword>independence concepts</keyword>
        <keyword>graphoids</keyword>
        <keyword>lexicographic probability</keyword>
        <keyword>set of desirable gambles</keyword>
      </keywords>
      <abstract>In this paper we examine concepts of independence for sets of full conditional measures; that is, for measures where conditional probability is the primitive concept, and where conditioning can be considered on events of probability zero. We also discuss the related issue of independence for lexicographic measures and for sets of desirable gambles.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s009.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>Credal networks under epistemic irrelevance using sets of desirable gambles</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>graphoid properties</keyword>
        <keyword>irrelevant natural extension</keyword>
        <keyword>lower prevision</keyword>
        <keyword>coherence</keyword>
      </keywords>
      <abstract>We present a new approach to credal networks, which are graphical models that generalise Bayesian nets to deal with imprecise probabilities. Instead of applying the commonly used notion of strong independence, we replace it by the weaker notion of epistemic irrelevance. We show how assessments of epistemic irrelevance allow us to construct a global model out of given local uncertainty models, leading to an intuitive expression for the so-called irrelevant natural extension of a network. In contrast with Cozman (2000), who introduced this notion in terms of credal sets, our main results are presented using the language of sets of desirable gambles. This has allowed us to derive a number of useful properties of the irrelevant natural extension. It has powerful marginalisation properties and satisfies all graphoid properties but symmetry, both in their direct and reverse forms.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s010.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Allowing for probability zero in credal networks under epistemic irrelevance</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>lower prevision</keyword>
        <keyword>credal set</keyword>
        <keyword>coherence</keyword>
        <keyword>irrelevant natural extension</keyword>
        <keyword>independent natural extension</keyword>
      </keywords>
      <abstract>We generalise Cozman's concept of a credal network under epistemic irrelevance (2000, Section 8.3) to allow for lower (and upper) probabilities to be zero. We provide alternative representations for the resulting joint model, including a description by means of linear constraints. We apply our method to a simple case: the independent natural extension of two binary variables. This allows us to, for the first time, find analytical expressions for the extreme points of this special type of independent product.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s011.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Sample size determination with imprecise risk aversion</title>
      <authors>
        <author>
          <name>Malcolm Farrow</name>
          <email>malcolm.farrow@ncl.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Newcastle Upon Tyne</name>
              <latitude>54.97328</latitude>
              <longitude>-1.61396</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>design of experiments</keyword>
        <keyword>imprecise utility</keyword>
        <keyword>risk aversion</keyword>
        <keyword>sample size</keyword>
      </keywords>
      <abstract>We consider multi-attribute utility functions, particularly applied to the choice of a design and sample sizes for an experiment. We extend earlier work, which allowed imprecision in the trade-offs between attributes, to allow imprecision also in the shape of marginal utility functions. The method is illustrated with a simple example involving a two-group binomial experiment.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s012.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Computing with Confidence</title>
      <authors>
        <author>
          <name>Scott Ferson</name>
          <email>scott@ramas.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Setauket</name>
              <latitude>40.93510</latitude>
              <longitude>-73.11844</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michael Balch</name>
          <email>Michael.Balch.ctr@wpafb.af.mil</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Setauket</name>
              <latitude>40.93510</latitude>
              <longitude>-73.11844</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Kari Sentz</name>
          <email>ksentz@lanl.gov</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Los Alamos</name>
              <latitude>35.84951</latitude>
              <longitude>-106.28848</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jack Siegrist</name>
          <email>jack@ramas.com</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Setauket</name>
              <latitude>40.93510</latitude>
              <longitude>-73.11844</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>confidence intervals</keyword>
        <keyword>confidence structures</keyword>
        <keyword>c-boxes</keyword>
        <keyword>p-box</keyword>
        <keyword>probability bounds analysis</keyword>
        <keyword>binomial probability</keyword>
        <keyword>imprecise beta model</keyword>
        <keyword>t-distribution</keyword>
      </keywords>
      <abstract>Traditional confidence intervals are useful in engineering because they offer a guarantee of statistical performance through repeated use. However, it is difficult to employ them consistently in analyses and assessments because it is not clear how to propagate them through mathematical calculations. Confidence structures (c-boxes) generalize confidence distributions and provide an interpretation by which confidence intervals at any confidence level can be specified for a parameter of interest. C-boxes can be used in calculations using the standard methods of probability bounds analysis and yield results that also admit the confidence interpretation. Thus analysts using them can now literally compute with confidence. We illustrate the calculation and use of c-boxes for some elementary inference problems and describe R functions to compute them and some Monte Carlo simulations demonstrating the coverage performance of the c-boxes and calculations based on them.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s013.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Entropy Based Decision Trees</title>
      <authors>
        <author>
          <name>Paul Fink</name>
          <email>Paul.Fink@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Richard Crossman</name>
          <email>r.j.crossman@warwick.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Warwick</name>
              <latitude>52.33333</latitude>
              <longitude>-1.58333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>classification trees</keyword>
        <keyword>nonparametric predictive inference</keyword>
      </keywords>
      <abstract>One method for building classification trees is to choose split variables by maximising expected entropy. This can be extended through the application of imprecise probability by replacing instances of expected entropy with the maximum possible expected entropy over credal sets of probability distributions. Such methods may not take full advantage of the opportunities offered by imprecise probability theory. In this paper, we change focus from maximum possible expected entropy to the full range of expected entropy. We present an entropy minimisation algorithm using the non--parametric inference approach to multinomial data. We also present an interval comparison method based on two user--chosen parameters, which includes previously presented splitting criteria (maximum entropy and entropy interval dominance) as special cases. This method is then applied to 13 datasets, and the various possible values of the two user--chosen criteria are compared with regard to each other, and to the entropy maximisation criteria which our approach generalises.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s014.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>On Open Problems Connected with Application of the Iterative Proportional Fitting Procedure to Belief Functions</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Vaclav Kratochvil</name>
          <email>v.kratochvil@gmail.com</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>marginal problem</keyword>
        <keyword>belief function</keyword>
        <keyword>algorithm</keyword>
        <keyword>multidimensional model</keyword>
        <keyword>convergence</keyword>
      </keywords>
      <abstract>In probability theory, Iterative Proportional Fitting Procedure can be used for construction of a joint probability measure from a system of its marginals. The present paper studies a possibility of application of an analogous procedure for belief functions, which was made possible by the fact that there exist operators of composition for belief functions. In fact, two different procedures based on two different composition operators are introduced. The procedure based on the composition derived from the Dempster's rule of combination is of very high computationally complexity and, from the theoretical point of view, practically nothing is known about its behavior. The other one, which uses the composition derived from the notion of factorization, is much more computationally efficient, and its convergence is guaranteed by a theorem proved in this paper.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s015.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Dynamic Credal Networks: introduction and use in robustness analysis</title>
      <authors>
        <author>
          <name>Matthieu Hourbracq</name>
          <email>matthieu.hourbracq@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cedric Baudrit</name>
          <email>cbaudrit@grignon.inra.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Thiverval-Grignon</name>
              <latitude>48.85108</latitude>
              <longitude>1.91710</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Pierre-Henri Wuillemin</name>
          <email>pierre-henri.wuillemin@lip6.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Paris</name>
              <latitude>48.85341</latitude>
              <longitude>2.34880</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>sebastien.destercke@utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dynamic bayesian network</keyword>
        <keyword>credal network</keyword>
        <keyword>robustness analysis</keyword>
        <keyword>independence</keyword>
        <keyword>inference algorithms</keyword>
        <keyword>food processing</keyword>
      </keywords>
      <abstract>Dynamic Bayesian networks (DBN) are very handy tools to model complex dynamical system described by collected data and expert knowledge. However, expert knowledge may be incomplete, and data may be scarce (this is typically the case in Life Science processes). In such cases, using precise parameters to describe the network does not faithfully account for our lack of information. This is why we propose, in this paper, to extend the notion of Dynamic Bayesian networks to convex sets of probabilities, introducing the notion of dynamic credal networks (DCN). We propose different extension relying on different independence concepts, briefly discussing the difficulty of extending classical algorithms for each concepts. We then apply DCN to perform a robustness analysis of DBN in a real-case study concerning the microbial population growth during a French cheese ripening process.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s016.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>Second-Order Credal Combination of Evidence</title>
      <authors>
        <author>
          <name>Alexander Karlsson</name>
          <email>alexander.karlsson@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>David Sundgren</name>
          <email>dsn@dsv.su.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Stockholm</name>
              <latitude>59.33258</latitude>
              <longitude>18.06490</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>second-order credal combination</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>credal set</keyword>
        <keyword>second-order probability</keyword>
        <keyword>combination</keyword>
        <keyword>evidence</keyword>
      </keywords>
      <abstract>We utilize second-order probability distributions for modeling second-order information over imprecise evidence in the form of credal sets. We generalize the Dirichlet distribution to a shifted version, denoted the S-Dirichlet, which allows one to restrict the support of the distribution by lower bounds. Based on the S-Dirichlet distribution, we present a simple combination schema denoted as second-order credal combination (SOCC), which takes second-order probability into account. The combination schema is based on a set of particles, sampled from the operands, and a set of weights that are obtained through the S-Dirichlet distribution. We show by examples that the second-order probability distribution over the imprecise joint evidence can be remarkably concentrated and hence that the credal combination operator can significantly overestimate the imprecision.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s017.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Evaluation of Evidential Combination Operators</title>
      <authors>
        <author>
          <name>Alexander Karlsson</name>
          <email>alexander.karlsson@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joe Steinhauer</name>
          <email>joe.steinhauer@his.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Skovde</name>
              <latitude>58.39118</latitude>
              <longitude>13.84506</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>evidential combination</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>credal set</keyword>
      </keywords>
      <abstract>We present an experiment for evaluating precise and imprecise evidential combination operators. The experiment design is based on the assumption that only limited statistical information is available in the form of multinomial observations. We evaluate three different evidential combination operators; one precise, the Bayesian combination operator, and two imprecise, the credal and Dempster's combination operator, for combining independent pieces of evidence regarding some discrete state space of interest. The evaluation is performed by using a score function that takes imprecision into account. The results show that the precise framework, i.e., the precise Dirichlet model and the Bayesian combination operator, seems to perform equally well as the imprecise frameworks based on the imprecise Dirichlet model and the imprecise operators.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s018.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Rationalizability under Uncertainty using Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Hailin Liu</name>
          <email>hailinl@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>game theory</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>rationalizability</keyword>
        <keyword>gamma-maximin rationalizability</keyword>
      </keywords>
      <abstract>The notion of imprecise probability can be viewed as a generalization of the traditional notion of probability. Several theories and models of imprecise probability have been suggested in the literature as more appropriate representations of uncertainty in the context of single-agent decision making. In this paper we investigate the question of how such models can be incorporated into the traditional game-theoretic framework. In the spirit of rationalizability, we present a new solution concept called Gamma-maximin rationalizability that captures the idea that each player models the other players as decision makers who all employ Gamma-maximin as their decision rule. Some properties of this concept such as existence conditions and the relationship with rationalizability are studied.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s019.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Significance of a decision making problem under uncertainty</title>
      <authors>
        <author>
          <name>Kevin Loquin</name>
          <email>kevin.loquin@gmail.com</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Montpellier</name>
              <latitude>43.61092</latitude>
              <longitude>3.87723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>savage eum</keyword>
        <keyword>decision theory</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval dominance</keyword>
        <keyword>significance</keyword>
      </keywords>
      <abstract>In this paper, we work on the interval dominance based extension of the Savage EUM approach. While usual probabilities only handle variability due uncertainty, imprecise probabilities handle in a unique framework uncertainty due to variability and uncertainty due to lack of knowledge (epistemic uncertainty). This second side of uncertainty generates ambiguity in the decision process which does not appear in the usual probabilistic Savage approach. Ambiguity here means that the preference relation used to perform the decision is not complete, i.e. that some decisions are not comparable. This comparability (or its opposite ambiguity) is linked to the informativity of the assessed imprecise probabilities. Our proposal, in this paper, is that significance is the informativity degree of the imprecise probability model used in the imprecise SEUM approach which makes the problem change from ambiguous to comparable. We discuss a theoretical definition as well as a pragmatical one.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s020.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>New Prior Near-ignorance Models on the Simplex</title>
      <authors>
        <author>
          <name>Francesca Mangili</name>
          <email>francesca@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessio Benavoli</name>
          <email>alessio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>prior near-ignorance</keyword>
        <keyword>normalized infinitely divisible distribution</keyword>
        <keyword>imprecise dirichlet model</keyword>
      </keywords>
      <abstract>The aim of this paper is to derive new near-ignorance models on the probability simplex, which do not directly involve the Dirichlet distribution and, thus, that are alternative to the Imprecise Dirichlet Model. We focus our investigation to a particular class of distributions on the simplex which is known as the class of Normalized Infinitely Divisible distributions; it includes the Dirichlet distribution as a particular case. Starting from three members of this class, which admit a closed-form expression for the probability density function, we derive three new near-ignorance prior models on the simplex, we analyse their properties and compare them with the Imprecise Dirichlet Model.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s021.pdf</pdf>
    </paper>
    <paper>
      <id>022</id>
      <title>A New Framework for Learning Generalized Credal Networks</title>
      <authors>
        <author>
          <name>Andres Masegosa</name>
          <email>andrew@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Serafin Moral</name>
          <email>smc@decsai.ugr.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Granada</name>
              <latitude>37.18817</latitude>
              <longitude>-3.60667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>learning</keyword>
        <keyword>imprecise sample size dirichlet model</keyword>
        <keyword>search algorithms</keyword>
      </keywords>
      <abstract>In this paper we give a formal specification of the problem of learning credal networks from observations. It is based on considering different equivalent sample sizes for the Dirichlet prior distributions about the probabilities of the conditional distributions. The novelty is that we specify what is the set of possible decisions and that this set may also include the selection of the equivalent sample size from a set of observations. Different Bayesian approaches can be considered as particular cases of this general framework. Approximate and exact algorithms based on A$^*$ search procedure are provided to compute the set of undominated decisions. Some preliminary experiments are reported.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s022.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>Credal model averaging of logistic regression for modeling the distribution of marmot burrows</title>
      <authors>
        <author>
          <name>Andrea Mignatti</name>
          <email>mignatti@elet.polimi.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Milan</name>
              <latitude>45.46427</latitude>
              <longitude>9.18951</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giorgio Corani</name>
          <email>giorgio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian model averaging</keyword>
        <keyword>credal model averaging</keyword>
        <keyword>logistic regression</keyword>
        <keyword>classification</keyword>
        <keyword>ecological modelling</keyword>
      </keywords>
      <abstract>Credal model averaging (CMA) is a credal ensemble of Bayesian models, which generalizes Bayesian model averaging (BMA). An open problem of BMA is how to set the prior over the models. CMA overcomes this problem by substituting the single prior over the models by a set of priors. We devise CMA for logistic regression; the different logistic regressors, over which the credal averaging is performed, are characterized by different feature sets. CMA returns indeterminate classifications when the classification is prior-dependent, namely when the most probable class (presence or absence) depends on the prior which is set over the models. We apply CMA for modelling the distribution of marmot burrows in an Alpine valley in Italy.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s023.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Coherent updating of 2-monotone previsions</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ignacio Montes</name>
          <email>imontes@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower prevision</keyword>
        <keyword>n-monotonicity</keyword>
        <keyword>belief function</keyword>
        <keyword>minitive measures</keyword>
        <keyword>natural extension</keyword>
        <keyword>regular extension</keyword>
      </keywords>
      <abstract>The conditions under which a 2-monotone lower prevision can be uniquely updated to a conditional lower prevision are determined. Then a number of particular cases are investigated: completely monotone lower previsions, for which equivalent conditions in terms of the focal elements of the associated belief function are established; random sets, for which some conditions in terms of the measurable selections can be given; and minitive lower previsions, which are shown to correspond to the particular case of vacuous lower previsions.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s024.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Computing the conglomerable natural extension</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower prevision</keyword>
        <keyword>conglomerability</keyword>
        <keyword>conglomerable natural extension</keyword>
        <keyword>natural extension</keyword>
        <keyword>marginal extension</keyword>
      </keywords>
      <abstract>Given a coherent lower prevision P, we consider the problem of computing the smallest coherent lower prevision C greater than P that is conglomerable, in case it exists. C is called the conglomerable natural extension. Past work has shown that C can be approximated by an increasing sequence of coherent lower previsions. We close an open problem by showing that this sequence can be infinite, while being made of distinct elements. Moreover, we give sufficient conditions, of quite broad applicability, to make sure that the point-wise limit of the sequence is C in case P is the lower envelope of finitely many linear previsions. In addition, we study the question of the existence of C and its relationship with the notion of marginal extension.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s025.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>Modeling Uncertainty in First-Order Logic</title>
      <authors>
        <author>
          <name>Rafael Nunez</name>
          <email>nunez@umiami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Scheutz</name>
          <email>mscheutz@cs.tufts.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Somerville</name>
              <latitude>42.38092</latitude>
              <longitude>-71.09890</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Kamal Premaratne</name>
          <email>kamal@miami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Manohar N. Murthi</name>
          <email>mmurthi@miami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertain logic</keyword>
        <keyword>uncertain reasoning</keyword>
        <keyword>probabilistic logic</keyword>
        <keyword>dempster-shafer theory</keyword>
        <keyword>belief theory</keyword>
      </keywords>
      <abstract>First order logic lies at the core of many methods in mathematics, philosophy, linguistics, and computer science. Although important efforts have been made to extend first order logic to the task of handling uncertainty, there is still a lack of a consistent and unified approach, especially within the Dempster-Shafer (DS) theory framework. In this work we introduce a systematic approach for building belief assignments based on first order logic formulas. Furthermore, we outline the foundations of Uncertain Logic, a robust framework for inference and modeling when information is available in the form of first order logic formulas subject to uncertainty. Applications include data fusion, rule mining, credibility estimation, crowd sourcing, among many others.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s026.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Characterizing coherence, correcting incoherence</title>
      <authors>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>Erik.Quaeghebeur@UGent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>linear constraint</keyword>
        <keyword>polytope</keyword>
        <keyword>enumeration</keyword>
        <keyword>projection</keyword>
        <keyword>multi-objective linear programming</keyword>
        <keyword>incoherence</keyword>
        <keyword>dominance</keyword>
      </keywords>
      <abstract>Lower previsions defined on a finite set of gambles can be looked at as points in a finite-dimensional real vector space. Within that vector space, the sets of sure loss avoiding and coherent lower previsions form convex polyhedra. We present procedures for obtaining characterizations of these polyhedra in terms of a minimal, finite number of linear constraints. As compared to the previously known procedure, these procedures are more efficient and much more straightforward. Next, we take a look at a procedure for correcting incoherent lower previsions based on pointwise dominance. This procedure can be formulated as a multi-objective linear program, and the availability of the finite characterizations provide an avenue for making these programs computationally feasible.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s027.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>On Sharp Identification Regions for Regression Under Interval Data</title>
      <authors>
        <author>
          <name>Georg Schollmeyer</name>
          <email>georg.schollmeyer@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>thomas.augustin@stat.uni-muenchen.de/a&gt;</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial identification</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>interval data</keyword>
        <keyword>sharp identification regions</keyword>
        <keyword>coarse data</keyword>
        <keyword>adjunctions</keyword>
        <keyword>partially ordered sets</keyword>
        <keyword>linear regression</keyword>
        <keyword>best linear predictor</keyword>
        <keyword>set-domained loss function</keyword>
      </keywords>
      <abstract>The reliable analysis of interval data (coarsened data) is one of the most promising applications of imprecise probabilities in statistics. If one refrains from making untestable, and often materially unjustified, strong assumptions on the coarsening process, then the empirical distribution of the data is imprecise, and statistical models are, in Manski's terms, partially identified. We first elaborate some subtle differences between two natural ways of handling interval data in the dependent variable of regression models, distinguishing between two different types of identification regions, called Sharp Marrow Region (SMR) and \em Sharp Collection Region (SCR) here. Focusing on the case of linear regression analysis, we then derive some fundamental geometrical properties of SMR and SCR, allowing a comparison of the regions and providing some guidelines for their canonical construction. Relying on the algebraic framework of adjunctions of two mappings between partially ordered sets, we characterize SMR as a right adjoint and as the monotone kernel of a criterion function based mapping, while SCR is indeed interpretable as the corresponding monotone hull. Finally we sketch some ideas on a compromise between SMR and SCR based on a set-domained loss function.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s028.pdf</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>Two theories of conditional probability and non-conglomerability</title>
      <authors>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mark Schervish</name>
          <email>mark@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Joseph Kadane</name>
          <email>kadane@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>non-conglomerability</keyword>
        <keyword>conditional probability</keyword>
        <keyword>k-additive probability</keyword>
        <keyword>regular conditional distribution</keyword>
      </keywords>
      <abstract>Conglomerability of conditional probabilities is suggested by some (e.g., Walley, 1991) as necessary for rational degrees of belief. Here we give sufficient conditions for non-conglomerability of conditional probabilities in the de Finetti/Dubins sense of conditional probability. These sufficient conditions cover familiar cases where P(.) is a continuous, countably additive probability. In this regard, we contrast the de Finetti/Dubins sense of conditional probability with the more familiar account of regular conditional distributions, in the fashion of Kolmogorov.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s029.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Conflict and Ambiguity: Preliminary Models and Empirical Tests</title>
      <authors>
        <author>
          <name>Michael Smithson</name>
          <email>Michael.Smithson@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>uncertainty</keyword>
        <keyword>ambiguity</keyword>
        <keyword>conflict</keyword>
        <keyword>judgement</keyword>
        <keyword>decision</keyword>
      </keywords>
      <abstract>The proposition that conflict and ambiguity are distinct kinds of uncertainty remains debatable, although there is substantial behavioral and some neurological evidence favoring this claim. Recently formal decisional models that combine ambiguity and conflict have been proposed. This paper presents empirical tests of four hypotheses and five models of uncertainty judgments under ambiguity and conflict, via comparisons between pairs of conflicting and ambiguous interval estimates by a sample of 395 adults. The main findings are as follows.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s030.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>A Robust Data Driven Approach to Quantifying Common-Cause Failure in Power Networks</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Simon Blake</name>
          <email>s.r.blake@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>robust</keyword>
        <keyword>alpha-factor</keyword>
        <keyword>failure</keyword>
        <keyword>reliability</keyword>
        <keyword>gamma</keyword>
        <keyword>dirichlet</keyword>
      </keywords>
      <abstract>The standard alpha-factor model for common cause failure assumes symmetry, in that all components must have identical failure rates. In this paper, we generalise the alpha-factor model to deal with asymmetry, in order to apply the model to power networks, which are typically asymmetric. For parameter estimation, we propose a set of conjugate Dirichlet-Gamma priors, and we discuss how posterior bounds can be obtained. Finally, we demonstrate our methodology on a simple yet realistic example.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s031.pdf</pdf>
    </paper>
    <paper>
      <id>032</id>
      <title>A Note on the Temporal Sure Preference Principle and the Updating of Lower Previsions</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Michael Goldstein</name>
          <email>Michael.Goldstein@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>updating</keyword>
        <keyword>inference</keyword>
        <keyword>temporal coherence</keyword>
        <keyword>desirability</keyword>
        <keyword>lower prevision</keyword>
      </keywords>
      <abstract>This paper reviews the temporal sure preference principle as a basis for inference over time. We reformulate the principle in terms of desirability, and explore its implications for lower previsions. We report some initial results. We also discuss some of the technical difficulties encountered.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s032.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>Logistic Regression on Markov Chains for Crop Rotation Modelling</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Lewis Paton</name>
          <email>l.w.paton@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>logistic regression</keyword>
        <keyword>markov chain</keyword>
        <keyword>robust bayesian</keyword>
        <keyword>conjugate</keyword>
        <keyword>maximum likelihood</keyword>
        <keyword>crop</keyword>
      </keywords>
      <abstract>Often, in dynamical systems, such as farmer's crop choices, the dynamics is driven by external non-stationary factors, such as rainfall, temperature, and economy. Such dynamics can be modelled by a non-stationary Markov chain, where the transition probabilities are logistic functions of such external factors. We investigate the problem of estimating the parameters of the logistic model from data, using conjugate analysis with a fairly broad class of priors, to accommodate scarcity of data and lack of strong prior expert opinions. We show how maximum likelihood methods can be used to get bounds on the posterior mode of the parameters.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s033.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>Model checking for imprecise Markov chains</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise markov chain</keyword>
        <keyword>model checking</keyword>
        <keyword>parse tree</keyword>
        <keyword>logic</keyword>
        <keyword>computation</keyword>
      </keywords>
      <abstract>We extend probabilistic computational tree logic for expressing properties of Markov chains to imprecise Markov chains, and provide an efficient algorithm for model checking of imprecise Markov chains. Thereby, we provide a formal framework to answer a very wide range of questions about imprecise Markov chains, in a systematic and computationally efficient way.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s034.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>An imprecise boosting-like approach to regression</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Andrea Wiencierz</name>
          <email>andrea.wiencierz@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>regression</keyword>
        <keyword>adaboost</keyword>
        <keyword>algorithm</keyword>
        <keyword>linear-vacuous mixture model</keyword>
        <keyword>kolmogorov-smirnov bounds</keyword>
      </keywords>
      <abstract>This paper is about a generalization of ensemble methods for regression which are based on variants of the basic AdaBoost algorithm. The generalization of these regression methods consists in restricting the unit simplex for the weights of the instances to a smaller set of weighting probabilities. The proposed algorithms cover the standard AdaBoost-based regression algorithms and standard regression as special cases. Various imprecise statistical models can be used to obtain the restricted set of probabilities. One advantage of the proposed algorithms compared to the basic AdaBoost-based regression methods is that they have less tendency to over-fitting, because the weights of the hard instances are restricted. Finally, some simulations and applications also indicate a better performance of the proposed generalized methods.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s035.pdf</pdf>
    </paper>
    <paper>
      <id>036</id>
      <title>Operator of composition for credal sets</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal set</keyword>
        <keyword>graphical model</keyword>
        <keyword>conditional independence</keyword>
      </keywords>
      <abstract>This paper is the first attempt to introduce the operator of composition, already known from probability, possibility and evidence theories, also for credal sets. We prove that the proposed definition preserves all the necessary properties of the operator enabling us to define compositional models as an efficient tool for multidimensional models representation. Theoretical results are accompanied by numerous illustrative examples.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s036.pdf</pdf>
    </paper>
    <paper>
      <id>037</id>
      <title>Modelling practical certainty and its link with classical propositional logic</title>
      <authors>
        <author>
          <name>Arthur van Camp</name>
          <email>arthur.vancamp@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>accept and reject statement-based uncertainty models</keyword>
        <keyword>classical propositional logic</keyword>
        <keyword>belief structure</keyword>
      </keywords>
      <abstract>We model practical certainty in the language of accept &amp; reject statement-based uncertainty models. We present three different ways, each time using a different nature of assessment: we study coherent models following from (i) favourability assessments, (ii) acceptability assessments, and (iii) indifference assessments. We argue that a statement of favourability, when used with an appropriate background model, essentially boils down to stating a belief of practical certainty using acceptability assessments. We show that the corresponding models do not form not an intersection structure, in contradistinction with the coherent models following from an indifference assessment. We construct embeddings of classical propositional logic into each of our models for practical certainty.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s037.pdf</pdf>
    </paper>
    <paper>
      <id>038</id>
      <title>Interval-Valued Linear Model</title>
      <authors>
        <author>
          <name>Xun Wang</name>
          <email>xunwang00@gmail.com</email>
          <location>
            <country>
              <code>CN</code>
              <name>China</name>
            </country>
            <city>
              <name>Beijing</name>
              <latitude>39.90750</latitude>
              <longitude>116.39723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Shoumei Li</name>
          <email>lisma@bjut.edu.cn</email>
          <location>
            <country>
              <code>CN</code>
              <name>China</name>
            </country>
            <city>
              <name>Beijing</name>
              <latitude>39.90750</latitude>
              <longitude>116.39723</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thierry Denoeux</name>
          <email>thierry.denoeux@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>interval-valued linear model</keyword>
        <keyword>least square estimation</keyword>
        <keyword>best binary linear unbiased estimation</keyword>
        <keyword>d_p metric</keyword>
      </keywords>
      <abstract>This paper introduces a new type of statistical model: the interval-valued linear model, which describes the linear relationship between an interval-valued output random variable and real-valued input variables. Firstly, we discuss the notions of variance and covariance of set-valued and interval-valued random variables. Then, we give the definition of the interval-valued linear model and its least square estimation, as well as some properties of the least square estimation. Thirdly, we show that, whereas the best linear unbiased estimation does not exist, the best binary linear unbiased estimator exists and it is just the least square estimator. Finally, we present a simulation experiment and an application example regarding temperature of cities affected by their latitude, which illustrates the application of our model.</abstract>
      <pdf>http://www.sipta.org/isipta13/proceedings/papers/s038.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2015</year>
    <conference>
      <date>
        <start>2015-07-20</start>
        <end>2015-07-24</end>
      </date>
      <location>
        <country>
          <code>IT</code>
          <name>Italy</name>
        </country>
        <city>
          <name>Pescara</name>
          <latitude>42.46024</latitude>
          <longitude>14.21021</longitude>
        </city>
        <university>
          <name>University G. d'Annunzio</name>
          <department>Department of Engineering and Geology</department>
        </university>
      </location>
    </conference>
    <paper>
      <id>032</id>
      <title>The multilabel naive credal classifier</title>
      <authors>
        <author>
          <name>Alessandro Antonucci</name>
          <email>alessandro@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giorgio Corani</name>
          <email>giorgio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal classification</keyword>
        <keyword>imprecise dirichlet model</keyword>
        <keyword>multilabel classification</keyword>
      </keywords>
      <abstract>We present a credal classifier for multilabel data. The model generalizes the naive credal classifier to the multilabel case. An imprecise-probabilistic quantification is achieved by means of the imprecise Dirichlet model in its global formulation. A polynomial-time algorithm to compute whether or not a label is optimal according to the maximality criterion is derived. Experimental results show the importance of robust predictions in multilabel problems</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/32.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>Efficient L1-based probability assessments correction: algorithms and applications to belief merging and revision</title>
      <authors>
        <author>
          <name>Andrea Capotorti</name>
          <email>andrea.capotorti@unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Baioletti</name>
          <email>marco.baioletti@unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherence</keyword>
        <keyword>mixed-integer optimization</keyword>
        <keyword>probability merging and revision</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>In this article we define a procedure which corrects an incoherent probability assessment on a finite domain by exploiting a geometric property of L1-distance (known also as Manhattan distance) and mixed integer programming. L1-distance minimization does not produce, in general, a unique solution but rather a corrected assessment that could result an imprecise probability model. We propose a correction method for the merging of two separate assessments whose direct juxtaposition could be incoherent, and for the revision of beliefs where the core of the assessment must remain unchanged. A prototypical example on antidoping analysis guides the reader through this article to explain the various procedures.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/14.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>The geometry of imprecise inference</title>
      <authors>
        <author>
          <name>Mikelis Bickis</name>
          <email>bickis@snoopy.usask.ca</email>
          <location>
            <country>
              <code>CA</code>
              <name>Canada</name>
            </country>
            <city>
              <name>Saskatoon</name>
              <latitude>52.11679</latitude>
              <longitude>-106.63452</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>information geometry</keyword>
        <keyword>exponential family</keyword>
        <keyword>sets of measures</keyword>
      </keywords>
      <abstract>A statistical model can be constructed from a null probability measure by defining a set of statistics representing log-likelihood ratios of alternative measures to the null measure. Conversely, any model consisting of equivalent measures can be so expressed. A linear combination of statistics will also define a log-likelihood ratio if the normalizing constant is finite. In this way, any such model can be naturally extended to a convex subset of the linear span of these statistics. A finite dimensional subset defines an exponential family with the canonical parameters of a measure defined by coordinates relative to a set of basis functions. Given a base measure on the parameter space, one can implement a similar structure with a set of parametric functions. The log-likelihood itself being a parametric function, the set of all possible log-likelihoods thus defines a space of measures conjugate to the statistical model. The conjugate space will have one more dimension spanned by the above-mentioned parameter-dependent normalizing constant. If the base measure is considered a prior distribution, then the translation by the observed log-likelihood defines the posterior. An imprecise prior defined by a set of measures is in the same manner translated to a set of posterior measures. Upper and lower previsions can then be computed as extrema over this posterior set.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/31.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>How to choose among choice functions</title>
      <authors>
        <author>
          <name>Seamus Bradley</name>
          <email>seamus.bradley@lmu.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>decision making</keyword>
        <keyword>choice function</keyword>
        <keyword>sets of probabilities</keyword>
      </keywords>
      <abstract>If one models an agent's degrees of belief by a set of probabilities, how should that agent's choices be constrained? In other words, what choice function should the agent use? This paper summarises some suggestions, and outlines a collection of properties of choice functions that can distinguish between different functions.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/9.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>The generalization of the conjunctive rule for aggregating contradictory sources of information based on generalized credal sets</title>
      <authors>
        <author>
          <name>Andrew Bronevich</name>
          <email>brone@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Moscow</name>
              <latitude>55.75222</latitude>
              <longitude>37.61556</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Igor Rozenberg</name>
          <email>i.rozenberg@gismps.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Moscow</name>
              <latitude>55.75222</latitude>
              <longitude>37.61556</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>conjunctive rule</keyword>
        <keyword>generalized credal sets</keyword>
        <keyword>contradictory sources of information</keyword>
      </keywords>
      <abstract>In the paper we consider the generalization of the conjunctive rule in the theory of imprecise probabilities. Let us remind that the conjunction rule, produced on credal sets, gives their intersection and it is not defined if this intersection is empty. In the last case the sources of information are called contradictory. Meanwhile, in the Dempster-Shafer theory it is possible to use the conjunctive rule for contradictory sources of information having as a result a non-normalized belief function that can be greater than zero at empty set. In the paper we try to exploit this idea and introduce into consideration so called generalized credal sets allowing to model imprecision (non-specificity), conflict, and contradiction in information. Based on generalized credal sets the conjunctive rule is well defined for contradictory sources of information and it can be conceived as the generalization of the conjunctive rule for belief functions. We also show how generalized credal sets can be used for modeling information when the avoiding sure loss condition is not satisfied, and consider coherence conditions and natural extension based on generalized credal sets.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/8.pdf</pdf>
    </paper>
    <paper>
      <id>039</id>
      <title>Decisions under risk and partial knowledge modelling uncertainty and risk aversion</title>
      <authors>
        <author>
          <name>Barbara Vantaggi</name>
          <email>barbara.vantaggi@sbai.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Davide Petturiti</name>
          <email>davide.petturiti@dmi.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Giulianella Coletti</name>
          <email>coletti@dmi.unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>preference</keyword>
        <keyword>choquet rationality</keyword>
        <keyword>concave utility</keyword>
        <keyword>choquet expected utility</keyword>
      </keywords>
      <abstract>We deal with decisions under risk starting from a partial preference relation on a finite set of generalized convex lotteries, that are random quantities equipped with a convex capacity. A necessary and sufficient condition (Choquet rationality) is provided for its representability as a Choquet expected utility of a strictly increasing utility function. The restriction to concave utility functions is discussed. Moreover, we show that this condition, with or without the constraint of concavity for the utility function, assures the extension of the preference relation and it actually guides the decision maker in the extension process.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/39.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>Some remarks on sets of lexicographic probabilities and sets of desirable gambles</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords/>
      <abstract>Sets of lexicographic probabilities and sets of desirable gambles share several features, despite their apparent differences. In this paper we examine properties of marginalization, conditioning and independence for sets of lexicographic probabilities and sets of desirable gambles.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/5.pdf</pdf>
    </paper>
    <paper>
      <id>001</id>
      <title>On the complexity of propositional and relational credal networks</title>
      <authors>
        <author>
          <name>Denis Maua</name>
          <email>denis.maua@gmail.com</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal network</keyword>
        <keyword>propositional logic</keyword>
        <keyword>relational logic</keyword>
        <keyword>complexity</keyword>
        <keyword>data complexity</keyword>
      </keywords>
      <abstract>A credal network associates a directed acyclic graph with a collection of sets of probability measures. Usually these probability measures are specified through several tables containing probability values. Here we examine the complexity of inference in Boolean credal networks when probability measures are specified through formal languages, by extending a framework we have recently proposed for Bayesian networks. We show that sub-Boolean and relational logics lead to interesting complexity results. In short, we explore the relationship between language and complexity in credal networks.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/1.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>A pointwise ergodic theorem for imprecise Markov chains</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Stavros Lopatatzidis</name>
          <email>stavros.lopatatzidis@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>lower expectation</keyword>
        <keyword>pointwise ergodic theorem</keyword>
        <keyword>imprecise markov chain</keyword>
        <keyword>game-theoretic probability</keyword>
      </keywords>
      <abstract>We prove a game-theoretic version of the strong law of large numbers for submartingale differences, and use this to derive a pointwise ergodic theorem for discrete-time Markov chains with finite state sets, when the transition probabilities are imprecise, in the sense that they are only known to belong to some convex closed set of probability measures.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/2.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>Fully conglomerable coherent upper conditional prevision defined by the Choquet integral with respect to its associated Hausdorff outer measure</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent upper conditional previsions</keyword>
        <keyword>hausdorff outer measure</keyword>
        <keyword>choquet integral</keyword>
        <keyword>disintegration property</keyword>
        <keyword>conglomerability principle</keyword>
      </keywords>
      <abstract>Let (__, d) be a metric space where __ is a set with positive and finite Hausdorff outer measure in its Hausdorff dimension and let B be a partition of __. The coherent upper conditional prevision defined as the Choquet integral with respect to its associated Hausdorff outer measure is proven to satisfy the disintegration property and the conglomerative principle on every partition.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/6.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Coherent conditional measures of risk defined by the Choquet integral with respect to Hausdorff outer measures and dependent risks</title>
      <authors>
        <author>
          <name>Serena Doria</name>
          <email>s.doria@dst.unich.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Chieti</name>
              <latitude>42.36094</latitude>
              <longitude>14.13801</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent conditional measures of risk</keyword>
        <keyword>hausdorff outer measure</keyword>
        <keyword>choquet integral</keyword>
        <keyword>stochastic dependence</keyword>
      </keywords>
      <abstract>Let (__, d) be a metric space and let B be a partition of __. For every set B of B with positive and finite Hausdorff outer measure in its Hausdorff dimension, a coherent conditional measure of risk is defined as the Choquet integral with respect to Hausdorff outer measure. Two risks are defined to be s-independent if the atoms of the classes generated by their weak upper level sets are s-independent. The given notion permits to capture dependence between risks that are stochastically independent according to the axiomatic definition. Two risks which are surjective and injective are proven to be s-dependent and a sufficient condition is given such that s-independent simple risks satisfy the factorization property of their joint coherent measures of risk.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/7.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Imprecise random variables, random sets, and Monte Carlo simulation</title>
      <authors>
        <author>
          <name>Michael Oberguggenberger</name>
          <email>michael.oberguggenberger@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Fetz</name>
          <email>thomas.fetz@uibk.ac.at</email>
          <location>
            <country>
              <code>AT</code>
              <name>Austria</name>
            </country>
            <city>
              <name>Innsbruck</name>
              <latitude>47.26266</latitude>
              <longitude>11.39454</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>upper and lower probability</keyword>
        <keyword>imprecise random variables</keyword>
        <keyword>random set</keyword>
        <keyword>propagation of uncertainty through a function</keyword>
        <keyword>monte carlo simulation</keyword>
      </keywords>
      <abstract>The paper addresses the evaluation of upper and lower probabilities induced by functions of an imprecise random variable. Given a function g and a family X___ of random variables, where the parameter __ ranges in an index set __, one may ask for the upper/lower probability that g(X___) belongs to some Borel set B. Two interpretations are investigated. In the first case, the upper probability is computed as the supremum of the probabilities that g(X___) lies in B. In the second case, one considers the random set generated by all g(X___), _______, e.g. by transforming X___ to standard normal as a common probability space, and computes the corresponding upper probability. The two results are different, in general. We analyze this situation and highlight the implications for Monte Carlo simulation. Attention is given to efficient simulation procedures and an engineering application is presented.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/12.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>Robust parameter estimation of density functions under fuzzy interval observations</title>
      <authors>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Romain Guillaume</name>
          <email>guillaum@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>possibility theory</keyword>
        <keyword>fuzzy intervals</keyword>
        <keyword>maximum likelihood</keyword>
        <keyword>robust optimization</keyword>
        <keyword>epistemic uncertainty</keyword>
      </keywords>
      <abstract>This paper deals with the derivation of a probabilistic parametric model from interval or fuzzy data using the maximum likelihood principle. In contrast with classical techniques such as the EM algorithm, that define a precise likelihood function by averaging inside each imprecise observations, our approach presupposes that each imprecise observation underlies a precise one, and that the uncertainty that pervades its observation is epistemic, rather than representing noise. We define an interval-valued likelihood function and apply robust optimisation methods to find a safe plausible estimate of the statistical parameters. The resulting density has a standard deviation that is large enough to cover the imprecision of the observations, making a pessimistic assumption on dispersion. This approach is extended to fuzzy data by optimizing the average of lower likelihoods over a collection of data sets obtained from cuts of the fuzzy intervals, as a trade off between optimistic and pessimistic interpretations of fuzzy data. The principles of this method are compared with those of other existing approaches to handle incompleteness of observations, especially the EM technique.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/10.pdf</pdf>
    </paper>
    <paper>
      <id>035</id>
      <title>On two composition operators in Dempster-Shafer theory</title>
      <authors>
        <author>
          <name>Radim Jirousek</name>
          <email>radim@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Jindrichuv Hradec</name>
              <latitude>49.14404</latitude>
              <longitude>15.00301</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>factorization</keyword>
        <keyword>conditional independence</keyword>
        <keyword>combination</keyword>
        <keyword>composition</keyword>
        <keyword>decomposable model</keyword>
        <keyword>ipfp</keyword>
      </keywords>
      <abstract>Efficient computations with probabilistic multidimensional models are made possible if the respective probability measure (distribution) is in the form of a decomposable model. Some of the advantageous properties of these models are based on the fact that factorization and conditional independence coincide. It means that a decomposable multidimensional model can be assembled (composed) from its low-dimensional marginals with the help of an operator of composition, which introduces conditional independence relations among the variables. The problem arises when we also want to apply these ideas in Dempster-Shafer theory of evidence, because two different operators of composition have been introduced in literature. The present paper serves as a survey of results on these two operators, recollects their common properties and differences, and tries to find a proper role for each of them.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/35.pdf</pdf>
    </paper>
    <paper>
      <id>036</id>
      <title>Common knowledge, ambiguity, and the value of information in games</title>
      <authors>
        <author>
          <name>Hailin Liu</name>
          <email>hailinl@andrew.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bayesian games</keyword>
        <keyword>value of information</keyword>
        <keyword>ambiguity</keyword>
        <keyword>act-state dependence</keyword>
        <keyword>dilation</keyword>
        <keyword>gamma-maximin</keyword>
      </keywords>
      <abstract>This paper asks whether the salient result about non-negative value of cost-free information holds in the context of games. By reexamining Osborne's example where information may hurt, it argues that the failure of this result is mainly driven by the assumption of common knowledge in the traditional framework of incomplete information games, since it leads to act-state dependence in a sequential setting. This paper also shows that such a failure occurs when we extend the framework of incomplete information games to allow for a representation of uncertainty using sets of probabilities and the use of __-maximin. Nevertheless, the key to this negative result is that a phenomenon called dilation of sets of probabilities obtains in this generalized setting.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/36.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>Calculating bounds on expected return and first passage times in finite-state imprecise birth-death chains</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Stavros Lopatatzidis</name>
          <email>stavros.lopatatzidis@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>birth-death chain</keyword>
        <keyword>markov chain</keyword>
        <keyword>imprecise</keyword>
        <keyword>return time</keyword>
        <keyword>first passage time</keyword>
        <keyword>credal set</keyword>
      </keywords>
      <abstract>We provide simple methods for computing exact bounds on expected return and first passage times in finite-state birth-death chains, when the transition probabilities are imprecise, in the sense that they are only known to belong to convex closed sets of probability mass functions. These so-called imprecise birth-death chains are special types of time-homogeneous imprecise Markov chains. We also present numerical results and discuss the special case where the local models are linear-vacuous mixtures, for which our methods simplify even more.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/26.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>A prior near-ignorance Gaussian process model for nonparametric regression</title>
      <authors>
        <author>
          <name>Francesca Mangili</name>
          <email>francesca@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>gaussian process</keyword>
        <keyword>prior near-ignorance</keyword>
        <keyword>nonparametric regression</keyword>
        <keyword>hypothesis testing</keyword>
        <keyword>bayesian nonparametrics</keyword>
      </keywords>
      <abstract>A Gaussian Process (GP) defines a distribution over functions and thus it is a natural prior distribution for learning real-valued functions from a set of noisy data. GPs offer a great modeling flexibility and have found widespread application in many regression problems. A GP is fully defined by a mean function that represents our prior belief about the shape of the regression function and a covariance function, relating the function values at different covariates. In the absence of prior information, one typically assumes a GP with zero mean function. Therefore, a priori, it is assumed that the regression function is constantly equal to zero. The aim of this paper is to model a situation of prior near-ignorance about the GP mean function. For this we consider the set of all GPs with fixed covariance function and constant mean function free to vary from ______ to +___. We apply the model with constant mean function to hypothesis testing; in particular we test the equality of two regression functions and show that the use of a prior near-ignorance model allows the test to automatically detect when a reliable decision cannot be made based on the available data. Finally, we propose a generalization of this model that allows considering other sets of prior mean functions.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/15.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Conformity and independence with coherent lower previsions</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coherent lower prevision</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>epistemic irrelevance</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>marginal extension</keyword>
        <keyword>strong product</keyword>
      </keywords>
      <abstract>We study the conformity of marginal unconditional and conditional models with a joint model under assumptions of epistemic irrelevance and independence, within Walley's theory of coherent lower previsions. By doing so, we make a link with a number of prominent models within this theory: the marginal extension, the irrelevant natural extension, the independent natural extension and the strong product.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/16.pdf</pdf>
    </paper>
    <paper>
      <id>004</id>
      <title>Comonotone lower probabilities for bivariate and discrete structures</title>
      <authors>
        <author>
          <name>Ignacio Montes</name>
          <email>ignacio.montes@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>sebastien.destercke@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>comonotonicity</keyword>
        <keyword>copulas</keyword>
        <keyword>lower probabilities</keyword>
        <keyword>belief function</keyword>
        <keyword>p-box</keyword>
      </keywords>
      <abstract>Two random variables are called comonotone when there is an increasing relation between them, in the sense that when one of them increases (decreases), the other one also increases (decreases). This notion has been widely investigated in probability theory, and is related to the theory of copulas. This contribution studies the notion of comonotonicity in an imprecise setting. We define comonotone lower probabilities and investigate its characterizations. Also, we provide some sufficient conditions allowing to define a comonotone belief function with fixed marginals and characterize comonotone bivariate p-boxes.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/4.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>A robust Bayesian analysis of the impact of policy decisions on crop rotations</title>
      <authors>
        <author>
          <name>Lewis Paton</name>
          <email>l.w.paton@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Nigel Boatman</name>
          <email>nigel.boatman@fera.gsi.gov.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Sand Hutton</name>
              <latitude>54.01776</latitude>
              <longitude>-0.93832</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Mohamud Hussein</name>
          <email>mohamud.hussein@fera.gsi.gov.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Sand Hutton</name>
              <latitude>54.01776</latitude>
              <longitude>-0.93832</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>multinomial logistic regression</keyword>
        <keyword>stochastic process</keyword>
        <keyword>robust bayesian</keyword>
        <keyword>conjugate</keyword>
        <keyword>maximum likelihood</keyword>
        <keyword>crop</keyword>
        <keyword>decision</keyword>
      </keywords>
      <abstract>We analyse the impact of a policy decision on crop rotations, using the imprecise land use model that was developed by the authors in earlier work. A specific challenge in crop rotation models is that farmer's crop choices are driven by both policy changes and external non-stationary factors, such as rainfall, temperature and agricultural input and output prices. Such dynamics can be modelled by a non-stationary stochastic process, where crop transition probabilities are multinomial logistic functions of such external factors. We use a robust Bayesian approach to estimate the parameters of our model, and validate it by comparing the model response with a non-parametric estimate, as well as by cross validation. Finally, we use the resulting predictions to solve a hypothetical yet realistic policy problem.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/17.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>Dilation, disintegrations, and delayed decisions</title>
      <authors>
        <author>
          <name>Arthur Paul Pedersen</name>
          <email>pedersen@mpib-berlin.mpg.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Berlin</name>
              <latitude>52.52330</latitude>
              <longitude>13.41377</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gregory Wheeler</name>
          <email>gregory.wheeler@lrz.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords/>
      <abstract>Both dilation and non-conglomerability have been alleged to conflict with a fundamental principle of Bayesian methodology that we call Good's Principle: one should always delay making a terminal decision between alternative courses of action if given the opportunity to first learn, at zero cost, the outcome of an experiment relevant to the decision. In particular, both dilation and non-conglomerability have been alleged to permit or even mandate choosing to make a terminal decision in deliberate ignorance of relevant, cost-free information. Although dilation and non-conglomerability share some similarities, some authors maintain that there are important differences between the two that warrant endorsing different normative positions regarding dilation and non-conglomerability. This article reassesses the grounds for treating dilation and non-conglomerability differently. Our analysis exploits a new and general characterization result for dilation to draw a closer connection between dilation and non-conglomerability.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/23.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Weak consistency for imprecise conditional previsions</title>
      <authors>
        <author>
          <name>Paolo Vicig</name>
          <email>paolo.vicig@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>williams coherence</keyword>
        <keyword>2-coherent previsions</keyword>
        <keyword>2-convex previsions</keyword>
        <keyword>generalized bayes rule</keyword>
      </keywords>
      <abstract>In this paper we explore relaxations of (Williams) coherent and convex conditional previsions that form the families of n-coherent and n-convex conditional previsions, at the varying of n. We investigate which such previsions are the most general one may reasonably consider, suggesting (centered) 2-convex or, if positive homogeneity and conjugacy is needed, 2-coherent lower previsions. Basic properties of these previsions are studied. In particular, centered 2-convex previsions satisfy the Generalized Bayes Rule and always have a 2-convex natural extension. We discuss then the rationality requirements of 2-convexity and 2-coherence from a desirability perspective. Among the uncertainty concepts that can be modelled by 2-convexity, we mention generalizations of capacities and niveloids to a conditional framework.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/13.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>Statistical modelling under epistemic data imprecision: some results on estimating multinomial distributions and logistic regression for coarse categorical data</title>
      <authors>
        <author>
          <name>Georg Schollmeyer</name>
          <email>georg.schollmeyer@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Cattaneo</name>
          <email>m.cattaneo@hull.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Kingston upon Hull</name>
              <latitude>53.74460</latitude>
              <longitude>-0.33525</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Julia Plass</name>
          <email>julia.plass@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coarse data</keyword>
        <keyword>missing data</keyword>
        <keyword>epistemic data imprecision</keyword>
        <keyword>sensitivity analysis</keyword>
        <keyword>partial identification</keyword>
        <keyword>categorical data</keyword>
        <keyword>multinomial logit model</keyword>
        <keyword>coarsening at random (car)</keyword>
        <keyword>likelihood</keyword>
      </keywords>
      <abstract>The paper deals with parameter estimation for categorical data under epistemic data imprecision, where for a part of the data only coarse(ned) versions of the true values are observable. For different observation models formalizing the information available on the coarsening process, we derive the (typically set-valued) maximum likelihood estimators of the underlying distributions. We discuss the homogeneous case of independent and identically distributed variables as well as logistic regression under a categorical covariate. We start with the imprecise point estimator under an observation model describing the coarsening process without any further assumptions. Then we determine several sensitivity parameters that allow the refinement of the estimators in the presence of auxiliary information.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/20.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Statistical modelling in surveys without neglecting the undecided: multinomial logistic regression models and imprecise classification trees under ontic data imprecision</title>
      <authors>
        <author>
          <name>Paul Fink</name>
          <email>paul.fink@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Julia Plass</name>
          <email>julia.plass@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Norbert Schoening</name>
          <email>norbert.schoening@gsi.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>ontic data imprecision</keyword>
        <keyword>survey methodology</keyword>
        <keyword>election polls</keyword>
        <keyword>multinomial logistic regression</keyword>
        <keyword>discrete choice models</keyword>
        <keyword>imprecise classification trees</keyword>
        <keyword>conjunctive random sets</keyword>
        <keyword>disjunctive random sets</keyword>
        <keyword>epistemic prediction</keyword>
        <keyword>german longitudinal election study 2013 (gles 2013)</keyword>
      </keywords>
      <abstract>In surveys, and most notably in election polls, undecided participants frequently constitute subgroups of their own with specific individual characteristics. While traditional survey methods and corresponding statistical models are inherently damned to neglect this valuable information, an ontic random set view provides us with the full power of the whole statistical modelling framework. We elaborate this idea for a multinomial logistic regression model (which can be derived as a discrete choice model for voting behaviour) and an imprecise classification tree, and apply them as a prototypic illustration to the German Longitudinal Election Study 2013. Our results corroborate the importance of a sophisticated, random set-based modelling. Furthermore, by reinterpreting the undecided respondents' answers as disjunctive random sets, general forecasts based on interval-valued point estimators are calculated.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/19.pdf</pdf>
    </paper>
    <paper>
      <id>034</id>
      <title>A logic with upper and lower probability operators</title>
      <authors>
        <author>
          <name>Nenad Savic</name>
          <email>nsavic@uns.ac.rs</email>
          <location>
            <country>
              <code>RS</code>
              <name>Serbia</name>
            </country>
            <city>
              <name>Novi Sad</name>
              <latitude>45.25167</latitude>
              <longitude>19.83694</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dragan Doder</name>
          <email>dragan.doder@uni.lu</email>
          <location>
            <country>
              <code>LU</code>
              <name>Luxembourg</name>
            </country>
            <city>
              <name>Luxembourg</name>
              <latitude>49.61167</latitude>
              <longitude>6.13000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Zoran Ognjanovic</name>
          <email>zorano@mi.sanu.ac.rs</email>
          <location>
            <country>
              <code>RS</code>
              <name>Serbia</name>
            </country>
            <city>
              <name>Belgrade</name>
              <latitude>44.80401</latitude>
              <longitude>20.46513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probabilistic logic</keyword>
        <keyword>upper and lower probability</keyword>
        <keyword>axiomatization</keyword>
        <keyword>completeness theorem</keyword>
      </keywords>
      <abstract>We present a propositional logic with unary operators that speak about upper and lower probabilities. We describe the corresponding class of models and discuss decidability issues. We provide an infinitary axiomatization for the logic and we prove that the axiomatization is sound and strongly complete. For some restrictions of the logic we provide finitary axiomatic systems.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/34.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>On the number and characterization of the extreme points of the core of necessity measures on finite spaces</title>
      <authors>
        <author>
          <name>Georg Schollmeyer</name>
          <email>georg.schollmeyer@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>necessity measure</keyword>
        <keyword>core</keyword>
        <keyword>extreme point</keyword>
        <keyword>enumeration</keyword>
        <keyword>belief function</keyword>
        <keyword>moebius inverse</keyword>
        <keyword>mass transfer</keyword>
        <keyword>possibility measure</keyword>
        <keyword>credal set</keyword>
        <keyword>focal set</keyword>
      </keywords>
      <abstract>This paper develops a combinatorial description of the extreme points of the core of a necessity measure on a finite space. We use the ingredients of Dempster-Shafer theory to characterize a necessity measure and the extreme points of its core in terms of the Moebius inverse, as well as an interpretation of the elements of the core as obtained through a transfer of probability mass from non-elementary events to singletons. With this understanding we derive an exact formula for the number of extreme points of the core of a necessity measure and obtain a constructive combinatorial insight into how the extreme points are obtained in terms of mass transfers. Our result sharpens the bounds for the number of extreme points given in [15] or [14, 13]. Furthermore, we determine the number of edges of the core of a necessity measure and additionally show how our results could be used to enumerate the extreme points of the core of arbitrary belief functions in a not too inefficient way.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/11.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Using imprecise continuous time Markov chains for assessing the reliability of power networks with common cause failure and non-immediate repair</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@gmail.com</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Simon Blake</name>
          <email>simon.blake@ncl.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Newcastle Upon Tyne</name>
              <latitude>54.97328</latitude>
              <longitude>-1.61396</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jacob Gledhill</name>
          <email>jacob.gledhill@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords/>
      <abstract>We explore how imprecise continuous time Markov chains can improve traditional reliability models based on precise continuous time Markov chains. Specifically, we analyse the reliability of power networks under very weak statistical assumptions, explicitly accounting for non-stationary failure and repair rates and the limited accuracy by which common cause failure rates can be estimated. Bounds on typical quantities of interest are derived, namely the expected time spent in system failure state, as well as the expected number of transitions to that state. A worked numerical example demonstrates the theoretical techniques described. Interestingly, the number of iterations required for convergence is observed to be much lower than current theoretical bounds.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/18.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>Classification SVM algorithms with interval-valued training data using triangular and Epanechnikov kernels</title>
      <authors>
        <author>
          <name>Lev Utkin</name>
          <email>lev.utkin@gmail.com</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Anatoly Chekh</name>
          <email>anatoly.chekh@gmail.com</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Yulia Zhuk</name>
          <email>zhuk_yua@mail.ru</email>
          <location>
            <country>
              <code>RU</code>
              <name>Russian Federation</name>
            </country>
            <city>
              <name>Saint Petersburg</name>
              <latitude>59.89444</latitude>
              <longitude>30.26417</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>classification</keyword>
        <keyword>support vector machine</keyword>
        <keyword>kernel</keyword>
        <keyword>interval-valued data</keyword>
        <keyword>minimax strategy</keyword>
        <keyword>linear programming</keyword>
        <keyword>quadratic programming</keyword>
        <keyword>extreme point</keyword>
      </keywords>
      <abstract>Classification algorithms based on different forms of support vector machines (SVMs) for dealing with interval-valued training data are proposed in the paper. L2 -norm and L___-norm SVMs are used for constructing the algorithms. The main idea allowing us to represent the complex optimization problems as a set of simple linear or quadratic programming problems is to approximate the Gaussian kernel by the well-known triangular and Epanechnikov kernels. The minimax strategy is used to choose an optimal probability distribution from the set and to construct optimal separating functions.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/3.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Modelling indifference with choice functions</title>
      <authors>
        <author>
          <name>Arthur van Camp</name>
          <email>arthur.vancamp@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>erik.quaeghebeur@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>choice function</keyword>
        <keyword>coherence</keyword>
        <keyword>indifference</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>maximality</keyword>
        <keyword>e-admissibility</keyword>
      </keywords>
      <abstract>We investigate how to model indifference with choice functions. We take the coherence axioms for choice functions proposed by Seidenfeld, Schervisch and Kadane as a source of inspiration, but modify them to strengthen the connection with desirability. We discuss the properties of choice functions that are coherent under our modified set of axioms and the connection with desirability. Once this is in place, we present an axiomatisation of indifference in terms of desirability. On this we build our characterisation of indifference in terms of choice functions.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/27.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>Credal compositional models and credal networks</title>
      <authors>
        <author>
          <name>Jirina Vejnarova</name>
          <email>vejnar@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal set</keyword>
        <keyword>strong independence</keyword>
        <keyword>credal network</keyword>
        <keyword>separate specification</keyword>
        <keyword>compositional models</keyword>
      </keywords>
      <abstract>This paper studies the composition operator for credal sets introduced at the last ISIPTA conference in more detail. Our main attention is devoted to the relationship between a special type of compositional model, so-called perfect sequences of credal sets, and those of (precise) probability distributions, with the goal of finding the relationship between credal compositional models and credal networks. We prove that a perfect sequence of credal sets is a convex hull of perfect sequences of extreme points of these credal sets. Finally, we reveal the relationship among credal networks (in a general sense), perfect sequences of credal sets and separately specified credal networks.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/24.pdf</pdf>
    </paper>
    <paper>
      <id>033</id>
      <title>On the validity of minimin and minimax methods for support vector regression with interval data</title>
      <authors>
        <author>
          <name>Andrea Wiencierz</name>
          <email>andrea.wiencierz@york.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>York</name>
              <latitude>53.95938</latitude>
              <longitude>-1.08142</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Cattaneo</name>
          <email>m.cattaneo@hull.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Kingston upon Hull</name>
              <latitude>53.74460</latitude>
              <longitude>-0.33525</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>support vector regression</keyword>
        <keyword>interval data</keyword>
        <keyword>representer theorem</keyword>
      </keywords>
      <abstract>In the recent years, generalizations of support vector methods for analyzing interval-valued data have been suggested in both the regression and classification contexts. Standard Support Vector methods for precise data formalize these statistical problems as optimization problems that can be based on various loss functions. In the case of Support Vector Regression (SVR), on which we focus here, the function that best describes the relationship between a response and some explanatory variables is derived as the solution of the minimization problem associated with the expectation of some function of the residual, which is called the risk functional. The key idea of SVR is that even when considering an infinite-dimensional space of arbitrary regression functions, given a finite-dimensional data set, the function minimizing the risk can be represented as the finite weighted sum of kernel functions. This allows to practically determine the SVR estimate by solving a much simpler optimization problem, even in the case of nonlinear regression. In case that only interval-valued observations of the variables of interest are available, it has been suggested to minimize the minimal or maximal risk values that are compatible with the imprecise data, yielding precise SVR estimates on the basis of interval data. In this paper, we show that also in the case of an interval-valued response the optimal function can be represented as the finite weighted sum of kernel functions. Thus, the minimin and minimax SVR estimates can be obtained by minimizing the corresponding simplified expressions of the empirical lower and upper risks, respectively.</abstract>
      <pdf>http://www.sipta.org/isipta15/data/paper/33.pdf</pdf>
    </paper>
  </proceedings>
  <proceedings>
    <year>2017</year>
    <conference>
      <date>
        <start>2017-07-10</start>
        <end>2017-07-14</end>
      </date>
      <location>
        <country>
          <code>CH</code>
          <name>Switzerland</name>
        </country>
        <city>
          <name>Lugano</name>
          <latitude>46.01008</latitude>
          <longitude>8.96004</longitude>
        </city>
        <university>
          <name>University of Lugano</name>
          <department/>
        </university>
      </location>
    </conference>
    <paper>
      <id>001</id>
      <title>Differences of Opinion</title>
      <authors>
        <author>
          <name>Dionissi Aliprantis</name>
          <email>dionissi.aliprantis@clev.frb.org</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Cleveland</name>
              <latitude>41.49950</latitude>
              <longitude>-81.69541</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>belief formation</keyword>
        <keyword>subjective probability</keyword>
        <keyword>social learning</keyword>
        <keyword>partial identification</keyword>
        <keyword>causal inference</keyword>
        <keyword>degroot learning rule</keyword>
        <keyword>bounded confidence</keyword>
      </keywords>
      <abstract>This paper considers the resolution of ambiguity according to the scientific ideal of direct observation when there is a practical necessity for social learning. An agent faces ambiguity when she directly observes low-quality data yielding set-identified signals. I suppose the agent's objective is to choose the single belief replicating what would occur with high-quality data yielding point-identified signals. I allow the agent to solve this missing data problem using signals observed through her network in combination with a model of social learning. In some cases the agent's belief formation reduces to DeGroot updating and beliefs in a network reach a consensus. In other cases the agent's updating can generate polarization and sustain clustered disagreement, even on a connected network where everyone observes the same data and processes that data with the same model.</abstract>
      <pdf>http://proceedings.mlr.press/v62/aliprantis17a/aliprantis17a.pdf</pdf>
    </paper>
    <paper>
      <id>002</id>
      <title>Kurt Weichselberger's Contribution to Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Rudolf Seising</name>
          <email>r.seising@lrz.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>weichselberger</keyword>
        <keyword>kurt</keyword>
        <keyword>interval probability</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>logical probability</keyword>
        <keyword>symmetric theory</keyword>
        <keyword>history of probability and statistics</keyword>
      </keywords>
      <abstract>Kurt Weichselberger, one of the influential senior members of the imprecise probability community, passed away on February 7, 2016. Almost throughout his entire academic life the major focus of his research interest has been on the foundations of probability and statistics. The present article is a first attempt to trace back chronologically the development of Weichselberger's work on interval probability and his symmetric theory of logical probability based on it. We also try to work out the intellectual background of his different projects together with some close links between them.</abstract>
      <pdf>http://proceedings.mlr.press/v62/augustin17a/augustin17a.pdf</pdf>
    </paper>
    <paper>
      <id>003</id>
      <title>SOS for Bounded Rationality</title>
      <authors>
        <author>
          <name>Alessio Benavoli</name>
          <email>alessio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessandro Facchini</name>
          <email>alessandro.facchini@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dario Piga</name>
          <email>dario@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>bounded rationality</keyword>
        <keyword>polynomial gambles</keyword>
        <keyword>sum-of-squares</keyword>
      </keywords>
      <abstract>In the gambling foundation of probability theory, rationality requires that a subject should always (never) find desirable all nonnegative (negative) gambles, because no matter the result of the experiment the subject never (always) decreases her money. Evaluating the nonnegativity of a gamble in infinite spaces is a difficult task. In fact, even if we restrict the gambles to be polynomials in Rn, the problem of determining nonnegativity is NP-hard. The aim of this paper is to develop a computable theory of desirable gambles. Instead of requiring the subject to accept all nonnegative gambles, we only require her to accept gambles for which she can efficiently determine the nonnegativity (in particular SOS polynomials). We call this new criterion bounded rationality.</abstract>
      <pdf>http://proceedings.mlr.press/v62/benavoli17a/benavoli17a.pdf</pdf>
    </paper>
    <paper>
      <id>004</id>
      <title>A Polarity Theory for Sets of Desirable Gambles</title>
      <authors>
        <author>
          <name>Alessio Benavoli</name>
          <email>alessio@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alessandro Facchini</name>
          <email>alessandro.facchini@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Marco Zaffalon</name>
          <email>zaffalon@idsia.ch</email>
          <location>
            <country>
              <code>CH</code>
              <name>Switzerland</name>
            </country>
            <city>
              <name>Manno</name>
              <latitude>46.03072</latitude>
              <longitude>8.91937</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Jose Vicente-Perez</name>
          <email>jose.vicente@ua.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Alicante</name>
              <latitude>38.34517</latitude>
              <longitude>-0.48149</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>desirability</keyword>
        <keyword>credal set</keyword>
        <keyword>lexicographic probability</keyword>
        <keyword>separation theorem</keyword>
        <keyword>polarity</keyword>
      </keywords>
      <abstract>Coherent sets of almost desirable gambles and credal sets are known to be equivalent models. That is, there exists a bijection between the two collections of sets preserving the usual operations, e.g. conditioning. Such a correspondence is based on the polarity theory for closed convex cones. Learning from this simple observation, in this paper we introduce a new (lexicographic) polarity theory for general convex cones and then we apply it in order to establish an analogous correspondence between coherent sets of desirable gambles and convex sets of lexicographic probabilities.</abstract>
      <pdf>http://proceedings.mlr.press/v62/benavoli17b/benavoli17b.pdf</pdf>
    </paper>
    <paper>
      <id>005</id>
      <title>Modeling Markov Decision Processes with Imprecise Probabilities Using Probabilistic Logic Programming</title>
      <authors>
        <author>
          <name>Thiago Bueno</name>
          <email>tbueno@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Denis Maua</name>
          <email>ddm@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Leliane de Barros</name>
          <email>leliane@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>markov decision process</keyword>
        <keyword>mdp</keyword>
        <keyword>mdpip</keyword>
        <keyword>mdpst</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>non-determinism</keyword>
        <keyword>probabilistic logic program</keyword>
        <keyword>credal semantics</keyword>
      </keywords>
      <abstract>We study languages that specify Markov Decision Processes with Imprecise Probabilities (MDPIPs) by mixing probabilities and logic programming. We propose a novel language that can capture MDPIPs and Markov Decision Processes with Set-valued Transitions (MDPSTs) we then obtain the complexity of one-step inference for the resulting MDPIPs and MDPSTs. We also present results of independent interest on the complexity of inference with probabilistic logic programs containing interval-valued probabilistic assessments. Finally, we also discuss policy generation techniques.</abstract>
      <pdf>http://proceedings.mlr.press/v62/bueno17a/bueno17a.pdf</pdf>
    </paper>
    <paper>
      <id>006</id>
      <title>Empirical Interpretation of Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Marco Cattaneo</name>
          <email>m.cattaneo@hull.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Kingston upon Hull</name>
              <latitude>53.74460</latitude>
              <longitude>-0.33525</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise probability</keyword>
        <keyword>frequentist interpretation</keyword>
        <keyword>empirical meaning</keyword>
        <keyword>bag of marbles</keyword>
        <keyword>strong estimability</keyword>
        <keyword>consistent estimators</keyword>
        <keyword>empirical recognizability</keyword>
      </keywords>
      <abstract>This paper investigates the possibility of a frequentist interpretation of imprecise probabilities, by generalizing the approach of Bernoulli's Ars Conjectandi. That is, by studying, in the case of games of chance, under which assumptions imprecise probabilities can be satisfactorily estimated from data. In fact, estimability on the basis of finite amounts of data is a necessary condition for imprecise probabilities in order to have a clear empirical meaning. Unfortunately, imprecise probabilities can be estimated arbitrarily well from data only in very limited settings.</abstract>
      <pdf>http://proceedings.mlr.press/v62/cattaneo17a/cattaneo17a.pdf</pdf>
    </paper>
    <paper>
      <id>007</id>
      <title>Bayesian Inference under Ambiguity: Conditional Prior Belief Functions</title>
      <authors>
        <author>
          <name>Giulianella Coletti</name>
          <email>giulianella.coletti@unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Davide Petturiti</name>
          <email>davide.petturiti@unipg.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Perugia</name>
              <latitude>43.11220</latitude>
              <longitude>12.38878</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Barbara Vantaggi</name>
          <email>barbara.vantaggi@sbai.uniroma1.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Rome</name>
              <latitude>41.89474</latitude>
              <longitude>12.48390</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>conditional belief function</keyword>
        <keyword>bayesian conditioning rule</keyword>
        <keyword>inference</keyword>
        <keyword>ambiguity</keyword>
      </keywords>
      <abstract>Bayesian inference under imprecise prior information is studied: the starting point is a precise strategy $__$ and a full B-conditional prior belief function $Bel_B$, conveying ambiguity in probabilistic prior information. In finite spaces, we give a closed form expression for the lower envelope $\underline{P}$ of the class of full conditional probabilities dominating $(Bel_B,__)$ and, in particular, for the related {"}posterior probabilities{"}. The assessment $(Bel_B,__)$ is a coherent lower conditional probability in the sense of Williams and the characterized lower envelope $\underline{P}$ coincides with its natural extension.</abstract>
      <pdf>http://proceedings.mlr.press/v62/coletti17a/coletti17a.pdf</pdf>
    </paper>
    <paper>
      <id>008</id>
      <title>Weak Dutch Books versus Strict Consistency with Lower Previsions</title>
      <authors>
        <author>
          <name>Chiara Corsato</name>
          <email>ccorsato@units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Renato Pelessoni</name>
          <email>renato.pelessoni@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paolo Vicig</name>
          <email>paolo.vicig@econ.units.it</email>
          <location>
            <country>
              <code>IT</code>
              <name>Italy</name>
            </country>
            <city>
              <name>Trieste</name>
              <latitude>45.64861</latitude>
              <longitude>13.78000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>weak dutch books</keyword>
        <keyword>williams coherence</keyword>
        <keyword>convex prevision</keyword>
        <keyword>strict consistency</keyword>
      </keywords>
      <abstract>Several consistency notions for lower previsions (coherence, convexity, others) require that the suprema of certain gambles, having the meaning of gains, are non-negative. The limit situation that a gain supremum is zero is termed Weak Dutch Book (WDB). In the literature, the special case of WDBs with precise probabilities has mostly been analysed, and strict coherence has been proposed as a radical alternative. In this paper the focus is on WDBs and generalised strict coherence, termed strict consistency, with imprecise previsions. We discuss properties of lower previsions incurring WDBs and conditions for strict consistency, showing in both cases how they are differentiated by the degree of consistency of the given uncertainty assessment.</abstract>
      <pdf>http://proceedings.mlr.press/v62/corsato17a/corsato17a.pdf</pdf>
    </paper>
    <paper>
      <id>009</id>
      <title>Reconciling Bayesian and Frequentist Tests: the Imprecise Counterpart</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Antonio Alvarez-Caballero</name>
          <email>analca3@gmail.com</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Luciano Sanchez</name>
          <email>luciano@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>wilcoxon rank sum test</keyword>
        <keyword>imprecise tests</keyword>
        <keyword>one-sided test</keyword>
        <keyword>frequentist test</keyword>
        <keyword>bayesian test</keyword>
        <keyword>idp test</keyword>
        <keyword>interval p-values</keyword>
      </keywords>
      <abstract>Imprecise Dirichlet Process-based tests (IDP-tests, for short) have been recently introduced in the literature. They overcome the problem of deciding how to select a single prior in Bayesian hypothesis testing, in the absence of prior information. They make use of a {"}near-ignorance{"} model, that behaves a priori as a vacuous model for some basic inferences, but it provides non-vacuous posterior inferences. We perform empirical studies regarding the behavior of IDP-tests for the particular case of Wilcoxon rank sum test. We show that the upper and lower posterior probabilities can be expressed as tail probabilities based on the value of the $U$ statistic. We construct an imprecise frequentist-based test that reproduces the same decision rule as the the IDP test. It considers a neighbourhood around the $U$-statistic value. If all the values in the neighbourhood belong to the rejection zone (resp. to the acceptance region), the null hypothesis is rejected (resp. accepted). Otherwise, the judgement is suspended.</abstract>
      <pdf>http://proceedings.mlr.press/v62/couso17a/couso17a.pdf</pdf>
    </paper>
    <paper>
      <id>010</id>
      <title>Evenly Convex Credal Sets</title>
      <authors>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>credal set</keyword>
        <keyword>sets of probability measures</keyword>
        <keyword>preference axioms</keyword>
        <keyword>convexity</keyword>
      </keywords>
      <abstract>An evenly convex credal set is a set of probability measures that is evenly convex that is, a set that is an intersection of open halfspaces. An evenly convex credal set can for instance encode preference judgments through strict and non-strict inequalities such as $P(A) &gt; 1/2$ and $P(A) ___2/3$. This paper presents an axiomatization of evenly convex sets from preferences, where we introduce a new (and very weak) Archimedean condition.</abstract>
      <pdf>http://proceedings.mlr.press/v62/cozman17a/cozman17a.pdf</pdf>
    </paper>
    <paper>
      <id>011</id>
      <title>Independent Natural Extension for Infinite Spaces: Williams-Coherence to the Rescue</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>independent natural extension</keyword>
        <keyword>epistemic independence</keyword>
        <keyword>williams coherence</keyword>
        <keyword>infinite spaces</keyword>
        <keyword>external additivity</keyword>
        <keyword>factorization</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>conditional lower previsions</keyword>
      </keywords>
      <abstract>We define the independent natural extension of two local models for the general case of infinite spaces, using both sets of desirable gambles and conditional lower previsions. In contrast to Miranda and Zaffalon (2015), we adopt Williams-coherence instead of Walley-coherence. We show that our notion of independent natural extension always exists___whereas theirs does not___and that it satisfies various convenient properties, including factorisation and external additivity.</abstract>
      <pdf>http://proceedings.mlr.press/v62/de bock17a/de bock17a.pdf</pdf>
    </paper>
    <paper>
      <id>012</id>
      <title>Computable Randomness is Inherently Imprecise</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>computable randomness</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>game-theoretic probability</keyword>
        <keyword>interval forecast</keyword>
        <keyword>supermartingale</keyword>
        <keyword>computability</keyword>
      </keywords>
      <abstract>We use the martingale-theoretic approach of game-theoretic probability to incorporate imprecision into the study of randomness. In particular, we define a notion of computable randomness associated with interval, rather than precise, forecasting systems, and study its properties. The richer mathematical structure that thus arises lets us better understand and place existing results for the precise limit. When we focus on constant interval forecasts, we find that every infinite sequence of zeroes and ones has an associated filter of intervals with respect to which it is computably random. It may happen that none of these intervals is precise, which justifies the title of this paper. We illustrate this by showing that computable randomness associated with non-stationary precise forecasting systems can be captured by a stationary interval forecast, which must then be less precise: a gain in model simplicity is thus paid for by a loss in precision.</abstract>
      <pdf>http://proceedings.mlr.press/v62/cooman17a/cooman17a.pdf</pdf>
    </paper>
    <paper>
      <id>013</id>
      <title>Imprecise Continuous-Time Markov Chains: Efficient Computational Methods with Guaranteed Error Bounds</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Alexander Erreygers</name>
          <email>alexander.erreygers@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>imprecise continuous-time markov chain</keyword>
        <keyword>lower transition operator</keyword>
        <keyword>lower transition rate operator</keyword>
        <keyword>approximation method</keyword>
        <keyword>ergodicity</keyword>
        <keyword>coefficient of ergodicity</keyword>
      </keywords>
      <abstract>Imprecise continuous-time Markov chains are a robust type of continuous-time Markov chains that allow for partially specified time-dependent parameters. Computing inferences for them requires the solution of a non-linear differential equation. As there is no general analytical expression for this solution, efficient numerical approximation methods are essential to the applicability of this model. We here improve the uniform approximation method of Krak et al. (2016) in two ways and propose a novel and more efficient adaptive approximation method. For ergodic chains, we also provide a method that allows us to approximate stationary distributions up to any desired maximal error.</abstract>
      <pdf>http://proceedings.mlr.press/v62/erreygers17a/erreygers17a.pdf</pdf>
    </paper>
    <paper>
      <id>014</id>
      <title>(Generalized) Linear Regression on Microaggregated Data - From Nuisance Parameter Optimization to Partial Identification</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Paul Fink</name>
          <email>paul.fink@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>maximum likelihood estimation</keyword>
        <keyword>generalized linear regression</keyword>
        <keyword>microaggregation</keyword>
        <keyword>anonymization</keyword>
        <keyword>partial identification</keyword>
      </keywords>
      <abstract>Protecting sensitive micro data prior to publishing or passing the data itself on is a crucial aspect: A trade-off between sufficient disclosure control and analyzability needs to be found. This paper presents a starting point to evaluate the effect of $k$-anonymity microaggregated data in (generalized) linear regression. Taking a rigorous imprecision perspective, microaggregated data are understood inducing a set $X$ of potentially true data. Based on this representation two conceptually different approaches deriving estimations from the ideal likelihood are discussed. The first one picks a single element of $X$, for instance by naively treating the microaggregated data as true ones or by introducing a maximax approach taking the elements of $X$ as nuisance parameters to be optimized. The second one seeks, in the spirit of Partial Identification, the set of all maximum likelihood estimators compatible with the elements of $X$, thus creating cautious estimators. As the simulation study corroborates, the obtained sets of estimators of the latter approach are still precise enough to be practically relevant.</abstract>
      <pdf>http://proceedings.mlr.press/v62/fink17a/fink17a.pdf</pdf>
    </paper>
    <paper>
      <id>015</id>
      <title>Maximum Likelihood with Coarse Data based on Robust Optimisation</title>
      <authors>
        <author>
          <name>Ines Couso</name>
          <email>couso@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Romain Guillaume</name>
          <email>Romain.Guillaume@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Didier Dubois</name>
          <email>dubois@irit.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Toulouse</name>
              <latitude>43.60426</latitude>
              <longitude>1.44367</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>maximum likelihood</keyword>
        <keyword>incomplete information</keyword>
        <keyword>robust optimization</keyword>
        <keyword>entropy</keyword>
      </keywords>
      <abstract>This paper deals with the problem of probability estimation in the context of coarse data. Probabilities are estimated using the maximum likelihood principle. Our approach presupposes that each imprecise observation underlies a precise one, and that the uncertainty that pervades its observation is epistemic, rather than representing noise. As a consequence, the likelihood function of the ill-observed sample is set-valued. In this paper, we apply a robust optimization method to find a safe plausible estimate of the probabilities of elementary events on finite state spaces. More precisely we use a maximin criterion on the imprecise likelihood function. We show that there is a close connection between the robust maximum likelihood strategy and the maximization of entropy among empirical distributions compatible with the incomplete data. A mathematical model in terms of maximal flow on graphs, based on duality theory, is proposed. It results in a linear objective function and convex constraints. This result is somewhat surprizing since maximum entropy problems are known to be complex due to the maximization of a concave function on a convex set.</abstract>
      <pdf>http://proceedings.mlr.press/v62/guillaume17a/guillaume17a.pdf</pdf>
    </paper>
    <paper>
      <id>016</id>
      <title>Concepts for Decision Making under Severe Uncertainty with Partial Ordinal and Partial Cardinal Preferences</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Christoph Jansen</name>
          <email>christoph.jansen@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Georg Schollmeyer</name>
          <email>georg.schollmeyer@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>partial preference</keyword>
        <keyword>ordinality</keyword>
        <keyword>cardinality</keyword>
        <keyword>decision making under uncertainty</keyword>
        <keyword>linear programming</keyword>
        <keyword>decision criterion</keyword>
        <keyword>stochastic dominance</keyword>
        <keyword>utility representation</keyword>
        <keyword>admissibility</keyword>
      </keywords>
      <abstract>We introduce three different approaches for decision making under uncertainty, if (I) there is only partial (both cardinal and ordinal) information on an agent's preferences and (II) the uncertainty about the states of nature is described by a credal set. Particularly, (I) is modeled by a pair of relations, one specifying the partial rank order of the alternatives and the other modeling partial information on the strength of preference. Our first approach relies on criteria that construct complete rankings of the acts based on generalized expectation intervals. Subsequently, we introduce different concepts of global admissibility that construct partial orders by comparing all acts simultaneously. Finally, we define criteria induced by suitable binary relations on the set of acts and, therefore, can be understood as concepts of local admissibility. Whenever suitable, we provide linear programming based algorithms for checking optimality/admissibility of acts.</abstract>
      <pdf>http://proceedings.mlr.press/v62/jansen17a/jansen17a.pdf</pdf>
    </paper>
    <paper>
      <id>017</id>
      <title>Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains</title>
      <authors>
        <author>
          <name>Jasper de Bock</name>
          <email>jasper.debock@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Thomas Krak</name>
          <email>t.e.krak@uu.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Utrecht</name>
              <latitude>52.09083</latitude>
              <longitude>5.12222</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Arno Siebes</name>
          <email>a.p.j.m.siebes@uu.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Utrecht</name>
              <latitude>52.09083</latitude>
              <longitude>5.12222</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>continuous-time hidden markov chains</keyword>
        <keyword>imprecise probability</keyword>
        <keyword>updating</keyword>
      </keywords>
      <abstract>We consider the problem of performing inference with imprecise continuous-time hidden Markov chains, that is, imprecise continuous-time Markov chains that are augmented with random output variables whose distribution depends on the hidden state of the chain. The prefix ___imprecise' refers to the fact that we do not consider a classical continuous-time Markov chain, but replace it with a robust extension that allows us to represent various types of model uncertainty, using the theory of imprecise probabilities. The inference problem amounts to computing lower expectations of functions on the state-space of the chain, given observations of the output variables. We develop and investigate this problem with very few assumptions on the output variables in particular, they can be chosen to be either discrete or continuous random variables. Our main result is a polynomial runtime algorithm to compute the lower expectation of functions on the state-space at any given time-point, given a collection of observations of the output variables.</abstract>
      <pdf>http://proceedings.mlr.press/v62/krak17a/krak17a.pdf</pdf>
    </paper>
    <paper>
      <id>018</id>
      <title>Credal Sum-Product Networks</title>
      <authors>
        <author>
          <name>Denis Maua</name>
          <email>ddm@ime.usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Fabio Cozman</name>
          <email>fgcozman@usp.br</email>
          <location>
            <country>
              <code>BR</code>
              <name>Brazil</name>
            </country>
            <city>
              <name>Sao Paulo</name>
              <latitude>-23.54750</latitude>
              <longitude>-46.63611</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Diarmaid Conaty</name>
          <email>dconaty01@qub.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Belfast</name>
              <latitude>54.58333</latitude>
              <longitude>-5.93333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Cassio Campos</name>
          <email>c.decampos@qub.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Belfast</name>
              <latitude>54.58333</latitude>
              <longitude>-5.93333</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>sum-product networks</keyword>
        <keyword>tractable probabilistic models</keyword>
        <keyword>credal classification</keyword>
      </keywords>
      <abstract>Sum-product networks are a relatively new and increasingly popular class of (precise) probabilistic graphical models that allow for marginal inference with polynomial effort. As with other probabilistic models, sum-product networks are often learned from data and used to perform classification. Hence, their results are prone to be unreliable and overconfident. In this work, we develop credal sum-product networks, an imprecise extension of sum-product networks. We present algorithms and complexity results for common inference tasks. We apply our algorithms on realistic classification task using images of digits and show that credal sum-product networks obtained by a perturbation of the parameters of learned sum-product networks are able to distinguish between reliable and unreliable classifications with high accuracy.</abstract>
      <pdf>http://proceedings.mlr.press/v62/mau%C3%A11717a/mau%C3%A11717a.pdf</pdf>
    </paper>
    <paper>
      <id>019</id>
      <title>Game Solutions, Probability Transformations and the Core</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ignacio Montes</name>
          <email>imontes@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>game solutions</keyword>
        <keyword>probability transformations</keyword>
        <keyword>lower probabilities</keyword>
        <keyword>belief function</keyword>
        <keyword>core</keyword>
        <keyword>shapley value</keyword>
        <keyword>banzhaf value</keyword>
        <keyword>pignistic transformation</keyword>
      </keywords>
      <abstract>We investigate the role of some game solutions, such the Shapley and the Banzhaf values, as probability transformations of lower probabilities. The first one coincides with the pignistic transformation proposed in the Transferable Belief Model the second one is not efficient in general, leading us to propose a normalized version. We consider a number of particular cases of lower probabilities: minitive measures, coherent lower probabilities, as well as the lower probabilities induced by comparative or distorsion models. For them, we provide some alternative expressions of the transformations and study when they belong to the core of the lower probability.</abstract>
      <pdf>http://proceedings.mlr.press/v62/miranda17a/miranda17a.pdf</pdf>
    </paper>
    <paper>
      <id>020</id>
      <title>A Study of the Pari-Mutuel Model from the Point of View of Imprecise Probabilities</title>
      <authors>
        <author>
          <name>Enrique Miranda</name>
          <email>mirandaenrique@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ignacio Montes</name>
          <email>imontes@uniovi.es</email>
          <location>
            <country>
              <code>ES</code>
              <name>Spain</name>
            </country>
            <city>
              <name>Oviedo</name>
              <latitude>43.36029</latitude>
              <longitude>-5.84476</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Sebastien Destercke</name>
          <email>sebastien.destercke@hds.utc.fr</email>
          <location>
            <country>
              <code>FR</code>
              <name>France</name>
            </country>
            <city>
              <name>Compiegne</name>
              <latitude>49.41794</latitude>
              <longitude>2.82606</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>pari-mutuel bets</keyword>
        <keyword>credal set</keyword>
        <keyword>probability intervals</keyword>
        <keyword>belief function</keyword>
        <keyword>information fusion</keyword>
      </keywords>
      <abstract>The Pari-Mutuel model is a distortion model that has its origin in horse racing. Since then, it has been applied in many different fields, such as finance or risk analysis. In this paper we investigate the properties of the Pari-Mutuel model within the framework of Imprecise Probabilities. Since a Pari-Mutuel model induces (2-monotone) coherent lower and upper probabilities, we investigate its connections with other relevant models within this theory, such as probability intervals and belief functions. We also determine the number of extreme points of the credal set induced by the Pari-Mutuel model and study how to combine the information given by multiple Pari-Mutuel models.</abstract>
      <pdf>http://proceedings.mlr.press/v62/montes17a/montes17a.pdf</pdf>
    </paper>
    <paper>
      <id>021</id>
      <title>Efficient Algorithms for Checking Avoiding Sure Loss</title>
      <authors>
        <author>
          <name>Nawapon Nakharutai</name>
          <email>nawapon.nakharutai@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Camila Caiado</name>
          <email>c.c.d.s.caiado@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>avoiding sure loss</keyword>
        <keyword>linear programming</keyword>
        <keyword>benchmarking</keyword>
        <keyword>simplex method</keyword>
        <keyword>affine scaling method</keyword>
        <keyword>primal-dual method</keyword>
        <keyword>algorithm</keyword>
      </keywords>
      <abstract>Sets of desirable gambles provide a general representation of uncertainty which can handle partial information in a more robust way than precise probabilities. Here we study the effectiveness of linear programming algorithms for determining whether or not a given set of desirable gambles avoids sure loss (i.e. is consistent). We also suggest improvements to these algorithms specifically for checking avoiding sure loss. By exploiting the structure of the problem, (i) we slightly reduce its dimension, (ii) we propose an extra stopping criterion based on its degenerate structure, and (iii) we show that one can directly calculate feasible starting points in various cases, therefore reducing the effort required in the presolve phase of some of these algorithms. To assess our results, we compare the impact of these improvements on the simplex method and two interior point methods (affine scaling and primal-dual) on randomly generated sets of desirable gambles that either avoid or do not avoid sure loss. We find that the simplex method is outperformed by the primal-dual and affine scaling methods, except for very small problems. We also find that using our starting feasible point and extra stopping criterion considerably improves the performance of the primal-dual and affine scaling methods.</abstract>
      <pdf>http://proceedings.mlr.press/v62/nakharutai17a/nakharutai17a.pdf</pdf>
    </paper>
    <paper>
      <id>022</id>
      <title>Towards a Cautious Modelling of Missing Data in Small Area Estimation</title>
      <authors>
        <author>
          <name>Thomas Augustin</name>
          <email>augustin@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Julia Plass</name>
          <email>julia.plass@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Aziz Omar</name>
          <email>aziz.omar@stat.uni-muenchen.de</email>
          <location>
            <country>
              <code>DE</code>
              <name>Germany</name>
            </country>
            <city>
              <name>Munich</name>
              <latitude>48.13743</latitude>
              <longitude>11.57549</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>small area estimation</keyword>
        <keyword>lgreg-synthetic estimator</keyword>
        <keyword>missing data</keyword>
        <keyword>partial identification</keyword>
        <keyword>sensitivity analysis</keyword>
        <keyword>likelihood</keyword>
        <keyword>logistic regression</keyword>
        <keyword>logistic mixed model</keyword>
        <keyword>german general social survey</keyword>
      </keywords>
      <abstract>In official statistics, the problem of sampling error is rushed to extremes when not only results on sub-population level are required, which is the focus of Small Area Estimation (SAE), but also missing data arise. When the nonresponse is wrongly assumed to occur at random, the situation becomes even more dramatic, since this potentially leads to a substantial bias. Even though there are some treatments jointly considering both problems, they are all reliant upon the guarantee of strong assumptions on the missingness. For that reason, we aim at developing cautious versions of well known estimators from SAE by exploiting the results from a recently suggested likelihood approach, capable of including tenable partial knowledge about the nonresponse behaviour in an adequate way. We generalize the synthetic estimator and propose a cautious version of the so-called LGREG-synthetic estimator in the context of design-based estimators. Then, we elaborate why the approach above does not directly extend to model-based estimators and proceed with some first studies investigating different missingness scenarios. All results are illustrated through the German General Social Survey 2014, also including area-specific auxiliary information from the German Federal Statistical Office's data report.</abstract>
      <pdf>http://proceedings.mlr.press/v62/plass17a/plass17a.pdf</pdf>
    </paper>
    <paper>
      <id>023</id>
      <title>Efficient Computation of Belief Theoretic Conditionals</title>
      <authors>
        <author>
          <name>Lalintha Polpitiya</name>
          <email>lalintha@umiami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Kamal Premaratne</name>
          <email>kamal@miami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Manohar Murthi</name>
          <email>mmurthi@miami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Dilip Sarkar</name>
          <email>sarkar@miami.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Coral Gables</name>
              <latitude>25.75333</latitude>
              <longitude>-80.27038</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>dempster-shafer's belief function theory</keyword>
        <keyword>dempster's conditional</keyword>
        <keyword>fagin-halpern conditional</keyword>
        <keyword>data structures</keyword>
        <keyword>algorithm</keyword>
        <keyword>computational complexity</keyword>
      </keywords>
      <abstract>Dempster-Shafer (DS) belief theory is a powerful general framework for dealing with a wider variety of uncertainties in data. As in Bayesian probability theory, the conditional operation plays a critical role in DS theoretic strategies for evidence updating and fusion. A major limitation associated with the application of DS theoretic techniques for reasoning under uncertainty is the absence of a feasible computational framework to overcome the prohibitive computational burden this conditional operation entails. This paper addresses this critical challenge via a novel generalized conditional computational model ___ DS-Conditional-One ___ which allows the conditional to be computed in significantly less computational and space complexity. This computational model also provides valuable insight into the DS theoretic conditional itself and can be utilized as a tool for visualizing the conditional computation. We provide a thorough analysis and experimental validation of the utility, efficiency, and implementation of the proposed data structures and algorithms for carrying out both the Dempster's conditional and Fagin-Halpern conditional, the two most widely utilized DS theoretic conditional strategies.</abstract>
      <pdf>http://proceedings.mlr.press/v62/polpitiya17a/polpitiya17a.pdf</pdf>
    </paper>
    <paper>
      <id>024</id>
      <title>The CWI World Cup Competition: Eliciting Sets of Acceptable Gambles</title>
      <authors>
        <author>
          <name>Erik Quaeghebeur</name>
          <email>E.R.G.Quaeghebeur@tudelft.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Delft</name>
              <latitude>52.00667</latitude>
              <longitude>4.35556</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Chris Wesseling</name>
          <email>Chris.Wesseling@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Emma Beauxis-Aussalet</name>
          <email>Emmanuelle.Beauxis-Aussalet@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Teresa Piovesan</name>
          <email>T.Piovesan@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Tom Sterkenburg</name>
          <email>Tom@cwi.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>elicitation</keyword>
        <keyword>gamble</keyword>
        <keyword>acceptability</keyword>
        <keyword>desirability</keyword>
        <keyword>user interface</keyword>
        <keyword>experiment</keyword>
        <keyword>fair bet</keyword>
      </keywords>
      <abstract>We present an interface for eliciting sets of acceptable gambles on a three-outcome possibility space, discuss an experiment conducted for testing this interface, and present the results of this experiment. Sets of acceptable gambles form a representation for imprecise probabilities that is close to human behavior and eliciting them directly may improve the quality of the resulting uncertainty model. The experiment consisted of a betting competition for the 2014 FIFA World Cup: For each match bets were assigned based on the sets of acceptable gambles elicited from the participants. A new algorithm was designed for generating fair bets for assignment. Participant feedback indicated that improving the usability and transparency of the interface would ease the elicitation procedure. The experiment's results underlined that imprecision is an essential aspect of real-life uncertainty modeling.</abstract>
      <pdf>http://proceedings.mlr.press/v62/quaeghebeur17a/quaeghebeur17a.pdf</pdf>
    </paper>
    <paper>
      <id>025</id>
      <title>Errors Bounds for Finite Approximations of Coherent Lower Previsions</title>
      <authors>
        <author>
          <name>Damjan Skulj</name>
          <email>damjan.skulj@fdv.uni-lj.si</email>
          <location>
            <country>
              <code>SI</code>
              <name>Slovenia</name>
            </country>
            <city>
              <name>Ljubljana</name>
              <latitude>46.05108</latitude>
              <longitude>14.50513</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>lower prevision</keyword>
        <keyword>partially specified lower prevision</keyword>
        <keyword>credal set</keyword>
        <keyword>convex polyhedron</keyword>
        <keyword>quadratic programming</keyword>
      </keywords>
      <abstract>Coherent lower previsions are general probabilistic models allowing incompletely specified probability distributions. However, for complete description of a coherent lower prevision ___ even on finite underlying sample spaces ___ an infinite number of assessments is needed in general. Therefore, they are often only described approximately by some less general models, such as coherent lower probabilities or in terms of some other finite set of constraints. The magnitude of error induced by the approximations has often been neglected in the literature, despite the fact that it can be significant with substantial impact on consequent decisions. An apparent reason is that no widely used general method for estimating the error seems to be available at the moment. The goal of this paper is to provide such a method. The proposed method allows calculating an upper bound for the error of a finite approximation of coherent lower prevision on a finite underlying sample space. An estimate of the maximal error is especially useful in the cases where calculating assessments is computationally demanding. Our method is based on convex analysis applied to credal sets, which in the case of finite sample spaces correspond to convex polyhedra.</abstract>
      <pdf>http://proceedings.mlr.press/v62/%C5%A0kulj17a/%C5%A0kulj17a.pdf</pdf>
    </paper>
    <paper>
      <id>026</id>
      <title>New Distributions for Modeling Subjective Lower and Upper Probabilities</title>
      <authors>
        <author>
          <name>Michael Smithson</name>
          <email>Michael.Smithson@anu.edu.au</email>
          <location>
            <country>
              <code>AU</code>
              <name>Australia</name>
            </country>
            <city>
              <name>Canberra</name>
              <latitude>-35.28346</latitude>
              <longitude>149.12807</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>probability judgment</keyword>
        <keyword>distribution</keyword>
        <keyword>quantile regression</keyword>
        <keyword>generalized linear model</keyword>
      </keywords>
      <abstract>This paper presents an investigation of approaches to modeling lower and upper subjective probabilities. A relatively unexplored approach is introduced, based on the fact that every cumulative distribution function (CDF) with support (0,1) has a {"}dual{"} CDF that obeys the conjugacy relation between coherent lower and upper probabilities. A new 2-parameter family of {"}CDF-Quantile{"} distributions with support (0,1) is extended via a third parameter for the purpose of modeling lower-upper probabilities. The extension exploits certain properties of the CDF-Quantile family, and the fact that continuous CDFs on (0,1) random variables form an algebraic group that is closed under composition. This extension also yields models for testing specific models of lower-upper probability assignments. Finally, the new models are applied to a real data-set, and compared with the alternative approaches for their relative advantages and drawbacks.</abstract>
      <pdf>http://proceedings.mlr.press/v62/smithson17a/smithson17a.pdf</pdf>
    </paper>
    <paper>
      <id>027</id>
      <title>Linear Core-Based Criterion for Testing Extreme Exact Games</title>
      <authors>
        <author>
          <name>Milan Studeny</name>
          <email>studeny@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Vaclav Kratochvil</name>
          <email>velorex@utia.cas.cz</email>
          <location>
            <country>
              <code>CZ</code>
              <name>Czech Republic</name>
            </country>
            <city>
              <name>Prague</name>
              <latitude>50.08804</latitude>
              <longitude>14.42076</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>extreme exact game</keyword>
        <keyword>coherent lower probability</keyword>
        <keyword>core</keyword>
        <keyword>credal set</keyword>
        <keyword>supermodular game</keyword>
        <keyword>2-monotone lower probability</keyword>
        <keyword>min-representation</keyword>
        <keyword>oxytrophic game</keyword>
      </keywords>
      <abstract>The notion of a (discrete) coherent lower probability corresponds to a game-theoretical concept of an exact (cooperative) game. The collection of (standardized) exact games forms a pointed polyhedral cone and the paper is devoted to the extreme rays of that cone, known as extreme exact games. A criterion is introduced for testing whether an exact game is extreme. The criterion leads to solving simple linear equation systems determined by (the vertices of) the core polytope (of the game), which concept corresponds to the notion of an induced credal set in the context of imprecise probabilities. The criterion extends and modifies a former necessary and sufficient condition for the extremity of a supermodular game, which concept corresponds to the notion of a 2-monotone lower probability. The linear condition we give in this paper is shown to be necessary for an exact game to be extreme. We also know that the condition is sufficient for the extremity of an exact game in an important special case. The criterion has been implemented on a computer and we have made a few observations on basis of our computational experiments.</abstract>
      <pdf>http://proceedings.mlr.press/v62/studen%C3%BD17a/studen%C3%BD17a.pdf</pdf>
    </paper>
    <paper>
      <id>028</id>
      <title>A Note on Imprecise Monte Carlo over Credal Sets via Importance Sampling</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>importance sampling</keyword>
        <keyword>lower prevision</keyword>
        <keyword>monte carlo</keyword>
        <keyword>optimization</keyword>
      </keywords>
      <abstract>This brief paper is an exploratory investigation of how we can apply sensitivity analysis over importance sampling weights in order to obtain sampling estimates of lower previsions described by a parametric family of distributions. We demonstrate our results on the imprecise Dirichlet model, where we can compare with the analytically exact solution. We discuss the computational limitations of the approach, and propose a simple iterative importance sampling method in order to overcome these limitations. We find that the proposed method works pretty well, at least in the example studied, and we discuss some further possible extensions.</abstract>
      <pdf>http://proceedings.mlr.press/v62/troffaes17a/troffaes17a.pdf</pdf>
    </paper>
    <paper>
      <id>029</id>
      <title>Imprecise Swing Weighting for Multi-Attribute Utility Elicitation Based on Partial Preferences</title>
      <authors>
        <author>
          <name>Matthias Troffaes</name>
          <email>matthias.troffaes@durham.ac.uk</email>
          <location>
            <country>
              <code>GB</code>
              <name>United Kingdom</name>
            </country>
            <city>
              <name>Durham (UK)</name>
              <latitude>54.77639</latitude>
              <longitude>-1.57587</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Ullrika Sahlin</name>
          <email>ullrika.sahlin@cec.lu.se</email>
          <location>
            <country>
              <code>SE</code>
              <name>Sweden</name>
            </country>
            <city>
              <name>Lund</name>
              <latitude>55.70584</latitude>
              <longitude>13.19321</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>utility</keyword>
        <keyword>partial preference</keyword>
        <keyword>consistency</keyword>
        <keyword>uniqueness</keyword>
        <keyword>multi-attribute</keyword>
        <keyword>elicitation</keyword>
        <keyword>imprecise</keyword>
        <keyword>robust</keyword>
        <keyword>swing weighting</keyword>
      </keywords>
      <abstract>We describe a novel approach to multi-attribute utility elicitation which is both general enough to cover a wide range of problems, whilst at the same time simple enough to admit reasonably straightforward calculations. We allow both utilities and probabilities to be only partially specified, through bounding. We still assume marginal utilities to be precise. We derive necessary and sufficient conditions under which our elicitation procedure is consistent. As a special case, we obtain an imprecise generalization of the well known swing weighting method for eliciting multi-attribute utility functions. An example from ecological risk assessment demonstrates our method.</abstract>
      <pdf>http://proceedings.mlr.press/v62/troffaes17b/troffaes17b.pdf</pdf>
    </paper>
    <paper>
      <id>030</id>
      <title>Exchangeable Choice Functions</title>
      <authors>
        <author>
          <name>Gert de Cooman</name>
          <email>gert.decooman@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Arthur van Camp</name>
          <email>arthur.vancamp@ugent.be</email>
          <location>
            <country>
              <code>BE</code>
              <name>Belgium</name>
            </country>
            <city>
              <name>Ghent</name>
              <latitude>51.05000</latitude>
              <longitude>3.71667</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>exchangeability</keyword>
        <keyword>choice function</keyword>
        <keyword>indifference</keyword>
        <keyword>set of desirable gambles</keyword>
        <keyword>representation</keyword>
      </keywords>
      <abstract>We investigate how to model exchangeability with choice functions. Exchangeability is a structural assessment on a sequence of uncertain variables. We show how such assessments constitute a special kind of indifference assessment, and how this idea leads to a counterpart of de Finetti's Representation Theorem, both in a finite and a countable context.</abstract>
      <pdf>http://proceedings.mlr.press/v62/van%C2%A0camp17a/van%C2%A0camp17a.pdf</pdf>
    </paper>
    <paper>
      <id>031</id>
      <title>Computing Minimax Decisions with Incomplete Observations</title>
      <authors>
        <author>
          <name>Thijs van Ommen</name>
          <email>T.vanOmmen@uva.nl</email>
          <location>
            <country>
              <code>NL</code>
              <name>Netherlands</name>
            </country>
            <city>
              <name>Amsterdam</name>
              <latitude>52.37403</latitude>
              <longitude>4.88969</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>coarse data</keyword>
        <keyword>incomplete observations</keyword>
        <keyword>minimax decision making</keyword>
      </keywords>
      <abstract>Decision makers must often base their decisions on incomplete (coarse) data. Recent research has shown that in a wide variety of coarse data problems, minimax optimal strategies can be recognized using a simple probabilistic condition. This paper develops a computational method to find such strategies in special cases, and shows what difficulties may arise in more general cases.</abstract>
      <pdf>http://proceedings.mlr.press/v62/van%C2%A0ommen17a/van%C2%A0ommen17a.pdf</pdf>
    </paper>
    <paper>
      <id>032</id>
      <title>Agreeing to Disagree and Dilation</title>
      <authors>
        <author>
          <name>Jiji Zhang</name>
          <email>jijizhang@ln.edu.hk</email>
          <location>
            <country>
              <code>CN</code>
              <name>China</name>
            </country>
            <city>
              <name>Hong Kong</name>
              <latitude>22.28552</latitude>
              <longitude>114.15769</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Hailin Liu</name>
          <email>liuhlin3@mail.sysu.edu.cn</email>
          <location>
            <country>
              <code>CN</code>
              <name>China</name>
            </country>
            <city>
              <name>Guangzhou</name>
              <latitude>23.11667</latitude>
              <longitude>113.25000</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
        <author>
          <name>Teddy Seidenfeld</name>
          <email>teddy@stat.cmu.edu</email>
          <location>
            <country>
              <code>US</code>
              <name>United States</name>
            </country>
            <city>
              <name>Pittsburgh</name>
              <latitude>40.47441</latitude>
              <longitude>-79.95097</longitude>
            </city>
            <university>
              <name>NA</name>
              <department>NA</department>
            </university>
          </location>
        </author>
      </authors>
      <keywords>
        <keyword>agreeing to disagree</keyword>
        <keyword>common knowledge</keyword>
        <keyword>dilation</keyword>
        <keyword>imprecise probability</keyword>
      </keywords>
      <abstract>We consider Geanakoplos and Polemarchakis's generalization of Aumman's famous result on {"}agreeing to disagree{"}, in the context of imprecise probability. The main purpose is to reveal a connection between the possibility of agreeing to disagree and the interesting and anomalous phenomenon known as dilation. We show that for two agents who share the same set of priors and update by conditioning on every prior, it is impossible to agree to disagree on the lower or upper probability of a hypothesis unless a certain dilation occurs. With some common topological assumptions, the result entails that it is impossible to agree not to have the same set of posterior probabilities unless dilation is present. This result may be used to generate sufficient conditions for guaranteed full agreement in the generalized Aumman-setting for some important models of imprecise priors, and we illustrate the potential with an agreement result involving the density ratio classes. We also provide a formulation of our results in terms of {"}dilation-averse{"} agents who ignore information about the value of a dilating partition but otherwise update by full Bayesian conditioning.</abstract>
      <pdf>http://proceedings.mlr.press/v62/zhang17a/zhang17a.pdf</pdf>
    </paper>
  </proceedings>
</proceedingslist>